

# Redis

## 高性能的原因

### 操作在内存中完成

### 单线程模式

Redis 的网络 IO和键值对读写是由一个线程来完成的，避免了上下文切换和锁的竞争

但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的

### 高效的数据结构

底层的存储结构并不是一成不变的，会根据存储的类型和元素的个数选择合适的数据结构，在时间复杂度和空间复杂度之间做权衡

采用了 I/O 多路复用机制，处理大量的客户端 Socket 请求

## 数据类型

### String 

采用 sds 来进行存储

### List

相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢

ziplist列表项包括三部分内容，分别是前一项的⻓度（prevlen）、当前项⻓度信息的编码结果（encoding），以及当前项的实际数据（data）

编码技术，就是指用不同数量的字节来表示保存的信息用不同数量的字节来表示保存的信息。在ziplist中，编码技术主要应用在列表项中的prevlen和encoding这两个元数据上。而当前项的实际数据data，则正常用整数或是字符串来表示

了方便查找，每个列表项中都会记录前一项的⻓度。因为每个列表项的⻓度不一样，所以如果使用相同的字节大小来记录prevlen，就会造成内存空间浪费。

一个列表项的实际数据，既可以是整数也可以是字符串。整数可以是16、32、64等字节⻓度，同时字符串的⻓度也可以大小不一

但是ziplist会有查找复杂度高、连锁更新的⻛险。ziplist新增某个元素或修改某个元素时，可能会导致后续元素的prevlen占用空间都发生变化，从而引起连锁更新问题，导致每个元素的空间都要重新分配，这就会导致ziplist的访问性能下降

quicklist

quicklist的设计，其实是结合了链表和ziplist各自的优势。简单来说，一个quicklist就是一个链表，而链表中的每个元素又是一个ziplist。

quicklist中的元素的定义，也就是quicklistNode。因为quicklist是一个链表，所以每个quicklistNode中，都包含了分别指向它前序和后序节点的指针指针*prev和和*next。同时，每个quicklistNode又是一个ziplist，所以，在quicklistNode的结构体中，还有指向ziplist的指针指针*zl

插入元素时，单个ziplist是否不超过8KB，或是单个ziplist里的元素个数是否满足要求，只要这里面的一个条件能满足，quicklist就可以在当前的quicklistNode中插入新元素，否则quicklist就会新建一个quicklistNode，以此来保存新插入的元素。

quicklist通过控制每个quicklistNode中，ziplist的大小或是元素个数，就有效减少了在ziplist中新增或修改元素后，发生连锁更新的情况，从而提供了更好的访问性能。

listpack

listpack数据结构，用来彻底避免连锁更新。
listpack也叫紧凑列表，它的特点就是用一块连续的内存空间来紧凑地保存数据用一块连续的内存空间来紧凑地保存数据，同时为了节省内存空间
listpack列表项使用了多种编码方式，来表示不同⻓度的数据listpack列表项使用了多种编码方式，来表示不同⻓度的数据，这些数据包括整数和字符串。

listpack列表项也包含了元数据信息和数据本身。不过，为了避免ziplist引起的连锁更新问题，listpack中的每个列表项不再像ziplist列表项那样，保存其前一个列表项的⻓度，它只会包含三个方面内容，分别是当前元素的编码类型（entry-encoding）、元素数据(entry-data)，以及编码类型和元素数据这两部分的⻓度(entry-len)

### Set

相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值 NULL

如果是int，则采用intset

### SortedSet

如果元素数小于 128 且元素长度小于 64，则使用 ziplist 存储，否则使用 zskiplist 存储

支持范围查询，这是因为它的核心数据结构设计采用了跳表，跳表其实是一种多层的有序链表

如何level数组具体有几层

一种设计方法是，让每一层上的结点数约是下一层上结点数的一半，这种设计方法带来的好处是，当跳表从最高层开始进行查找时，由于每一层结点数都约是下一层结点数的一半，这种查找过程就类似于二分查找，查找复杂度可以降低到O(logN)查找复杂度可以降低到O(logN)

但这种设计方法也会带来负面影响，那就是为了维持相邻两层上结点数的比例为2:1，一旦有新的结点插入
或是有结点被删除，那么插入或删除处的结点，及其后续结点的层数都需要进行调整，而这样就带来了额外
的开销

另一种设计方法，即随机生成每个结点的层数，每增加一层的概率不会超过25%，新插入一个节点，只需修改这一层前后节点的指针，不影响其它节点的层数，降低了操作复杂度

而它又能以常数复杂度获取元素权重，这是因为它同时采用了哈希表进行索引

### Hash

相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来

如果元素数小于 512，并且元素长度小于 64，则用 ziplist 存储，否则使用 dict 字典存储

Redis中的的dict结构，采用链式哈希的方式存储，当哈希冲突严重时，会开辟一个新的哈希
表，翻倍扩容，并采用渐进式rehash方式迁移数据

**底层的存储结构并非一成不变，根据存放的数据类型和存放的数据条数决定使用何种数据结构进行存储在空间复杂度和时间复杂度做权衡**

## 渐进式rehash

### 为什么需要渐进式

Redis是单进程模式，当键值对数量达到了千万、亿级别时，整个rehash过程将非常缓慢，可能会造成很严重的服务不可用现象

### 执行流程

#### 文件事件

执行插入、删除、查找、修改等操作时，如果当前字典rehash操作正在进行，则迁移1个桶

#### 时间事件

每次对100个桶进行rehash操作，共执行1毫秒

渐进式rehash会分批拷贝完成rehash，将集中处理的时间分散到多次操作中

## 持久化

### 为什么需要持久化？

有效的避免进程退出造成的数据丢失问题

当下次重启时可以利用持久化的文件即可实现数据恢复

缓存就是暂存数据的地方，有必要持久化吗？***

### RDB

#### 特点

默认的持久化方式，把当前进程的数据生成快照保存到硬盘

RDB文件是经过压缩的二进制文件，占用的空间会小于内存中的数据，更加利于网络传输，进行备份

安全退出的模式redis-cli SHUTDOWN，会将内存中的数据生成一次RDB快照

#### 触发方式

主动触发

#在60秒内如果有1000个key发生变化，就会触发一次RDB快照的执行
save 60 1000

手动触发

save命令

在主线程中执行，会导致阻塞Redis服务器

bgsave命令

Redis服务器执行fork操作，创建子进程，执行持久化操作，完成后自动结束

阻塞只发生在fork阶段，一般时间很短

#### 流程

执行bgsave命令，Redis父进程判断当前是否有子进程执行RDB或者AOF，如果存在bgsave命令直接返回

执行fork操作，会导致父进程短暂的阻塞（通过info stats命令查看latest_fork_usec选项，可以获取最近一次fork操作的耗时，单位为毫秒）

父进程fork完成后，不再阻塞父进程，可以继续响应其他命令

子进程创建RDB文件，根据父进程的数据生成快照，完成后替换原有文件

与此同时父进程继续接收并处理客户端发来的命令，主线程对修改的数据复制一份，生成一个副本，bgsave 子进程会把这个副本数据写入RDB 文件

#### 压缩

默认采用LZF算法对生成的RDB文件进行压缩处理，压缩后的文件远小于内存大小

压缩虽然消耗CPU，但可大幅降低文件体积，方便保存到硬盘或通过网络发送给从节点

#### 建议

避免频繁的执行快照

每次执行快照都会将全量数据写入磁盘，会给磁盘带来很大压力

fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长

#### 缺点

丢失最后一次快照以后更改的所有数据

Redis数据量很大，执行一次RDB需要的时间会相应增加，磁盘压力随之增加，如果存在主从复制，网络压力也会增加

### AOF

#### 特点

最大秒级丢失数据

记录执行的操作，体积大，性能消耗高，数据恢复慢

#### 开启AOF

appendonly yes

#### 执行过程

首先执行命令

所有的写命令追加到AOF缓冲区（如果每次都写入硬盘，那么性能取决于当前硬盘的负载）

根据对应的策略向硬盘做同步操作

随着AOF文件增大，需定期执行重写操作，达到压缩的目的

当服务器重启时，加载AOF文件进行数据恢复

如果在重写，写入重写缓存中

#### 刷盘策略

每个写命令执行完，立马同步地将日志写回磁盘：appendfsync always

每秒调用一次fsync，将日志写回磁盘：appendfsync everysec

由操作系统周期性的将日志写回磁盘：appendfsync no

#### AOF重写

##### 为什么重写

AOF文件不能过大，操作系统会文件的大小有限制

文件太大，追加命令记录的效率会降低

实例宕机，恢复比较慢，影响到 Redis 的正常使用，更小的AOF文件可以更快的加载

##### 触发方式

###### 自动触发

auto-aof-rewrite-min-size,表示AOF文件重写时文件最小体积，默认64M

auto-aof-rewrite-percentage，代表当前AOF文件空间和上次重写后的AOF文件空间的比值

###### 手动触发

执行bgrewriteaof命令显式触发

##### 实现流程

fork子进程将内存快照存储到临时文件

主进程继续接收用户请求，写指令写入rewrite缓冲区

子进程写完临时文件，通知主进程

主进程将aof rewrite缓冲区的数据写入aof临时文件

将aof临时文件替换旧的aof文件，异步关闭删除旧的aof文件

### RDB&AOF

RDB保存的是最终的数据，是一个最终状态

AOF保存的是达到这个最终状态的过程

### 混合存储

进行AOF重写时子进程将当前时间点的数据快照保存为RDB文件格式，而后将父进程累积命令保存为AOF格式

加载时，首先会识别AOF文件是否以REDIS字符串开头，如果是，就按RDB格式加载，加载完RDB后继续按AOF格式加载剩余部分

### 建议

如果允许分钟级别的数据丢失，可以只使用 RDB

如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡

注意监控fork耗时

fork操作创建子进程，不会拷贝父进程的物理内存空间，但是会复制父进程的空间内存页表

fork操作的耗时与进程的总内存大小成正相关

可以使用info stats命令查看lastest_fork_usec指标获取最近一次fork操作的耗时

控制Redis实例的最大可用内存，fork操作耗时跟内存量成正比

降低fork操作的频率

## 缓存淘汰策略

### 淘汰原理

当占用空间超过阈值，根据淘汰策略进行数据淘汰

### 八二原理

有 20% 的数据贡献了 80% 的访问

按照“八二原理”来设置缓存空间容量，也就是把缓存空间容量设置为总数据量的 20% 的话，就有可能拦截到 80% 的访问

建议把缓存容量设置为总数据量的 15% 到 30%，兼顾访问性能和内存空间开销

### 删除方式

#### 同步删除

#### 异步删除

对于big key的删除，可以有效的避免延迟

### 淘汰策略

#### 不进行数据淘汰的策略

no-enviction：禁止驱逐数据

#### 进行数据淘汰的策略

在设置了过期时间的数据中进行淘汰

volatile-lru

淘汰所有设置了过期时间的键值中最久未使用的键值

volatile-lfu

淘汰所有设置了过期时间的键值中最少使用的键值

volatile-random

任意选择数据淘汰

volatile-ttl

越早过期的越先被删除

在所有数据中进行淘汰

allkeys-lru

挑选最近最少使用的数据淘汰

allkeys-lfu

淘汰最近访问频次最小的key

allkeys-random

任意选择数据淘汰

### LRU算法

#### 算法原理

刚刚被访问的数据，肯定还会被再次访问，所以就把它放在链表头部。长久不访问的数据，肯定就不会再被访问了，所以就让它逐渐后移到链表尾部，在缓存满时，就优先删除它

#### 优化

原生的LRU算法，需要链表管理所有的缓存数据，带来额外的空间开销。大量数据被访问时，会带来链表的移动操作，会很耗时，降低redis的性能

默认会记录每个数据最近访问的时间，当淘汰数据时，会所随机选出N个数据，作为候选集合，然后比较N个数据的lru字段，将lru字段值最小的数据从缓存中淘汰

Redis不用为所有的数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能

### LFU算法

Redis 并没有采用数据每被访问一次，就给对应的counter值加1的计数规则。LFU策略实现的计数规则是：每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有p值大于r值时，计数器才加 1

衰减机制

配置项 lfu_decay_time控制访问次数的衰减。LFU策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以 lfu_decay_time 值，所得的结果就是数据 counter 要衰减的值。lfu_decay_time越大，衰减值也就越小，衰减效果也就越弱

### 建议

业务数据中有明显的冷热数据区分，使用 allkeys-lru 策略

如果业务应用中的数据访问频率相差不大，没有明显的冷热数据区分，建议使用allkeys-random 策略

## 过期删除策略

### 惰性过期

只有当访问一个key时，才会判断该key是否已过期，过期则清除

该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存

### 定时过期

采用随机抽取的形式，每次从过期字典中（redis中含有数据字典和过期key字典），取出 20 个键进行过期检测。

如果这批随机检查的数据中有 25% 的比例过期，那么会再抽取 20 个随机键值进行检测和删除，并且会循环执行这个流程

直到抽取的这批数据中过期键值小于 25%或者超过指定的执行时间，此次检测才算完成。默认随机抽取16次之后会判断是否超过时间限制

**注：限定定时任务执行的时间，避免对其他请求的执行造成影响**

## 主从复制

### 为什么需要主从复制

读写分离、提升并发的能力

数据备份

### 复制命令

eplicaof  主库ip 主库port（Redis 5.0之前使用 slaveof）

### 复制分类

#### 全量同步

##### 概述

在 Redis 2.8 之前，Redis 基本只支持全量复制。在 slave 与 master 断开连接，或 slave 重启后，都需要进行全量复制

##### 实现

第一次复制为全量复制，发送psync命令（2.8版本之前使用sync命令），包含了主库的runId和复制进度offset，第一次复制时runId为?，offset为-1

主库收到 psync 命令后，会返回FULLRESYNC响应，表示是全量复制，同时包含了主库的runId还有主库目前的复制进度offset

主库生成RDB快照发送给从库，从库先清除本地的数据，再加载RDB文件

主库同步期间仍然可以接受客户端请求，将写入的数据放在replication buffer

RDB文件同步完成后，将replication buffer中的数据同步给从库

#### 增量同步

##### 概述

把主从库网络断连期间主库收到的命令，同步给从库

##### 实现

当主从库断连后，主库会把断连期间收到的写操作命令写入 repl_backlog_buffer 这个环形缓冲区

主库会记录自己写到的位置，从库则会记录自己已经读到的位置，开始时主库和从库的读写位置一致

当从库重新连接上主库，发送psync命令并且把salve_repl_offset发送给主库，主库根据slave_repl_offset和mater_repl_offset之间的差距，决定是增量同步还是全量同步

如果slave尚未读取的数据已经被主库写入的操作覆盖，则会导致主从不一致进行全量同步，如果runId与主库的runId不一致也会进行全量同步

如果slave同步的数据仍然在replication buffer中，则执行增量同步，将slave_repl_offset之后的数据同步给从库

##### 4.0增强实现

主服务器故障导致主从切换，主服务器ID发生改变，触发全量同步

强化了psync，引入了 psync2。在 pysnc2 中，主从复制不再使用 runid，而使用 replid、replid2 来作为复制判断依据

对于主服务器，replid表示的是当前服务器的运行ID；对于从服务器，replid表示其复制的主服务器的运行ID。

某一时刻主服务器发生故障，从服务器升级为主服务器时，修改从服务器的replid2为主服务器的replid

在主从同步时，如果从服务的replid相与主服务器的replid相同或者与replid2相同，并且psync_offset满足条件，则会进行增量同步

### 心跳检测

主从建立连接后，默认会以每秒一次的频率，向主服务器发送命令：replconf ack <replication offset>

检测主从服务器的网络连接状态,通过向主服务器发送INFO replication命令，可以看到从服务器最后一次向主服务器发送此命令距离现在过了多少秒

辅助实现min-slaves选项；min-slaves-to-write和min-slaves-max-lag两个选项可以防止主服务器在不安全的情况下执行写命令

检测命令丢失；因为网络故障，主服务器传播给从服务器的写命令在半路丢失，通过此命令，主服务器发现主从数据不一致，向从服务器发送缺失的写命令

### 复制分析

短时重连、slave重启、切主等均可增量同步

较长时间的断连仍然要进行全量同步

增量同步严重依赖环形复制缓冲区

全量同步基于RDB文件进行同步，RDB文件内容是经过压缩的二进制文件，占用空间小，消耗的网络资源也就少。RDB加载比AOF快

### 使用建议

主从复制不要用图状结构，用单向链表结构更为稳定，即主->从->从模式，可以减轻主库的压力（包括生成RDB的磁盘压力和同步RDB的网络压力）

合理设置环形复制缓冲区

### 无磁盘复制

Redis在与从数据库进行复制初始化时将不会将快照存储到磁盘，而是直接通过网络发送给从数据库，避免了 IO性能差问题

开启无磁盘复制:repl-diskless-sync yes

### 配置参数

bind：设置为其它哨兵实例的 IP 地址。只有在 bind 中设置了 IP 地址的哨兵，才可以访问当前实例

protected-mode ：限定哨兵实例能否被其他服务器访问，设置为 no 时，其他服务器也可以访问这个哨兵实例

合理设置环形复制缓冲区的大小repl_backlog_size
repl_backlog_size =  2 *（主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小）

### 主从复制存在的问题

#### 主从不一致问题

由于主从库间的命令复制是异步进行的，客户端从从库中读取到的值和主库中的最新值并不一致

解决方案

保证主从间的通络通信良好，避免主从实例跨机房部署

监控主从库间的复制进度（ INFO replication 命令可以查看主库接收写命令的进度信息（master_repl_offset）和从库复制写命令的进度信息（slave_repl_offset））

#### 读取过期数据

原因

应用主从集群时，尽量使用 Redis 3.2 及以上版本，之前版本中从库在服务读请求时，并不会判断数据是否过期，而是会返回过期数据

数据过期命令使用不当

EXPIRE 和 PEXPIRE：它们给数据设置的是从命令执行时开始计算的存活时间

EXPIREAT 和 PEXPIREAT：它们会直接把数据的过期时间设置为具体的一个时间点

解决方案

使用 Redis 3.2 及以上版本

使用 EXPIREAT/PEXPIREAT 命令设置过期时间(主从节点上的时钟保持一致)

## 哨兵机制

### 作用

通过主从库的自动切换，实现故障转移，保证写的高可用

### 流程

#### 如何判断主库挂了

##### 主观下线（S_DOWN）

哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态

##### 客观下线（O_DOWN）

多个哨兵实例（由配置文件中的quorum决定），都判断主库已经“主观下线”了，主库才会被标记为“客观下线”，此时才会开始执行切换

多个哨兵实例组成的集群模式进行部署；引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况

#### 选择新的主库

首先筛选出可以参加选主操作的从节点

当前在线并且之前的网络连接状态良好的从节点

配置项 down-after-milliseconds * 10
down-aftermilliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-aftermilliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。

从库打分，选出适合当做主节点的从节点

优先级最高的从库得分高，通过 slave-priority 配置项，给不同的从库设置不同优先级

和旧主库同步程度最接近的从库得分高

ID 号小的从库得分高

#### 哨兵Leader选举

哨兵实例进行投票选举Leader，负责进行主从切换操作

第一，拿到半数以上的赞成票

第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值

#### 通知

把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制

哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上

### 基于发布订阅的哨兵集群

主库上有一个名为“__sentinel__:hello”的频道，哨兵实例向此频道发布自己的IP地址和端口号，同时订阅此频道，获取哨兵集群中其他的哨兵实例，彼此之间建立连接

哨兵实例通过向主库发送info命令，获取从库的ip和port，并建立连接

基于哨兵自身的 pub/sub 功能，这实现了客户端和哨兵之间的事件通知

### 脑裂

#### 发生原因

客户端与redis主库通信良好

redis主库与哨兵集群发生网络分区，认为redis主库宕机，执行主从切换，此时客户端仍然可以向主库写数据

哨兵提升从库为新的主库。网络分区仍然存在，此时存在两个主库。在切换这段时间内，原主库新写入的数据无法同步至从库，所以新的主库所存储的数据不是最新的

当网络分区消失之后，原来的主库执行slave of命令，和新主库进行全量复制，先清空本地数据，再加载新主库发送的RDB文件

和主库部署在同一台服务器上的其他程序临时占用了大量资源（例如 CPU 资源），导致主库资源使用受限

主库自身遇到了阻塞的情况，例如，处理 bigkey 或是发生内存 swap、或者Lua脚本中执行操作过多

#### 解决方案

min-slaves-to-write：主库能进行数据同步的最少从库数量

min-slaves-max-lag：主从库间进行数据复制时，从库给主库发送ACK 消息的最大延迟（以秒为单位）

主库连接的从库中至少有N个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的请求了

## 集群

### 为什么需要集群

集群用来提供横向扩展能力，即当数据量增多之后，通过增加服务节点就可以扩展服务能力，将数据分摊到各个节点

### 数据分区规则

#### 哈希分区

特点

离散度好

数据分布业务无关

无法顺序访问

规则

节点取余分区

hash（key）%N(集群节点数量)，计算出哈希值，用来决定数据映射到哪一个节点

当节点数量发生变化时，如扩容或收缩节点，数据节点的映射关系需要重新计算，会导致数据的重新迁移

常用于数据库分库分表规则，一般采用预分区，提前规划好分区数，保证可支撑未来一段时间的数据量。扩容采用翻倍扩容，避免全量数据的迁移

一致性哈希分区

虚拟槽分区

#### 顺序分区

离散度容易倾斜，新写入的数据访问的次数多

数据分布业务相关*

可顺序访问

### 分片规则

实现数据分片集群，定义了数据和实例的对应规则；每个实例只存放部分数据

用哈希槽（Hash Slot），来处理数据和实例之间的映射关系.集群共有 16384个哈希槽

每个节点中都会保存所有16384个slot对应到哪一个节点的信息，所以客户端可以请求任意一个节点获取slot对应的实例，客户端会对映射关系进行缓存

创建集群时，Redis会自动把这16384个槽平均分布在集群实例上

### 重定向机制

客户端无法及时感知集群中槽位信息的变化。客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。

#### MOVE

槽位已经迁移完成，返回槽位对应的机器，并更新客户端本地的槽位信息

#### ASK

槽位正在迁移中，返回迁移的实例，不会更新客户端缓存的哈希槽分配信息

如果能在source节点找到对应的key，那么直接执行client的命令

如果找不到该key，source节点会向client发送一个ASK错误，

### Gossip 协议

1、每个实例之间会按照一定的频率，从集群中随机挑选一些实例，发送PING 消息
		发送PING消息，用来检测这些实例是否在线，并交换彼此的状态信息
		PING 消息中封装了实例自身的状态信息、部分其它实例的状态信息，以及 Slot 映射表
2、一个实例在接收到 PING 消息后，会返回一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样
	Gossip 协议可以保证在一段时间后，集群中的每一个实例都能获得其它所有实例的状态信息
	实例间使用 Gossip 协议进行通信时，通信开销受到通信消息大小和通信频率这两方面的影响

### 通信开销

#### Gossip 消息大小

​		每个实例在发送一个 Gossip 消息时，除了会传递自身的状态信息，默认还会传递集群十分之一实例的状态信息。一个 Gossip 消息的大小，即 104 字节，为了让 Slot 映射表能够在不同实例间传播，PING 消息中还带有一个长度为 16,384bit 的 Bitmap，这个 Bitmap 的每一位对应了一个 Slot，如果某一位为 1，就表示这个Slot 属于当前实例。这个 Bitmap 大小换算成字节后，是 2KB

#### 实例间通信频率

​		默认1秒发送一条 PING 消息
​		也会按照每 100ms 一次的频率，扫描本地的实例列表，如果发现最近一次接收 PONG 消息的时间，已经大于配置项 cluster-node-timeout 的一半了，就会立刻给该实例发送 PING 消息
​		当集群规模扩大之后，因为网络拥塞或是不同服务器间的流量竞争，会导致实例间的网络通信延迟增加，增加额外的网络通信开销

### 降低通信开销

​	修改集群实例被判断为故障的心跳超时时间，即配置项cluster-node-timeout。默认是 15秒。如果 cluster-node-timeout 值比较小，在大规模集群中，就会比较频繁地出现 PONG 消息接收超时的情况。
​	为了避免过多的心跳消息挤占集群带宽，我们可以调大 cluster-node-timeout值，比如说调大到 20 秒或 25 秒
​	也不要把 cluster-node-timeout 调得太大，否则，如果实例真的发生了故障，导致故障恢复时间被延长，会影响到集群服务的正常使用

### 限制Redis Cluster规模的关键因素

心跳消息大小

默认每个1秒发送一条 PING 消息，PING 消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 Slot 映射表；一个实例在接收到 PING 消息后，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样。

通信频繁

每个实例每 100 毫秒会做一次检测，给 PONG 消息接收超过 cluster-node-timeout/2的节点发送 PING 消息。

### 思考

​	集群内通信主要是为了获知其他实例的状态信息以及slot的存储信息
​	如果将通信的信息存放到第三方存储系统上，就可以减少集群内的通信
​	集群内的每个实例只需向第三方上报状态信息和slot信息即可
​	比如Codis

### hash tags的机制

当一条命令需要操作的key分属于不同的节点时，Redis会报错

hash tags的机制保证当需要进行多个key的处理时将所有key分布到同一个节点

在迁移数据时要控制迁移的 key 数量和 key 大小，避免一次性迁移过多的 key 或是过大的 key，而导致 Redis 阻塞



在B节点执行：把slot3从A节点迁入过来

CLUSTER SETSLOT slot3 IMPORTING nodeA

在A节点执行：把slot3迁移到B节点

CLUSTER SETSLOT slot3 MIGRATING nodeB





## 消息模式

### 队列模式

list类型的lpush和rpop实现消息队列；消息接收方如果不知道队列中是否有消息，会一直发送rpop命令，如果这样的话，会每一次都建立一次连接，这样显然不好。
可以使用brpop命令，它如果从队列中取不出来数据，会一直阻塞，在一定范围内没有取出则返回null

### 发布订阅模式

subscribe、publish 

## 指令安全

 keys 指令会导致 Redis 卡顿；flushdb 和 flushall 会让 Redis 的所有数据全部清空

 rename-command 指令用于将某些危险的指令修改成特别的名称，用来避免人为误操作

密码访问限制

## 安全通信

spiped SSL代理软件

## 内存碎片

### 原因

内存分配器一般是按固定大小来分配内存，不会完全按照应用程序申请的内存空间大小给程序分配

redis中的键值对不小不一，并且增删改

### 监控内存碎片率

INFO 命令，可以用来查询内存使用的详细信息，其中的mem_fragmentation_ratio 表示的就是 Redis 当前的内存碎片率
mem_fragmentation_ratio=used_memory_rss/used_memory
used_memory_rss 是操作系统实际分配给 Redis 的物理内存空间，里面就包含了碎片；而 used_memory 是 Redis 为了保存数据实际申请使用的空间

例如，Redis 申请使用了 100 字节（used_memory），操作系统实际分配了 128 字节（used_memory_rss），此时，mem_fragmentation_ratio 就是1.28

mem_fragmentation_ratio 大于 1.5 。这表明内存碎片率已经超过了 50%。一般情况下，这个时候，我们就需要采取一些措施来降低内存碎片率了

当mem_fragmentation_ratio小于1时，说明操作系统分配的内存小于实际存储数据的内存，内存不够用，发生了swap

### 解决方案

1、重启redis实例

2、自动内存碎片清理

config set activedefrag yes：启用了自动清理功能

active-defrag-ignore-bytes 100mb：表示内存碎片的字节数达到 100MB 时，开始清理

active-defrag-threshold-lower 10：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理

active-defrag-cycle-min 25： 表示自动清理过程所用 CPU 时间的比例不低于25%

active-defrag-cycle-max 75：表示自动清理过程所用 CPU 时间的比例不高于75%

优秀设计

使用连续的内存可以避免内存碎片

针对不同长度的数据，采用不同大小的元数据，避免使用同一大小的元数据，造成空间的浪费

## NVM（非易失存储）

### 概述

可以直接持久化保存数据

NVM 内存的容量很大

NVM 内存的访问速度接近 DRAM 的速度

### Memory模式

NVM 内存作为大容量内存来使用的，只使用 NVM 容量大和性能高的特性，没有启用数据持久化的功能。

### App Direct模式

应用软件把数据写到AEP内存上时，数据就直接持久化保存下来了

服务器中部署了 PM 后，我们可以在操作系统的 /dev 目录下看到一个 PM 设备

使用ext4-dax格式化

挂载到服务器的某一个目录

在目录下创建文件，再将文件映射到redis进程空间

## 原子操作方法

把多个操作在 Redis 中实现成一个操作，也就是单命令操作

把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本

## 监控

查看每秒执行的多少次指令

redis-cli info stats | grep ops

可以通过monitor指令查看哪些key访问比较频繁，从而在业务上进行优化

连接了多少客户端

redis-cli info clients

查看复制积压缓冲区

redis-cli info replication | grep backlog

## 6.0新特性

### 线程模型

#### 核心	

多个IO线程处理网络请求，但是对于读写命令，还是单线程处理

#### 请求处理流程

1、服务端和客户端建立 Socket 连接，并分配处理线程
	  当有客户端请求和实例建立 Socket 连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中
	  紧接着，主线程通过轮询方法把 Socket 连接分配给 IO 线程
2、IO 线程读取并解析请求
	   主线程一旦把 Socket 分配给 IO 线程，就会进入阻塞状态，等待 IO 线程完成客户端请求读取和解析
3、主线程执行请求操作，等到 IO 线程解析完请求，主线程还是会以单线程的方式执行这些命令操作
4、IO 线程回写 Socket 和主线程清空全局队列
	  当主线程执行完请求操作后，会把需要返回的结果写入缓冲区
	  然后，主线程会阻塞等待IO 线程把这些结果回写到 Socket 中，并返回给客户端

#### 参数配置

设置 io-thread-do-reads 配置项为 yes，表示启用多线程，io-threads 配置的IO线程数

### 弊端

读取数据、写入数据主线程都会进入阻塞状态，等的时间比较长，且这段时间无法提供服务

#### 阿里云Redis企业版-改进版

##### 改进之后的流程

​			主线程只负责命令处理，所有的读、写处理由IO线程全权负责。
​			当客户端连接进来之后，直接交给其他IO线程，从此客户端可读、可写的所有事件，主线程不再关心
​			当有命令到达，IO线程会把命令转发给主线程处理，处理完之后，通过通知方式把处理结果转给IO线程，由IO线程去写

##### 仍然存在的弊端

​			还是只有一个线程在处理命令，对于O(1)命令提升效果非常理想，但对于本身比较耗CPU的命令，效果不是很理想

#### 如何对上述弊端改进？

如果不考虑多线程并发的问题，可以根据命令的复杂度将请求分开，分别使用自己的线程处理。避免了慢命令对其他快命令的影响
redis之所以使用单线程处理命令，主要为了避免锁的竞争
那如何才能避免锁的竞争又可以多线程执行呢
只要保证对某个key的处理只被一个线程处理即可，对key进行分组，根据分组的数量设置处理线程数

### 绑定 CPU

CPU 亲和性是一种调度属性，它将一个进程或线程绑定到一个或一组 CPU 上。也称为 CPU 绑定

CPU绑定可以有效减少线程上下文切换
可以分别为server_cpulist（主线程和IO线程）、bio_cpulist（耗时的异步任务）、aof_rewrite_cpulist、bgsave_cpulist绑定CPU

### 客户端缓存（Tracking）

#### 普通模式(ESP 3 协议)

打开或关闭普通模式下的 Tracking 功能：CLIENT TRACKING ON|OFF

服务端记录客户端读取过的 key，一旦监测到 key 的值发生变化，服务端会给客户端发送 invalidate 消息，通知客户端缓存失效了。服务端对于记录的 key 只会报告一次invalidate 消息，类似于zk中的Watch机制

#### 广播模式(ESP 3 协议)

注册需要跟踪的key:LIENT TRACKING ON BCAST PREFIX user

在广播模式下，即使客户端还没有读取过 key，但只要它注册了要跟踪的 key，服务端都会把key 失效消息通知给这个客户端。一般会跟踪key的前缀

## 阻塞原因

### 外在原因

#### CPU竞争

Redis是CPU密集型应用，不建议与其他CPU应用部署在一起。当其他进程消耗CPU过多，会影响Redis的吞吐量

将Redis进程和CPU绑定

#### 内存交换

将Redis使用的部分内存换出到硬盘，会导致发生交换后的Redis性能急剧下降

使用 cat /proc/{redis-processId}/smaps | grep查看内存交换信息

保证机器充足的可用内存、降低系统使用swap优先级、设置Redis可使用的最大内存

#### 网络问题

网卡软中断

连接拒绝

Redis连接拒绝

Redis通过参数maxclients控制客户端最大连接数，默认10000

客户端应使用长连接或者连接池

连接溢出

进程限制

使用命令ulimit-n查看进程可代开的最大文件数

为了支持高并发，设置ulimit-n65535

backlog队列溢出

使用echo 511 > /proc/sys/net/core/maxconn

使用netstat -s| grep overflowed查看溢出情况

### 内在原因

不合理使用API或数据结构

如何发现慢查询

开启Redis慢查询统计功能，慢查询只记录了命名的执行时间，并没有记录网络传输时间和排队时间

如何优化慢查询

修改为低算法度的命令，如hgetall改为hmget等，禁用keys、sort等命令

缩减大对象数据或将大对象拆分为多个小对象

如何发现大对象

redis-cli -h {ip} -p {ip} bigkeys

内部采用分段进行scan操作

CPU饱和

概念

redis实例将单核CPU使用率跑到100%

如何查看Redis实例的CPU使用率

使用top命令查看Redis进程的CPU使用率

原因

并发量是否达到极限

使用stat命令查看获取redis使用情况

使用集群化水平扩展分摊OPS压力

使用了高算法复杂度的命令

过度的内存优化

为了追求低内存使用量，使用复杂度高的数据结构

使用info commandstats命令查看命令的平均执行耗时

##### fork阻塞

fork发生在RDB和AOF重写时

fork操作时会复制主进程的内存页表，如果redis实例占用的内存过大，fork会比较耗时

使用info stats命令获取latest_fork_usec指标，表示最近一次fork的耗时

避免使用过大的内存实例、规避fork操作缓慢的操作系统

##### AOF刷盘阻塞

为了保证性能和防止数据丢失，一般采用每秒执行一次刷盘，后台线程每秒对AOF文件做fsync操作。当磁盘负载过高，fsync需要进行等待，直到写入完成

Redis主线程中如果发现距上次fsync超过2秒，为了数据安全性，主线程会进行阻塞直到fsync操作完成

使用info persistence命令获取中aof_delayed_fsync指标

使用iotop查看具体哪个进程消耗过多的硬盘资源

##### HugePage写操作阻塞

内存页的大小由4k变为2M

降低fork操作时复制主进程的页表开销

每次写命令引起的复制内存页单位由4k变为2M，拖慢写操作的执行时间

Redis中都是随机内存访问，不会涉及连续的内存访问，不会享受到HugePage带来的益处

## 性能优化

获取redis的基线性能

redis-cli 命令提供了–intrinsic-latency 选项，可以用来监测和统计测试期间内的最大延迟，这个延迟可以作为 Redis 的基线性能。其中，测试时长可以用–intrinsic-latency 选项的参数来指定。

latency monitor 监控工具

监控 Redis 运行过程中的峰值延迟情况

要使用 latency monitor，首先要设置命令执行时长的阈值

开启慢查询，根据业务需求替换慢查询的命令

slowlog-log-slower-than：慢查询日志对执行时间大于多少微秒的命令进行记录

slowlog-max-len：慢查询日志最多能记录多少条命令记录

避免设置相同的过期时间，在每个key的过期时间上加一个随机数

避免了同一时间大量的key失效

​		默认配置下，每100毫秒执行一次扫描过期的key，如果过期的key的数量少于25%，停止扫描，否则继续扫描

避免对bigkey的直接删除，4.0以后的版本可以利用异步线程删除。4.0以前的版本可以用scan迭代删除

写AOF文件时的write、fdatasync系统调用导致的延迟

​	合理配置AOF刷盘策略，采用固态硬盘作为AOF日志的写入设备

输出缓冲区已满，内核需要刷新磁盘以接受新的写操作时导致write阻塞

磁盘负载过高的情况下导致fdatasync阻塞

禁用大页，避免写时复制时的大量拷贝

echo never /sys/kernel/mm/transparent_hugepage/enabled

redis单独部署，避免在同一台机器部署过多应用，导致对cpu、内存、网络资源的争抢

运行主从集群时，控制主库的数据量在2~4G，避免主从复制时，从库因加载过大的RDB文件而阻塞

将网络中断处理程序和redis实例绑定到同一个物理核

使用scan、sscan、hscan代替keys命令

Linux配置优化

内存分配

设置vm.overcommit_memory=1,表示允许超量使用内存直到用完为止

设置合理的maxmemory，保证机器有20%-30%的闲置内存

swap

当物理内存不足时，可以将一部分内存页进行swap操作，swap空间由硬盘提供。对与高并发、高吞吐的应用，磁盘IO通常会成为系统的瓶颈

是否使用swap操作由参数swappiness决定，取值范围0-100，值越大使用swap的概率越高，默认为60

Linux3.5版本以上，vm.swappiness=1，宁愿使用swap，也不用OOM killer（物理内存充足时，使redis足够快。物理内存不足时，避免redis挂掉）

free -m(以兆为单位)查看swap总体情况

vmstat 1实时查看swap（每个一秒输出一次）

cat /proc/{PID}/smaps | grep swap 查看指定进程的swap使用情况

增大机器内存，如果处于集群模式下，增加机器实例，将数据分摊到其他机器

禁用THP

# Codis

## codis server

Codis 项目维护的一个 Redis 分支, 基于 2.8.21 开发, 加入了 slot 的支持和原子的数据迁移指令.

## codis proxy

客户端连接的 Redis 代理服务, 实现了 Redis 协议,codis-proxy 本身是无状态的.灵活方便的进行扩缩容

## zookeeper

存放数据路由表和 codis-proxy 节点的元信息, codis-config 发起的命令都会通过 ZooKeeper 同步到各个存活的 codis-proxy.

## codis dashboard

直接在浏览器上观察 Codis 集群的运行状态

## codis config

管理工具, 支持包括, 添加/删除 Redis 节点, 添加/删除 Proxy 节点, 发起数据迁移等操作.

# twemproxy

Twitter开源，分片资源访问的代理组件，封装后端资源池及 hash 规则

让client访问尽可能简单，后端资源增加或减少时，只要在 Twemproxy 变更即可

Twemproxy 跟后端通过单个长连接访问，避免了众多的client与后端资源直接建立连接，导致后端资源维护众多的连接，减轻后端资源的连接压力

# Guava cache

优势

缓存过期和淘汰机制

可以设置Key的过期时间，包括访问过期和创建过期

在缓存容量达到指定大小时，采用LRU的方式，将不常使用的键值从Cache中删除

并发处理能力

提供了设置并发级别的api，使得缓存支持并发的写入和读取，采用分离锁机制，分离锁能够减小锁力度，提升并发能力

更新锁定

在高并发下会出现，多次查源并重复回填缓存，可能会造成源的宕机。在CacheLoader的load方法中加以控制，对同一个key，只让一个请求去读源并 回填缓存，其他请求阻塞等

集成数据源

集成数据源，在从缓存中读取不到时可以从数据源中读取数据并回填缓存

监控缓存加载/命中情况

CacheLoader

当缓存不存在时能够自动加载数据到缓存中

缓存数据的删除

被动删除

基于数据大小的删除(LRU+FIFO)

基于过期时间的删除

隔多长时间后没有被访问过的key被删除（expireAfterAccess）

写入多长时间后过期（expireAfterWrite）

基于引用的删除

主动删除

单独删除

批量删除

清空所有数据

# Tair

## ConfigServer

存储数据在集群中的分布信息

管理dataserver，通过心跳维护dataserver的状态信息

采用一主一备的方式保证可靠性

基于一致性Hash算法存储数据，将固定数量的桶(bucket) 设置为Hash环节点，hash(key) 顺时针寻找桶， 桶是负载均衡和数据迁移的基本单位

创建快照表，保存桶在各节点的分布情况

负载均衡优先

尽量的把桶均匀的分布到各个data server上

位置安全优先

一个桶的备份分布在不同的机房、同一机房的不同机架

根据一定的策略把每个桶分配到不同的data server上，尽量保证桶分布的均衡性

当dataserver节点增加或减少时，负责桶的迁移

支持自定义的备份数，提高数据的可靠性

## Dataserver 

负责数据的存储，复制和迁移

## 读热点方案

在DataServer上划分一块HotZone存储区域的方式来解决热点数据的访问

每个HotZone都存储相同的读热点数据

客户端对热点数据Key的请求会随机到任意一台DataServer的HotZone区域，这样单点的热点请求就被散列到多个节点乃至整个集群

客户端在第一次请求前初始化时，会获取整个Tair集群的节点信息以及完整的数据路由表，同时也会获取配置的热点散列机器数（即客户端访问的HotZone的节点范围）

客户端随机选择一个HotZone区域作为自身固定的读写HotZone区域。在DataServer数量和散列机器数配置未发生变化的情况下，不会改变选择。即每个客户端只访问唯一的HotZone区域

客户端收到服务端反馈的热点Key信息后，至少在客户端生效N秒

客户端首先请求HotZone节点，如果数据不存在，则继续请求源数据节点，获取数据后异步将数据存储到HotZone节点里

## 写热点方案

热点Key的写请求在IO线程被分发到专门的热点合并线程处理，该线程根据Key对写请求进行一定时间内的合并，随后由定时线程按照预设的合并周期将合并后的请求提交到引擎层

合并过程中请求结果暂时不返回给客户端，等请求合并写入引擎成功后统一返回

# NUMA为什么出现

在早期的多处理器系统中，所有CPU都共享同一个物理内存，这种内存访问方式会导致处理器争夺内存带宽，从而影响系统的性能
在当前主流的NUMA架构下;每个 CPU 有自己的本地内存。访问本地内存有更快的速度。而访问其他 CPU 内存会导致较大的延迟
通过命令numactl -H来查看 NUMA 硬件信息

# Pika

## 大内存 Redis 潜在问题

实例内存容量大，RDB 文件也会相应增大，RDB文件生成时的fork时长就会增加，这就会导致Redis实例阻塞

RDB文件增大后，使用RDB进行恢复的时长也会增加，会导致Redis较长时间无法对外提供服

 RDB 文件增大后，导致全量同步的时长增加，可能会导致复制缓冲区溢出。缓冲区溢出了，主从节点间就会又开始全量同步

RDB 文件很大，也会导致主从切换的过程耗时增加，同样会影响业务的可用性

## 整体架构

### 线程模块

#### 请求分发线程

专门监听网络端口，一旦接收到客户端的连接请求后，就和客户端建立连接，并把连接交由工作线程处理

#### 工作线程

负责接收客户端连接上发送的具体命令请求，并把命令请求封装成 Task，再交给线程池中的线程

#### 线程池

进行实际的数据存取处理，处理Task

### binlog

 binlog 本质是顺序写文件, 通过Index + offset 进行同步点检查

实现增量命令同步，既节省了内存，还避免了缓冲区溢出

Binlog文件固定大小为100MB，每个Binlog文件由多个Block组成，每个Block大小固定为64KB，每一个写redis命令称为一个Record。一个Record可以分布在多个Block中

一个Record包括三部分：Length（3字节）+type（1字节）+CMD（Length字节），如果一个Record可以在一个block全部放下，type为FullType，否则就要进行拆分到多个block，type类型依次为FirstType + 若干个MiddleType+LastType

### Nemo存储模块

实现了 Pika 和 Redis 的数据类型兼容，当把Redis服务迁移到Pika时，不用修改业务应用中操作 Redis 的代码

nemo本质上是对rocksdb的改造和封装，使其支持多数据结构的存储（rocksdb只支持kv存储）。总的来说，nemo支持五种数据结构类型的存储：KV键值对（为了区分，nemo的的键值对结构用大写的“KV”表示）、Hash结构、List结构、Set结构和ZSet结构

### RocksDB

基于SSD保存数据

需要保存数据时，RocksDB 会使用两小块内存空间（Memtable1 和Memtable2）来交替缓存写入的数据。当有数据要写入，会先把数据写入到Memtable1。等到 Memtable1写满后，RocksDB 再把数据以文件的形式，快速写入底层的 SSD。同时使用 Memtable2 来代替 Memtable1，缓存新写入的数据。等到 Memtable1 的数据都写入SSD了，RocksDB 会在Memtable2写满后，再Memtable1缓存新写入的数据。

### 如何实现数据类型兼容

#### string

对Redis的String类型来说，本身就是单值的键值对，直接用 RocksDB 保存就行

#### list

数据信息

Key：l(1字节)+size(1字节)+key+sequence
Value:pre seq+next seq+ value +version+ttl

元信息

Key:L(1字节)+key+version+ttl
Value:length+left seq+right seq+cur seq   

list中的数据会被移除，需要记下left、right

#### set

Set集合的key 和元素member值，都被嵌入到了 Pika 单值键值对的键当中。单值键值对的key前面有值“s”，用来表示数据是 Set 类型，同时还有 size 字段，用来表示 key 的大小

Pika 单值键值对的值只保存了数据的版本信息和剩余存活时间

数据信息

Key:s+size+key+member
Value:version+ttl

元信息

Key:S+key+version+ttl
Value:length(元素的个数)

#### hash

Hash集合的key 和元素的field被嵌入到单值键值对的键当中，

Hash集合元素的value则是嵌入到单值键值对的值当中，并且带有版本信息和剩余存活时间

元信息

key:H(1字节)+key+version+ttl
value:(field的数量)

数据信息

key：h(1字节)+ size（1字节）+ key + filed
value:value+version+ttl

#### sorted set

Sorted Set集合key、元素的 score 和 member 值都嵌入到了单值键值对的键当中

单值键值对中的值只保存了数据的版本信息和剩余存活时间

Key:z+size+key+member
Value:score+version+ttl

Key:z+size+key+score+member
Value:version+ttl

Key:Z+key+version+ttl
Value:length(元素的个数)

## 优势

 单实例能保存更多的数据了，实现了实例数据扩容

实例重启快

无需担心复制缓冲区的溢出触发从发的全量复制

## 不足

当把数据保存到 SSD 上后，会降低数据的访问性能

# memcache

## 网络模型

### 主线程

监听客户端连接请求，当有连接到来时，将连接调度给工作线程

### 工作线程

读取数据

解析命令

执行命令

写回响应

## 特性

高性能，支持百万级QPS

协议简单，set、get、touch

存储结构简单，只支持单key、value

完全基于内存操作

服务器间不通信

## 哈希表

快速定位 key，通过链表解决Hash冲突的问题

扩容时，会使用新旧两张哈希表，后台维护线程将旧表的数据迁移到新表。

迁移过程中，根据迁移位置，用户请求会同时查旧表和新的主表

## 分段LRU

### TEMP_LRU 

存放过期时间短的数据

配置项:temporary_ttl=N，如果TTL小于或等于这个值，则使用TEMP LRU，否则放入HOT_LRU

该列队中的 Item 永远不会发生在队列内搬运，也不会迁移到其他队列

### HOT_LRU 

存放第一次放入的数据，也就是最新放入的数据

对于 HOT LRU，内部不搬运

当队列满时，如果队尾Item被访问过，那么会迁移到WARM队列，否则迁移到 COLD 队列。

### WARM_LRU

存放多次访问的数据

如果队列的 Item 被再次访问，就搬到队首，否则迁移到 COLD 队列

### COLD_LRU

存放的是最不活跃的数据

一旦内存满了，队尾的Item会被剔除。

如果COLD LRU里的Item被再次访问，会迁移到WARM LRU

## slab内存分配机制

避免内存碎片越来越多，减轻系统内存管理器的负担

根据需要不断分配默认大小为 1MB 的 slab，每个 slab 又被分为相同大小的 chunk。

默认创建64 个slabclass，slabclass会按照增长因子逐步增大chunk size，具体数值会进一步对8取整。默认的增长因子是 1.25，启动时可以通过 -f 将增长因子设为其他值；最小chunk size为102字节

存储数据时会选择最接近的slabclass，所以chunk浪费的字节非常有限

## 应用

### 分拆缓存池

根据业务将数据分拆到独立Mc资源池

在每个资源池采用合适的分布算法

### Master-Slave 两级架构

 master节点异常后，由slave来承接外部请求

分布方式可以采用哈希取模方案，可以避免一致性hash在rehash后产生的脏数据问题

### M-S-L1 架构

80% 的请求会集中在 20% 的数据上

L1层应对热数据的访问

## 适合场景

纯KV、数据量大、并发量大时选择memcache

# 缓存相关的问题

## 数据倾斜

### 数据量倾斜

存在BigKey

​	避免创建Bigkey

​	集合类型的拆分为多个小集合，分散存储

Slot分配不均

​	避免把过多slot分配到一个实例上

​	监控每个实例维护的slot，维护的slot过多进行数据迁移

使用HashTag导致数据倾斜

​		Hash Tag 是指加在键值对 key 中的一对花括号{}，客户端在计算 key 的 CRC16 值时，只对 Hash Tag 花括号中的 key 内容进行计算。如果没用 Hash Tag 的话，客户端计算整个 key 的 CRC16 的值

​	避免使用hashtag

### 数据访问倾斜

存在热点数据

​	多副本分散存储

​	多级缓存存放

## Hot Key（读热点）

### 问题描述

突发的热门事件，特定的Key所在的cache节点服务过载，卡顿

预估相关的key，借助大数据离线批处理（Hadoop）、实时计算（Storm、SparkStreaming）统计热点key

### 解决方案

热点key分散：hotkey#1，hotkey#2 ...hotket#n

相同key分散，多级缓存，多slave；避免相同的热点数据访问相同的cache节点

业务端本地缓存记录热点数据、nginx缓存（需要注意缓存一致性的问题）

## 写热点

例如秒杀场景下对库存的频繁修改

合并写

并行写变串行写，将写请求存放到redis队列或者消息中间件，开启消费线程进行处理

## Big Key

### 问题描述

部分key的value过大，读写、加载容易超时；如果value过大的key被频繁访问带宽网卡容易被打满

### 如何发现

redis-cli -h 127.0.0.1 -p 7001 –-bigkeys -i 0.1
每隔100条scan指令就会休眠0.1s，主从复制模式下，建议在从库执行此命令

### 解决方案

拆分为多个小key

尽量避免过期或删除

启用压缩

## 缓存穿透

### 问题描述

查询一个不存在的key，每次访问都穿刺到DB，对DB造成压力

### 解决方案

1、不存在的key，在缓存中设置默认值。如果不存在的key太多，导致占用过多的缓存空间

2、构建BloomFilter过滤器，过滤非法的key

3、规范key的格式，对于不符合一定规则的key操作进行过滤

## 缓存雪崩

### 问题描述

大量数据同时失效

Redis缓存实例发生故障宕机

### 解决方案

1、不同的key，设置不同的过期时间，让缓存失效的时间点尽量分散

2、多副本cache架构，任何cache池miss后，读其他cache副本

3、某个key只允许一个线程查询数据库，其他线程等待。可通过分布式锁实现，如果服务实例不是太多，也可以通过单机锁实现

4、如果访问缓存超时，进行服务熔断

5、对于数据库的访问进行限流

6、多级缓存，nginx缓存->本地缓存->redis缓存

## 缓存击穿

### 问题描述

某一热点key过期，缓存未命中，穿透到数据库，数据库压力大增，导致查询变慢

### 解决方案

多线程进行查询，可以给第一个请求加互斥锁然后其他请求排队等待；使用redis的setnx互斥锁进行判断

设置热点key永不过期

## 数据一致性问题

### 问题描述

cache中的数据和DB中的不一致

一致性hash算法,在节点多次上下线以后，也会产生脏数据

### 解决方案

1、先修改数据库，再删除缓存

2、修改数据时，同时淘汰缓存中的数据，为避免删除缓存失败，可以进行重试，删除失败写入日志

3、更新数据库后，异步更新缓存。使用MQ+Cannl实现

# 缓存的监控

缓存命中率

缓存使用率

慢日志

延迟

客户端连接数

热命令、热数据

# 缓存的高可用设计

分片

数据冗余

故障转移/主从自动切换

负载均衡

一致性保证

# 缓存基本思想

时间局部性原理：被获取一次的数据将来还会被获取

以空间换时间：开辟一块高速独立的空间，提供高效访问

# 缓存三种模式

## Cache Aside（旁路缓存）

写操作，更新DB，删除Cache

读操作，Miss后读取DB+回写

场景，强一致性，Cache数据构建复杂

## Read/Write Through（读写穿透）

写操作，Cache不存在，更新DB；Cache存在更新Cache+DB

读操作，Miss后由缓存服务加载并回写Cache

场景，数据有冷热区分；对于活跃的数据，往往会在Cache中，直接更行Cache，在写DB；对于不活跃的数据往往不存在Cache中，直接更新DB

## Write Behind Caching(异步缓存写入)

写操作，只更新缓存，缓存服务异步更新DB

读操作，缓存Miss后由缓存服务加载+写Cache

特点，写性能高，定期异步刷新；但是存在数据丢失的概率

场景，写频率超高，需要合并

# 缓存的引入以及设计方案

## 选择Cache组件

本地缓存

开源：Redis、Memcache、Pika

自研、开源的二次开发

## 设计Cache数据结构

KV、Json、Protocol Buffer

## Cache分布

算法：取模或者一致性Hash

实施：Client或者Proxy

迁移：Proxy 或者Server

## Cache架构部署及管理

分池：业务数据分离，独立不干扰

分层：高可用

分IDC：跨IDC多谢，DataBus/Data Trigger

异构：不同缓存组件组合

服务化：集群管理、监控、自动化运维

## 设计架构时的考量点

### 读写方式

只整体读写Value

需要部分读写以及变更

### KV size

Size过大，拆分成多个value

Size差异过大的业务数据分置不同的缓存

### Key 数量

数据量小时，容量足够，进行全量缓存

数量大、海量，缓存频繁访问的热数据

### 读写峰值

Cache分池

Cache分层

远程Cache+本地Cache

### 命中率

持续监控，恰当的分布式策略

自动故障转移、数据备份

### 过期策略

# 引入缓存的代价

系统复杂度提升，稳定性降低

成本增加

出现数据一致性问题

# 热点key解决方案

## 京东[HotKey](https://gitee.com/mirrors/Tair.git)

### etcd集群

​		存放配置热key的规则、worker地址

### worker节点

​		通过滑动窗口探测热点key，将热点key推送给服务实例

### 服务实例

​		启动从etcd拉取worker地址，建立连接，拉取热key规则，将符合规则的key上报给worker节点（对key计算hash，根据worker节点的数量取模）；接收worker节点推送的热key，将数据缓存再本机

## [淘宝Tair](https://gitee.com/mirrors/Tair.git)

### DataServer端

​		在DataServer上划分一块HotZone存储区域存放热点数据，每个HotZone都存储相同的读热点数据
对于Tair来说，每个key的访问都会落到同一数据节点DataServer上，由DataServer探测热点key，将热点推送其他节点的HotZone中

### 客户端

​		客户端在第一次请求前初始化时，会获取配置HotZone的数据节点
​		客户端随机选择一个HotZone区域作为自身固定的读写HotZone区域（挑选前将数据节点地址打散）
​		在DataServer未发生变化的情况下，不会改变选择。即每个客户端只访问唯一的HotZone区域
​		客户端收到服务端反馈的热点Key信息后，至少在客户端生效N秒
​		客户端首先请求HotZone节点，如果数据不存在，则继续请求源数据节点，获取数据后异步将数据存储到HotZone节点里

## [饿了么samaritan](https://github.com/samaritan-proxy/samaritan.git)

### 设计前提

​			类似于Codis，都是中间层代理
​			所有的 Redis 请求都是经过代理 Samaritan，由代理层进行收集上报热点key
​			不️依赖任何外部组件，外部聚和组件的初衷是为了聚合不同机器的数据.基于局部可以代表整体的原则，那么单实例上的热 key 就可以代表全局的一个情况

### 实现

​			每个代理会有一个全局的 Hotkey Collector
​			每个Redis的连接维护自己独立的 Counter，Counter 采用LFU 算法（只保留访问频次最高的key）
​			Collector 会定时地去收集每个 Counter 的数据并进行聚合，聚合的时候不会使用真实的计数，而是使用概率计数，并且为了适应访问模式的变化 counter 的值会随着时间衰减

### 注意

​			当前的方案只能够快速定位系统中的热 key，但并没有真正解决热 key 本身带来的问题，仍旧需要业务方自行改造或者将那些热点 key 调度到单独的节点上

# 得物 Redis 设计与实践

## 自建 Redis 架构与核心组件

自建 Redis 分布式 KV 缓存系统由 ConfigServer、Redis-Proxy、Redis-Server 等核心组件构成。

类似于Codis，只不过redis的高可用是通过ConfigServer实现，而不是redis-sentinel

### ConfigServer

ConfigServer 主要负责三方面职责

负责 Proxy 添加与删除、Group 创建与删除、Redis-Server 实例添加与删除、Redis-Server 实例手动主从切换、水平扩容与数据迁移等功能操作

集群拓扑发生变化时，主动向 Redi-Proxy 更新集群拓扑

负责 Redis-Server 实例故障检测与自动故障转移

与codis类似，也有group的概念，每个group下一个master redis，多个slaveredis，每个group负责一部分的slot

#### 故障检测

ConfigServer 会对每一个 Group 的 Master 节点进行定期探活，如果发现某一个 Group 的 Master 节点不可用，就会执行 Failover 流程，选择该 Group 内一个可用的 Slave 节点提升为新的 Master 节点，保证该 Group 可继续对外提供服务

每个 ConfigServer 定期给该实例的所有 Redis-Server 节点发送 Ping 和 Info 命令，用于检测节点的可用性和发现新的从节点

当 ConfigServer 向 Redis-Server 节点发送命令超时时，将节点标记为主观下线，并传播节点的主观下线状态

ConfigServer Leader 节点发现某节点处于主观下线时，会主动查询其他 ConfigServer 对该节点的状态判定，如果多数 ConfigServer 节点都将该 Redis-Server 节点标记为主观下线，则 Leader 节点将该 Redis 节点标记为客观下线，并发起故障转移流程

#### 故障转移

从故障 Redis 主节点的所有从节点中选择一个最优的从节点，选择策略包含

过滤掉不健康的从节点，比如处于主观下线或者客观下线状态

选择 Slave-Priority 最高的从节点

选择复制偏移量最大的从节点

选择 Runid 最小的从节点

将选取出来的从节点提升为新的主节点，即向该节点执行 `slaveof no one` 命令

将其他从节点设置为新主节点的从节点

并保持对旧主节点的状态关注，如果旧主节点恢复，将旧主节点也更新为新主节点的从节点

### redis-proxy

Redis-Proxy 组件是自建 Redis 系统中的代理服务，负责接受客户端连接，然后转发客户端命令到后端相应的 Redis-Server 节点，使得后端 Redis-Server 集群部署架构对业务透明

Proxy 是一个无状态服务，可以很方便的进行水平扩容，提高业务访问自建 Redis 系统的 QPS

在自建 Redis 中，每个集群可能包含多个 Group，每个 Group 对应一组主从 Redis-Server 节点，每个 Group 负责一部分 Key，同时，整个集群划分为 1024 个槽（slot），每个 Group 负责其中一部分槽（slot），用户写入的 Key 通过以下算法来计算对应的 slot，然后存储到对应节点。

slot 计算公式：` slot(key) = crc32(key) % 1024`

同城双活

自建 Redis 为了保证数据的高可用和高可靠，每个 Redis 实例中每个分组采用至少一主一从的部署方案，且主从节点分别部署在不同可用区，在默认的访问模式下，业务读写都访问主节点，从节点仅仅作为热备节点

为了提升业务在多可用区部署场景下访问缓存性能与降低延迟，以及最大化利用从节点的价值，自建 Redis-Proxy 支持同城双活功能。自建 Redis 同城双活采用**单写就近读**的方案实现

redis-server采用至少一主一从的部署方案，并且主从节点跨可用区部署，分别部署在与业务对应的可用区

Redis-Proxy 同样采用多可用区部署，与业务可用区相同。各可用区 Proxy 将写请求自动路由到主节点，依然写主节点。读请求优先就近访问本可用区从节点，本可用区无可用从节点时，支持自动访问主节点或优先访问其他可用区从节点。

### Redis-Server

edis-Server 组件为开源 Redis 版本基础上，增加槽 slot 同步迁移与异步迁移等相关功能；支持原生开源 Redis 的所有特性

Share-Nothing 架构：自建 Redis 系统中，Redis-Server 服务采用集群化部署，整个集群由多个 Group 共同组成，每个 Group 中包含一主 N 从多个 Redis-Server 实例，group 之间的 Redis-Server 节点相互没有通信，为 Share-Nothing 架构。同时，整个集群划分为 1024 个槽（slot），每个 Group 负责其中一部分槽（slot），用户写入的 Key 通过上面提到的算法来计算对应的 slot，然后存储到负责该 slot 的 Group 中的 Redis-Server 节点上。查询 Key 时，通过同样的算法去对应的节点上查询

#### Async-Fork 特性

在 Redis 中，在 AOF 文件重写、生成 RDB 备份文件以及主从全量同步过程中，都需要使用系统调用 Fork 创建一个子进程来获取内存数据快照，在 Fork() 函数创建子进程的时候，内核会把父进程的「页表」复制一份给子进程，如果页表很大，在现有常见操作系统中，复制页表的过程耗时会非常长，那么在此期间，业务访问 Redis 读写延迟会大幅增加

自建 Redis 系统中，Redis-Server 通过优化并适配最新的支持 Async-Fork 特性的操作系统，极大的提升 Fork 操作性能

**Async-fork，将 fork 调用过程中最耗时的页表拷贝部分从父进程移动到子进程，父进程因而可以快速返回用户态处理用户查询，子进程则在此期间完成页表拷贝，从而减少  fork 期间到达请求的尾延迟**

#### 数据迁移

当进行水平扩容时，需要重新分配 slot 的分布，并且将一部分 slot 从原有节点迁移到新节点，同时将由 slot 负责的数据一起迁移到新节点

数据迁移过程包括在源节点将 Key 对应的 Value 使用 Dump 命令进行序列化，然后将数据发送到目标节点，目标节点使用 Restore 命令将 Key 和 Value 存储到本地 Redis 中，最后源节点将该 Key 删除

**同步迁移**：在上述整个迁移过程中，源端的 Migrate 命令处于阻塞状态，直到目标端成功加载数据，并且返回成功，源节点删除该 Key 后，Migrate 命令才响应客户端成功。由于 Redis 数据操作是一个单线程模型，所有命令的处理的都在主线程中处理，因此 Migrate 命令会极大地影响其他正常业务的访问。

**异步迁移**：Migrate 命令仅阻塞 Dump 序列化数据、并且异步发送到目标节点，然后即返回客户端成功；目标节点接收到数据后，使用 Restore 命令进行保存，保存成功后，主动给源节点发送一个 ACK 命令，通知源节点将迁移的 Key 删除。异步迁移减少源端 Migrate 命令的阻塞时间，减少了 slot 迁移过程中对业务的影响。

## 实例扩容

**垂直扩容**，即动态修改单节点的 Maxmemory 参数，提高单节点的容量。

**水平扩容**，即增加实例的 Group 数量，对应增加主从节点数量，然后重新进行 slot 分配和对应的数据迁移动作

一般来说，当单节点规格小于 4G 时，会优先考虑垂直扩容，简单快速，对业务无任何影响

## 诊断与分析

关键指标包括：Proxy CPU、Redis CPU、Redis 内存使用率、Proxy 占用内存、Proxy GC 次数、最大 RT、Redis 流入流量、Redis 流出流量等

**慢日志**：运维平台支持 Redis-Server 记录的慢日志和 Redis-proxy 记录的慢日志查询

**RDB 离线分析：**运维平台支持生成 RDB 文件，并自动进行数据离线分析。分析结果包含 Top100 Value 最大的 Key 和元素个数最多的 Key；每种类型 Key，不同 Key 前缀包含的 Key 数量等

**rdb解析工具**

git clone https://github.com/sripathikrishnan/redis-rdb-tools

cd redis-rdb-tools 

sudo python setup.py install

以json格式打印

rdb -c json /var/redis/6379/dump.rdb

过滤出以user开头

rdb --command justkeyvals --key "user.*" /var/redis/6379/dump.rdb

生成内存报告

`--bytes C` and `'--largest N` can be used to limit output to keys larger than C bytes, or the N largest keys

rdb -c memory /var/redis/6379/dump.rdb --bytes 128 -f memory.csv

查找单个key占用的内存

redis-memory-for-key person:1

redis-memory-for-key -s localhost -p 6379 -a mypassword person:1



性能测试

在使用 Redis-benchmark 压测的过程中，手动执行 bgsave 命令，观察 fork 耗时和压测指标 TP100

-d 指定value的大小

-t 指定测试命令

-n 指定发出的请求总数

-c指定并发连接数，默认50

-r 指定key是否随机生成

-P 是否使用pipeline功能，即一次命令发送的请求个数

Redis-benchmark -d 256 -t set -n 1000000  -a xxxxxx -p 6380

strace 常用来跟踪进程执行时的系统调用和所接收的信号

strace -p 32088 -T -tt -o strace00.out





多级页表（*Multi-Level Page Table*）：解决存储进程页表项占用大量内存空间的问题

32位操作系统

将页表（一级页表）分为 1024 个页表（二级页表），每个二级页表中包含 1024 个「页表项」，形成二级分页

一级页表就可以覆盖整个 4GB 虚拟地址空间，但如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表

也就是，内存中只需要保存一级页表以及使用到的二级页表，大量的未被使用的二级页表则不需要分配内存并加载在内存中，因此，达到节省页表占用内存空间的目的

64位系统

四级分页目录，分别是：

页全局目录项 PGD（*Page Global Directory*）

页上级目录项 PUD（*Page Upper Directory*）

页中间目录项 PMD（*Page Middle Directory*）

页表项 PTE（*Page Table Entry*）



fork原理

在默认 fork 的调用过程中，父进程需要将许多进程元数据（例如文件描述符、信号量、页表等）复制到子进程，而页表的复制是其中最耗时的部分（占据 fork 调用耗时的 97% 以上）

Linux 的 fork() 使用写时拷贝 (copy-on-write) 页的方式实现。写时拷贝是一种可以推迟甚至避免拷贝数据的技术。在创建子进程的过程中，操作系统会把父进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，此时，操作系统并不复制整个进程的物理内存，而是让父子进程共享同一个物理内存。同时，操作系统内核会把共享的所有的内存页的权限都设为 read-only

写时复制

当父进程或者子进程在向共享内存发起写操作时，内存管理单元 MMU 检测到内存页是 read-only 的，于是触发缺页中断异常（page-fault），处理器会从中断描述符表（IDT）中获取到对应的处理程序。在中断程序中，内核就会**把触发异常的物理内存页复制一份**，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，于是父子进程各自持有独立的一份，之后进程才会对内存进行写操作，这个过程也被称为写时复制（Copy On Write）



设计思想

1、根据字符串的长度设计不同类型的结构头，灵活保存不同大小的字符串，从而有效节省内存空间

2、使用了专⻔的编译优化来节省内存空间，在struct和sdshdr8之间使用了__attribute__((__packed__))，告诉编译器不要使用字节对齐的方式，而采用紧凑的方式分配内存

默认情况况下，编译器会按照8字节对⻬的方式，给变量分配内存。也就是说，即使一个变量的大小不到8个字节，编译器也会给它分配8个字节

3、渐进式rehash，每次仅迁移有限个数的bucket，避免一次性迁移给所有bucket带来的性能影响

4、如何提升内存使用效率？

使用连续的内存空间用连续的内存空间，避免内存碎片开销

针对不同⻓度的数据，采用不同大小的元数据针对不同⻓度的数据，采用不同大小的元数据，以避免使用统一大小的元数据，造成内存空间的浪费。
使用共享对象使用共享对象其实可以避免重复创建冗余的数据，从而也可以有效地节省内存空间。不过，共享对象主要适用于只读场景只读场景，如果一个字符串被反复地修改，就无法被多个请求共享访问了
