存储

https://zhuanlan.zhihu.com/nosql（NoSQL技术剖析）
https://www.zhihu.com/column/distributed-storage（分布式和存储的那些事）
[时序数据库技术体系 – InfluxDB TSM存储引擎之TSMFile](https://www.cnblogs.com/hetonghai/p/8594533.html)	
[InfluxDB详解之TSM存储引擎解析](http://blog.fatedier.com/2016/08/05/detailed-in-influxdb-tsm-storage-engine-one/)
[时序数据库连载系列: 时序数据库一哥InfluxDB之存储机制解析](https://zhuanlan.zhihu.com/p/57745927	)



# 存储引擎

## B+树存储引擎

特性

根节点常驻内存，最多h-1次磁盘IO

非叶子节点容纳尽量多的元素

擅长范围查询

随机写问题

节点分裂问题

代表产品	MySQL

## LSM树存储引擎

### 写流程

首先将数据写入 WAL log

写到内存中mutable memtable（skiplist）

如果mutable memtable到达一定的大小，转换成immutable memtable

执行Minor Compaction（Flushing）将immutable memtable转换成 level 0 sstable

后台根据一定的策略对不同level的sstable进行compaction

### 合并的作用

丢弃不再被使用的旧版本数据

提升读性能

### write长尾延迟

#### 产生原因

Level0文件过多，Flushing就会停止，造成client卡顿

底层的合并过多，占用大量的磁盘资源，就造成写WAL、flushing延迟

#### 解决方案

优先flushing和低级别的合并，低level的合并优先于高level的合并，

根据系统的负载，给不同的操作设置不同的磁盘带宽

### 存在问题

### 读放大

读取时，首先读取memoryTable，如果读取不到，需要查找所在的sstable，由于sstable分层，因此该过程要由低向高level逐渐查找，在极端情况下，需要遍历所有sstable文件

### 写放大

写入时，除了将数据顺序写入日志外，还考虑将memtable写入sstable文件，放到level 0以及sstable合并问题





### 代表产品

[SILK](https://www.usenix.org/system/files/atc19-balmau.pdf)

Preventing Latency Spikes in Log-Structured Merge Key-Value Stores

在低负载时期分配更多的IO带宽给内部操作
在client的流量洪峰时会减少Higher level Compaction的IO带宽占用，在client 流量低峰时增加Higher Level的Compactions的带宽

对LSM tree的internal ops进行优先级调度
Flush的优先级>L0 -> L1 Compaction优先级>Higher Level Compaction的优先级

#### LevelDB

http://bean-li.github.io/tags/#leveldb

https://kernelmaker.github.io/

https://youjiali1995.github.io/categories/

#### [PebblesDB](https://www.cs.utexas.edu/~vijay/papers/sosp17-pebblesdb.pdf)

Building Key-Value Stores using Fragmented Log-Structured Merge Trees

Compaction 造成了一些 key-value 反复的 rewrite，从而造成了 LSM-tree 的 write amplification，针对write amplification 做优化

从skip list借鉴了思想，引入guard概念
其将每一层进行分段，每个段称为一个guard，guard之间没有重叠的key，且每层的guard之间要求保序，但是guard内部可以无序

LSM造成写放大的原因是Level n和Level n+1合并时所有重叠的文件都需要重写一次

引入guard后，会以guard为单位进行合并，如果文件跨多个guard会进行重写，重写后的文件移到对应的guard，否则将其直接移到所属的guard，guard内部的文件是不需要排序的

#### RocksDB

https://zhuanlan.zhihu.com/p/77543818

https://blog.csdn.net/Z_Stand/article/details/109683804

https://kernelmaker.github.io/

https://youjiali1995.github.io/categories/

#### [Wisckey](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf)

Key、Value分开存储

Key的元数据信息（所属的文件以及文件中的位置）用LSM存储
Value按照顺序追加的方式存储在ValueLog文件中，但是带来了随机IO，可以搭配SSD
如果Value的占用空间小于一定阈值，可以将其和key同时存储在LSM中，避免了随机IO

只有元数据信息存储在LSM中，虽然也存在IO放大，但是因为数据量小，所以放大的效果明显降低
适用于Value比较大的场景

[TerarkDB](https://github.com/krareT/trkdb)

[goleveldb](https://github.com/syndtr/goleveldb)

[HashKV](https://www.usenix.org/system/files/conference/atc18/atc18-chan.pdf)

### 特性

随机写变顺序写

牺牲读性能

增加后台合并开销

### 读优化

布隆过滤器

​	通过多个 Hash 函数将一个数值映射到某几个字节上。这样用少量的字节就可以存储大量的数值，同时能快速地判断某个数值是否存在
​	虽然没有做映射的数值会有一定概率的误报，但可以保证“数值不存在”是绝对准确的

文件内部有序

## 哈希存储引擎

基于哈希表结构的键值存储系统

所有的写操作只追加而不修改老的数据
每个文件都有大小限制，当文件增加到相应的大小时，就会产生一个新的文件

内存中通过哈希表存储索引信息，包括主键（key）、文件名称、value位置、时间戳

文件内容结构：crc、时间戳、key的长度、value的长度、key的值、value的值、标志（是否删除）

定期合并。删除无效的数据，释放存储空间；

快速恢复，索引存储在内存中，当重启时需要扫描一遍数据文件，当文件过多时，会耗费过多的时间
通过索引文件(hint file)来提高重建索引的速度；与数据文件相似，hintfile不会存储value的内容。只会存储value的位置；

特性

​	时间复杂度O(1)

​	不支持范围查询

​	不支持排序

开源实现

​	bitcask

​	借鉴了Bicask模型，用adaptive radix tree 代替哈希表存储索引信息

# 分布式文件存储

## TFS

### 设计思路

多个存储文件共享一个物理文件（Block），每个block在集群中都有唯一的编号（块Id），通过<块Id，块内偏移量>可以定位一个文件。

元信息只包括block与存储节点之间的对应关系，不需要存储文件目录树信息，也不需要存储文件与block的映射关系，降低了元信息占用的空间

负载均衡需要考虑机架分布、磁盘利用率、DataServer的读写负载

当集群中新增DataServer节点时，需要限制同时迁移的Block数量，防止被压垮

### NameServer

一主一备部署，保证高可用性

负责Block的创建、删除、负载均衡

处理DataServer发送的心跳

处理DataServer加入和退出

管理Block与DataServer之间的映射关系

### DataServer

负责存储数据，接收客户端或者主副本的请求写入数据

发送心跳，上报维护的Block、磁盘利用率、读写负载

### 写入流程

向Nameserver发起写请求，Nameserver选择一个Block负责本次的数据写入

从Block所在的DataServer中选择一个作为写入的主副本，其他的作为备副本

客端端向主副本写入数据，主副本再将数据同步至备副本，备副本都同步成功，才会向客户端返回操作结果，包括文件所属的BlockId和Block偏移量

### 读取流程

首先根据Block编号从NameServer查找Block所在的DataServer， 然后根据Block偏移读取文件

## FastDFS

### 组件

#### TrackerServer

管理StrorageServer

维护的元信息由StoreageServer汇报的信息生成，不需要持久化

集群中的TrackerServer都是对等的，方便进行扩展

#### StorageServer

以group为单位对StorageServer进行管理，同一group内的StorageServer存储的数据都相同

定期向TrackerServer发送心跳

上报所属的group

上报磁盘剩余空间

服务器状态，比如同步状态

文件上传、下载的次数

不会对文件进行分块处理，客户端上传对的文件与StorageServer存储的文件一一对应

通过增加group内的storageserver或者增加group进行扩容

### 功能点

#### 数据同步

##### 文件

​	binlog文件

​	记录了某个时间点对某个文件执行了某些操作

​	包括时间戳、文件执行操作（C表示源创建、c表示副本创建；A表示源追加、a表示副本追加；D表示源删除、d表示副本删除）、文件名

Mark文件

​	记录向组内的其他storage同步的进度

​	包括当前同步的binlog文件和偏移量

##### 流程

异步复制数据，为组内的其他storageserver都开启一条线程负责复制数据

打开对应StorageServer的Mark文件，读取binlog_index和binlog_offset,然后定位到binlog文件、seek到对应的位置

读取binlog，若读取不到则进行等待；若读到数据，并且该数据的操作方式为源操作，则将改行数据同步给StorageServer。

及时上报本机对其他StorageServer的同步进度

#### 组内新增节点

tracker发现有组内有新增的storage节点，则为其分配同步的源服务器和同步的截止时间点

如果组内的所有节点都无上传的文件时，新增的存储节点可以随时堆外提供服务

如果组内有上传的文件，则由同步的原服务器进行数据同步

组内的其他节点通过心跳得知有新增的节点，则向tracker询问对新增节点同步的源服务器和截止时间点

源服务器向新增的存储节点追加同步截止时间点之前所有的数据

非源服务器向新增节点同步截止时间点之后的源操作

同步至截止时间点时，源服务器则转为正常同步，只同步源操作

当源服务器把所有数据到同步至新增节点，则向tracker节点发送将新增节点的状态改为 online的请求

之后tracker收到新增节点的心跳请求时，将状态修改active状态，可以对外提供服务

#### 客户端请求

##### 上传文件

集群中有多台TrackerServer，任意选择一个tracker发送上传请求

tracker接收到上传请求后，会根据一定的规则选择一个group，负责对文件存储

当选定group后，会从group中选择一个StorageServer返回给客户端

客户端向StorageServer发送上传请求，storage会为文件分配存储目录，

storage生成文件名，由storage ip、创建时间、文件大小、文件Crc32和随机数构成。每个存储目录下有两级256*256 的子目录，storage会根据文件名进行两次hash，路由到其中一个子目录，将文件存储到该目录

返回文件Id，由group、 存储目录、两级子目录、内部文件名、文件后缀名拼接而成。

##### 下载文件

选择任意一个tracker节点发送文件下载请求

tracker根据文件名解析出group、创建时间等信息，选择group内的一台storage节点返回给客户端

向storage节点发送文件下载请求，读取文件

# 列式存储数据库

## Hbase

### 架构

#### Zookeeper

保证任何时候，集群中只有一个活跃master

存放所有Region的寻址入口

实时监控Region server的上线和下线信息。并实时通知Master

存储HBase的schema和table元数据

#### HMaster

为Region server分配region

负责Region server的负载均衡

发现失效的Region server并重新分配其上的region

管理用户对table的增删改操作

#### HRegionServer

Region server维护region，处理对这些region的IO请求

Region server负责切分在运行过程中变得过大的region

#### Region

 HBase自动把表水平划分成多个区域region，每个region会保存一个表里面某段连续的数据

当region不断增大，增大到一个阈值的时候，region就会切分成两个新的region

HRegion是HBase中分布式存储和负载均衡的最小单元

HRegion由一个或者多个Store组成，每个store保存一个columns family

#### store

由一个memStore和0至多个StoreFile组成

写操作先写入memstore， 当memstore中的数据达到某个阈值，将内存中的数据写入磁盘，形成单独的一个storefile

当storefile文件的数量增长到一定阈值后，系统会进行合并(minor、major compaction)，在合并过程中会进行版本合并和删除工作

当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡

### MOB（Moderate object）

应用场景

存储文档、图片、视频；存储上限是10M

启用

在建表时，我们可以对某个Family设置MOB属性，并指定MOB阈值，当value超过MOB阈值，则按照MOB的方式存储，否则正常存储

写流程

超过MOB阈值的cell和正常的cell样，先写WAL日志，再写Memstore

在刷新Memstore时，会判断当前的cell是否是MOB cell，若不是则按照原来的正常方式存储。若是MOB cell,则将Meta数据写入正常的StoreFile,把 MOB的value写入叫做MobStoreFile的文件中

Meta cell存储的内容

row、family、timestamp、qualify

value字段主要存储MOB cell中value的长度、实际存储的文件名、两个tag信息（一个tag表明当前时MOB cell、另一个tag表明所属的表名）

### MSLAB内存管理

优化内存碎片可能导致的Full GC,线程本地分配缓存（Thread-Local Allocation Buffer，TLAB）的内存管理方式，通过顺序化分配内存、内存数据分块等特性使得内存碎片更加粗粒度，有效改善Full GC情况

实现流程

每个Memst都创建一个MemStoreLAB对象

MemStoreLAB默认会申请2M大小的Chunk（onheap或者offheap），chunk维护一个偏移量初始为0的ByteBuffer

当写入数据时，会先获取chunk，再将data复制到chunk中的ByteBuffer中，并将偏移量向前移动data.length

如果当前chunk空间不足或已满，则申请新的chunk

### MemStore Chunk Pool

对chunk进行循环使用，避免频繁申请chunk导致JVM新生代Eden空间耗尽，触发YGC。

实现流程

创建MemStoreChunkPool用来管理不被引用的chunk，这些chunk不会被GC回收掉

当一个chunk不再被引用时，将其放入MemStoreChunkPool

如果MemStoreChunkPool达到上限，不再接受chunk

写数据时，先从MemStoreChunkPool中获取chunk，如果获取到就重复使用，如果获取不到就申请chunk

#### 内存分配器

堆内分配器

堆外分配器

划分成多个64KB大小内存块，把大部分内存占用都分配在堆外，从而减小old区的内存，缓解GC压力

#### 申请流程

分配小于8KB的小内存，此时仍然从堆内分配器分配

分配大于等于8KB且不超过64KB的中等大小内存，此时可以直接分配一个64KB的堆外内存块

分配大于64KB的较大内存，此时需要将多个64KB的堆外内存组成一个大内存，剩余部分通过第1条或第2条规则来分配。例如，要申请130KB的内存，首先分配器会申请2个64KB的堆外内存块，剩余的2KB直接去堆内分配

### 读写路径offheap化

直接向JVM offheap申请对象，offheap划分出来的内存需要用户显式地释放

### 表的设计

表的预分区

设计表

​	rowkey的设计

​	二级索引的设计

# 键值对KV存储

Amazon Dynamo

采用类似Quorum的方式保证数据正确，即W+R>N

数据分片，增加虚拟节点的一致性哈希算法

分片备份，每个分片都有N个副本，存储在hash ring上顺时针方向的N个节点

Merkle Tree，对比分片副本的差异，减少数据传输

hinted handoff

yahoo HaloDB

所有的写操作都是顺序追加到日志文件，并且记录文件无效的数据累积量

后台线程进行合并清除无效的数据。文件的无效数据超过阈值就会合并

元数据信息保存在堆外内存中

读取数据时根据Key从堆外内存获取元数据信息，再根据元数据信息从磁盘读取相应的数据

启动时需要扫描数据构建索引，耗费的时间取决于Key的数量

只支持随机查询，不支持范围扫描

PalDB

一次写入键值存储数据库，多次读取

Mvstore

LevelDB

Bolt

tokyo cabinet

Badger

buntdb

LedisDB

Qdb

BeansDB

Pegasus

ZanRedisDB

Tendis

去中心化的集群管理，类似redis-cluster，节点之间gossip协议通信

兼容redis协议

持久化存储，使用rocksdb存储数据

主从自动切换

RebornDB

A Hybrid Index Key-Value Store

SILK: Preventing Latency Spikes
in Log-Structured Merge Key-Value Stores

HashKV: Enabling Efficient Updates in KV Storage via Hashing

anna

tinydb

高性能Key/Value存储引擎SessionDB

Voldemort

分布式数据库NewSQL

OceanBase

组件

RootServer

管理集群中的所有服务器，数据分布以及副本管理

一主一备，主备之间数据强同步

UpdateServer

存储增量更新数据

一主 一备，主备之间可以配置不同的同步模式。可以和RootServer共用物理服务器

写数据时先写到memstore，当memstore占用的空间达到一定阈值，会当前的memstore冻结，新的memstore被启用，此后新的写入写到新的memstore，冻结的memstore的数据会被转存到磁盘，转存结束后，冻结的memstore占用的空间会被释放。

手动或者定时的合并，将增量的修改数据和chunk server中的基线数据进行合并。合并会消耗一定的cpu、磁盘和网络等资源，导致系统性能的下降。可以降低合并线程的优先级，在系统负载过大的情况下可以放慢合并的速度

特殊场景下会出现修改量太大超出UpdateServer的内存，可以通过分库来解决，分库后每个库的修改量减少，每个库对应一个Oceanbase集群

为了实现范围查询，UpdateServer采用B树来保存修改增量。基于Copy-on-write实现数据的修改

通过copy-on-write，UpdateServer实现了读写事务互不影响。读写分离，到达UpdateServer的读写事务请求，分成读事务和写事务两个相互独立的队列

通过group commit机制，主备UpdateServer都减少了网络IO次数和磁盘IO次数，增加了写事务响应时间，但是提升了系统的TPS

ChunkServer

存储基线数据，一般往往存储三份

MergeServer

接收并解析用户的SQL请求，转发给相应的ChunkServer或者UpdateServer

TiDB

组件

Placement Driver

存储集群中的元数据信息

对TiKV集群进行调度和负载均衡

TiKV服务器

负责存储数据

Rust实现的提供事务的键值（Key-Value）存储引擎

存储数据的基本单位是区域（Region）

通过Raft协议保持数据的一致性

TiDB服务器

负责接收客户端请求，从PD服务器获取响应的元数据信息，将请求转发至TiKV集群获取数据

无状态，可以水平扩容

分片策略

Range范围分片

一段连续的 key 作为一个 Sharding 单元

临近的数据大概率在一起（例如共同前缀），可以很好的支持 range scan 这样的操作。对于扩容友好

对于压力比较大的顺序写是不太友好的，容易出现写尾部热点问题

采用预分片机制，初始化若干分片并分配不同的节点，分散写入负载，避免热点问题。Hbase支持预分片机制

Hash哈希分片

将 key 经过一个 Hash 函数，用得到的值来决定所属的分片

数据均匀分布，对于写压力比较大、同时读基本上是随机读的系统来说更加友好

无法做到范围扫描，扩缩容代价比较大

多级的分片策略

最上层Hash分片

每一个 Hash Sharding 中，数据有序的存储

NSM行式存储

OLTP 数据库默认的存储方式

一条数据的记录集中存在一起，写入的效率较高，在读取时也可以快速获得一个完整数据记录，这种特点称为记录内的局部

行式存储对于 OLAP 分析查询并不友好，带来无效的IO操作

DSM列式存储

所有列集中存储，适应 OLAP 的访问特点

入开销更大、很难将不同列高效地关联起来

并发控制

两阶段锁（2PL）

多版本并发控制（MVCC）

在MVCC中，当要更改或者删除某个数据对象时，DBMS 不会在原地去删除或这修改这个已有的数据对象本身，而是创建一个该数据对象的新的版本，这样的话同时并发的读取操作仍旧可以读取老版本的数据，而写操作就可以同时进行。这个模式的好处在于，可以让读取操作不再阻塞

时序数据库

设计

How to design a time series database

metric

timestamp

fieldKey/fieldvalue

tagKey/tagValue

metric、tagKey、tagValue都用唯一的定长主键标志，节省存储空间

实现

OpenTSDB

Questdb

LinDB

M3

Awesome time series database

时序数据库技术体系 – 初识InfluxDB

数据分析之时序数据库

图数据库存储

Neo4j的存储结构

HugeGraph

深入学习Gremlin

优化

SSD优化

避免就地更新，SSD先擦除，后更新，而且擦除的单位是块

将热数据与冷数据分开

避免长而繁重的写入

采用紧凑的数据结构

写操作优化

组提交:将多个写操作聚合在一起，一次性刷入磁盘中

多个线程同时追加数据时需要尽可能地减少锁冲突
追加过程可以包含两个阶段:第一个阶段是占位，第二个阶段是拷贝数据，拷贝数据相比占位比较耗时，只锁定占位，可以减少锁定时间

配置多个磁盘目录

spdk

DPDK

跳过内核协议栈，直接由用户态进程通过轮询的方式，来处理网络接收
跳过内核协议栈后，就省去了繁杂的硬中断、软中断再到 Linux 网络协议栈逐层处理的过程

Succinct压缩技术

禁用swap
vm.swappiness = 0 

禁用大页
transparent_hugepage=never

禁止记录最近一次访问时间戳,然后重新挂载
mount -o remount,noatime <real_data_volume>

修改磁盘队列长度
echo 1024 > /sys/block/sda/queue/nr_requests 

尽快的将脏页刷新到磁盘，但是要控制刷新脏页的频率，避免出现同步刷新脏页，阻塞IO
vm.dirty_background_ratio = 10 #脏页占用的空间比例超过此阈值，异步刷新脏页，不会阻塞IO操作
vm.dirty_background_bytes = 209715200
vm.dirty_ratio = 40 #脏页可以占用的最大内存比例，达到此阈值，同步刷新脏页，阻塞IO操作
vm.dirty_bytes = 0
vm.dirty_writeback_centisecs = 100 #指定脏数据能存活的时间
vm.dirty_expire_centisecs = 200 #指定多长时间pdflush/flush/kdmflush 这些进程会唤醒一次

posix_fadvise是linux上对文件进行预取的系统调用，主要有以下几种：
POSIX_FADV_NORMAL             无特别建议                    重置预读大小为默认值
POSIX_FADV_SEQUENTIAL        将要进行顺序操作               设预读大小为默认值的2 倍
POSIX_FADV_RANDOM            将要进行随机操作              将预读大小清零（禁止预读）
POSIX_FADV_NOREUSE           指定的数据将只访问一次       （暂无动作）
POSIX_FADV_WILLNEED          指定的数据即将被访问          立即预读数据到page cache
POSIX_FADV_DONTNEED         指定的数据近期不会被访问      立即从page cache 中丢弃数据

# 美团KV[存储](https://weibo.com/ttarticle/p/show?id=2309404521810107432984#_loginLayer_1699695768011)

## 持久化 KV Cellar

### 架构

跟阿里开源的 Tair 类似，但有两点不同，第一个是 OB，第二个是 ZooKeeper。

#### OB 

提供 Cellar 中心节点元数据的查询服务。它可以实时与中心节点的 Master 同步最新的路由表，客户端的路由表都是从 OB 去拿

好处主要有两点，第一，把大量的业务客户端跟集群的大脑 Master 做了天然的隔离，防止路由表请求影响集群的管理。第二，因为 OB 只供路由表查询，不参与集群的管理，所以它可以水平扩展，极大地提升了我们路由表的查询能力

####  ZooKeeper 

做分布式仲裁，解决 Master、Slave 在网络分割情况下的脑裂问题。并且通过把集群的元数据存储到 ZooKeeper，我们保证了元数据的高可靠

### 节点容灾

Handoff 机制

解决节点临时宕机带来的影响

例如A节点宕机，触发handoff机制，中心节点通知客户端A节点宕机，让客户端原本发送给A的请求，发送至B节点

B节点会记录本来发送A节点的请求记录到日志文件中

A节点恢复后,报心跳到中心节点，中心节点通知B节点，B节点就会把把本地的日志文件写回到A节点

### 强一致性

选择了 Raft 协议实现强一致性复制

中心节点会做 Raft 组的调度，它会决定每一个 Slot 的三副本存在哪些节点上

## 拆线程池、拆队列

### 队列分类

读快队列

写快队列

写慢队列

读慢队列

### 如何区分快慢

根据请求的Key个数、Value 大小、数据结构元素数

某个线程池空闲的时候会去帮助其它的线程池处理请求

## 热点 Key

中心节点增加对热点区域的管理，可以很方便的将热点区域放到集群的所有节点

数据节点不仅存储普通数据，还会开辟单独区域存储热点数据，数据节点收到客户端写请求后，根据实时的热点统计结果，判断key是否是热点key，如果是的点key，除了会本地存储外，还会将热点key的数据复制其他节点上的热点区域，同时告诉客户端此key是热点key，之后对与该key的读取操作，可以直接请求热点区域

很好地解决了类似客户端缓存热点 KV 方案造成的一致性问题

## 智能迁移

当A节点向B节点迁移slot时，A节点做给相应的slot做一个快照，之后对于客户端的写请求，写入增量的日志文件。

A节点同步快照到B节点，然后再同步增量的日志文件

迁移的过程中，B节点会回复引擎的压力、队列长度、网卡流量等信息，A节点根据B节点的状态信息调整迁移的速度

从零开始设计Kv

喜马拉雅KV存储系统

快慢命令分离

设置zset保存的最大元素个数、头尾删除

新增ehash数据类型，field支持单独设置过期时间，过期之后高效删除

支持string数据类型的kv分离存储。在value较大时，可以有效降低LSM的写放大问题

小米KV存储系统

架构

MetaServer

负责ReplicaServer的存活检测、Replica分配、负载均衡等，

借助Zookeeper实现一主多备

ReplicaServer

负责数据的存储和备份，接收客户端发来的数据存储请求

数据模型

采用简单的key-value数据模型，将key分为了hashkey和sortkey

hashkey用来计算所属分片；sortkey用来实现分片内的排序

数据分片

采用固定Hash分布

单机存储引擎RocksDB

一致性协议

PacificA

oceanbase 数据库

滴滴对象存储系统的架构演进实践

有赞分布式 KV 存储服务

GoBeansDB 架构设计

一条数据的漫游——X-Engine SIGMOD论文详解

分布式系统领域经典论文翻译集

6.824 Schedule

人人网海量存储系统Nuclear

CoolHash数据库引擎架构与设计分享

字节跳动KV存储

分布式时序数据库QTSDB的设计与实现

# 分布式协议与算法

[如何解决分布式系统中的“幽灵复现”？]https://www.jiqizhixin.com/articles/2020-03-24-7

Paxos

Raft

ZAB

一致性哈希算法

Gossip协议

Quorum NWR

灵活地自定义一致性

强一致性：保证写操作完成后，任何后续访问都能读到更新后的值

最终一致性：写操作完成后，可能会读到旧数据，但是最终所有后续访问都能读到相同的最近更新的值

三要素

N 表示副本数，又叫做复制因子

W，又称写一致性级别，表示成功完成 W 个副本更新，才完成写操作

R，又称读一致性级别，表示读取一个数据对象时需要读R副本，然后返回 R 个副本中最新的那份数据

PBF算法

POW算法

Pacifica

# 磁盘工具

df 命令，查看文件系统的使用情况

iostat -d -x 1 显示所有的IO指标

pidstat -d 1查看所有进程的IO读写情况

磁盘调度器

deadline：读写请求分离，读请求具有高优先调度权，可以保证读请求的延迟时间最小化

对请求的顺序批量处理。对那些地址临近的顺序化请求，减少磁盘抖动。每个请求都赋予了一个最大延迟时间，如果达到延迟时间的上限，那么这个请求就会被提前处理掉
保证每个请求的延迟时间，但是会破坏磁盘访问的顺序化特征，会影响性能

Noop：邻近bio进行了合并处理

# RUM猜想

读、写、存储只能优化两项，另一项将被弱化

## B+ Tree

读优化

写放大。当页表满了之后会进行页分裂，产生写放大

存储不连续。叶子节点构成的有序链表在逻辑上是有序的，但是在磁盘存储上，新增页表申请的存储空间与原有页表很可能是不相邻的

B+Tree 结构是为读取做了优化，但如果进行大量随机写还是会造成存储的碎片化，从而导致写放大和读放大

填充因子是一种常见的优化方法，它的原理就是在页表中预留一些空间，这样不会因为少量的数据写入造成树结构的大幅变动

## LSM Tree

写优化。将随机写转换为顺序写提升了写入速度，但只是延缓了写放大并没有真正减少写放大

写放大。compact操作会降低读放大，但是带了写放大

读放大。读取操作不仅读取memstore，还会读取磁盘上的sstable文件

### 合并策略

#### Tiered 策略

比如HBase

每当某个尺寸的SSTable 数量达到既定个数时，将所有 SSTable 合并成一个大的 SSTable。

读放大。SSTable文件在 Key 的范围上会存在交叉，所以每次读操作都要遍历所有 SSTable

写放大。Compact 会降低读放大，但却带来更多的写放大和空间放大。其实 LSM 只是推迟了写放大，短时间内，可以承载大量并发写入，但如果是持续写入，则需要一部分 I/O 开销用于处理 Compact。

#### Leveled 策略

比如LevelDB、RocksDB

将数据分成一系列 Key 互不重叠且固定大小的SSTable 文件，并分层（Level）管理

读放大。SSTable内部数据有序存储。引入布隆过滤器判断key是否存在。Level0层SSTable无序，需要遍历所有文件，从Level1层开始只需遍历一个SSTable

写放大。除了 L0 以外，每次更新仅涉及少量 SSTable。但是 L0 的 Compact 比较频繁



# Percolator分布式事务

前提

此事务模型的前提是事务的参与者，即数据库，要支持多版本并发控制（MVCC）

流程

准备阶段

事务管理器发送Prepare请求，包含了具体的数据操作要求

分片接到请求后要做两件事，写日志和在lock字段添加锁信息

每个事务都要选取一个记录作为主锁，主锁的选择是随机的，其他参与事务的记录在lock字段记录了指向主锁的信息

提交阶段

事务管理器只需要和拥有主锁的记录通讯，发送Commit 指令，且不用附带其他信息

增加一条新的记录标有事务提交的时间并且指向准备阶段写入的记录，清除写入的主锁信息

后台线程异步提交其他记录，保证对其他事务可见

# 如何解决“[幽冥复现](https://www.jiqizhixin.com/articles/2020-03-24-7)”问题

## Raft

Leader当选后立即追加一条Noop的特殊内部日志，并立即同步到其它节点，实现前面未Committed日志全部隐式提交

保证所有的已经Committed的Log Entry不会丢
保证不会读到未Committed的数据

## ZAB

每次leader选举完成后，都会保存一个本地文件，用来记录当前EpochId（记为CurrentEpoch），表明当前的选举轮次，每次选举都会+1
在选举时，会先读取CurrentEpoch并加入到选票中，发送给其他候选人，候选人如果发现CurrentEpoch比自己的小，就会忽略此选票，如果发现CurrentEpoch比自己的大，就会选择此选票

# 全局时钟

## Chronos时间戳服务

实现高可用、高性能、提供全局唯一而且严格单调递增timestamp的服务

采用主备架构，主服务器挂了以后备服务器迅速感知并接替服务

主服务器承接客户端的请求，分配的timestamp保证严格单调递增，并且将已分配的值持久化到ZooKeeper上

通过预分配时间戳（将来的时间戳）的方式避免与ZK频繁的网络通信，降低ZK的压力
在每次分配时间戳时，会判断是否已经持久化的时间戳大，如果大的话，会计算一个将来的时间戳持久化到ZK中，如果小的话，不需要持久化。
如果分配的时间戳快要接近ZK持久化的时间戳，会计算一个新的将来的时间戳将其异步持久化到ZK中

支持将批量的时间戳返回给客户端

## TiDB授时服务

TiDB 的全局时钟是一个数值，它由两部分构成，其中高位是物理时间，也就是操作系统的毫秒时间；低位是逻辑时间，是一个 18 位的数值

TiDB中提供授时服务的节点被称为Placement Driver，简称 PD。多个 PD 节点构成一个 Raft 组，通过共识算法可以保证在主节点宕机后马上选出新主，在短时间内恢复授时服务

为了保证新主产生的时间戳一定大于旧主，必须将旧主的时间戳存储起来
存储也必须是高可靠的，所以 TiDB 使用了 etcd，并不是每产生一个时间戳都要保存，避免频繁的网络通信和磁盘存储

TiDB 采用预申请时间窗口的方式，时间窗口的跨度是可以通过参数指定的，默认配置3秒
例如当前时间窗口的起点是PD当前时间 103，时间窗口的终点就在 106 毫秒处。写入 etcd 成功后，PD 将得到一个从 103 到 106 的“可分配时间窗口”，在这个时间窗口内 PD 可以使用系统的物理时间作为高位，拼接自己在内存中累加的逻辑时间。所有 PD 已分配时间戳的高位，也就是物理时间，永远小于 etcd 存储的最大值。

为了降低通讯开销，每个客户端一次可以申请多个时间戳，时间戳数量作为参数，由客户端传给 PD

## 分布式授时（HLC）*

# Raft跨机房部署

两地三中心

一城市部署两个数据中心，另一城市部署一个数据中心，每个数据中心部署一个数据节点。

能满足单个数据中心级的容灾，但是不能满足城市级容灾

三地五中心

5 个节点基本平分到了 3 个城市的数据中心，2个中心部署2个副本，1个中心部署1个副本

任何一个城市网络出现了问题，raft 协议都能正常运行，也是普遍使用的较高容灾级别的部署方式

Raft 协议还需要一个降级机制，也就是说不一定要过半投票，仍然维持服务。

# JRaft

优化点

批量化的手段合并 IO 请求
具体包括批量提交 Task、批量网络发送、本地 IO 批量写入以及状态机批量应用

JRaft 中整个链路几乎没有任何阻塞，完全异步，通过 Disruptor 来实现

Leader 向所有 Follwers 发送 Log 也是完全相互独立和并发的

Leader 持久化 Log 和向 Followers 发送 Log 是并行的

复制流水线主要是利用 Pipeline 的通信方式来提高日志复制的效率
不需要等待前一个Batch 同步完成仍然可以发送下一个batch, 可以降低同步延迟，提高吞吐量

# Etcd

## Lease租约

server端保证在约定的有效期内，不会删除关联到此Lease上的key-value

### checkponit机制

早起版本并未持久化存储 Lease 剩余 TTL 信息，当重启或者Leader切换之后自动给Lease续期。为了解决自动续期，引入检查点机制

Leader节点会后台运行任务定期批量的将Lease剩余的TTL基于RaftLog同步给Follower节点
当Leader节点收到KeepAlive 请求的时候，也会将Lease的TTL同步给Follower节点

### 淘汰机制

基于最小堆来管理 Lease，实现快速淘汰过期的 Lease

后台任务定期执行撤销 Lease 检查，每次轮询堆顶的元素，若已过期则加入到待淘汰列表，直到堆顶的 Lease 过期时间大于当前时间，则结束本轮轮询。

Leader节点将过期的LeaseId基于RaftLog发给Follower节点，Follower节点会获取根LeaseId关联的key列表，从boltdb删除key，从内存中删除Lease对象，从boltdb的Lease bucket删除Lease

### Key如何管理Lease

etcdctl lease grant 600 # 创建一个TTL为600秒的lease，etcd server返回LeaseId
etcdctl put node healthy --lease 326975935f48f818 #--lease 指定关联的LeaseId

一个 Lease 关联的key集合是保存在内存中的.当重启的时候会根据boltdb保存的信息，重建关联各个Lease的Key集合

## MVCC

基于多版本技术实现的一种并发控制机制

更新key-value数据的时候，并不会覆盖原数据，而是新增一个版本来存储新的数据，每个数据都有一个版本号

### treeIndex 模块

基于内存版 B-tree 实现了 key 索引管理

保存了用户 key 与版本号的映射关系等信息，比如创建版本、历史变更版本、当前版本列表

### boltdb模块

以版本号作为 boltdb key，以用户的 key-value 等信息作为 boltdb value；版本号包括主版本号、子版本号，主版本号时是全局的事务版本号，子版本号是在当前事务中从0开始递增

## Watch

快速获取数据变更通知，避免频繁无效的轮询

使用基于HTTP/2 的gRPC协议，将变更及时推送

将历史版本保存在磁盘中，避免了历史版本的丢失，提升了Watch机制的可靠性

watch命令中指定版本，避免当client 因网络等异常出现连接闪断时，错过历史事件

使用Map和区间树来维护key与watch的关联关系。watch不仅可以监听单个key，还可以监听Key范围、前缀

## 压缩

回收历史版本，释放空间，防止内存耗尽，db大小达到配额，抛出"etcdserver: mvcc: database space exceeded"错误

自动压缩

时间周期性压缩

只保留最近一段时间写入的版本号

版本号压缩

保留指定数目的版本号

手动压缩

## 读模式

### 串行读

直接读状态机数据返回、无需通过 Raft 协议与集群进行交互的模式
它具有低延时、高吞吐量的特点
适合对数据一致性要求不高的场景

### 线性读

一旦一个值更新成功，随后任何通过线性读的 client 都能及时访问到，适用于对数据一致性要求高的场景

 etcd 3.0中读请求通过走一遍 Raft 协议保证一致性这种 Raft logread 机制依赖磁盘 IO，性能较差

etcd 3.1 中引入readindex机制。
	当收到一个线性读请求时，首先会从Leader获取集群最新的已提交的日志索引，Leader收到请求后，为防止脑裂，确保当前仍是Leader需要向Follower发送心跳确认，过半节点确认Leaader身份后，将提交的日志索引返回给节点。节点会等待直到状态机应用的索引大于等于Leader已经提交的索引，然后读取状态机数据，返回响应给客户端

## 优化性能

在开启鉴权场景时，建议你尽量使用证书而不是密码认证，避免校验密码的昂贵开销

根据业务场景选择合适的读模式
串行读比线性度性能提高 30% 以上，延时降低一倍

尽量减少 expensive read request 次数
在程序启动的时候，获取一次全量数据，然后通过etcd Watch机制去获取增量变更数据

通过分页机制按批拉取，尽量减少一次性拉取数过多的数据

避免大 key-value，默认上限为1.5MB，对大key进行拆分

根据你的业务场景，适当调整db quota大小，默认db quota大小是2G，超过2G就只读无法写入

选择合适的压缩策略，etcd 支持按时间周期性压缩、按版本号压缩两种策略，避免频繁的按照时间周期性压缩数据

避免因限速触发"etcdserver: too many requests"错误给 client，当 committed Index 远大于 applied index 时会抛出此错误

降低Follower节点通过Leader快照重建的概率，使其尽量能通过增量的日志同步保持集群的一致性
etcd 提供了一个名为 --snapshot-count 的参数来控制快照行为。它是指收到多少个写请求后就触发生成一次快照，并对Raft日志条目进行压缩

尽量选择高配的节点，各个节点之间尽量就近部署，使节点之间RTT延时尽量低

使用SSD，提高磁盘IO性能

 部署多etcd集群，不同的业务请求不同的etcd集群

## grpc proxy组件

### 扩展串行读

gRPC proxy是个无状态节点，提供高性能的读缓存的能力。
根据业务场景需要水平扩容若干节点，降低服务端连接数，提供了故障探测和自动切换能力

### 扩展Watch

大量的 watcher 会显著增大 etcd server 的负载，导致读写性能下降。
gRPC proxy 组件里面提供了watcher合并的能力。如果多个client Watch 同 key或者范围时，它会尝试将你的 watcher 进行合并，降低服务端的 watcher 数

### 扩展Lease

Lease 特性提供了一种客户端活性检测机制。为了确保你的 key 不被淘汰，client 需要定时发送 keepalive 心跳给 server。
当 Lease 非常多时，这就会导致 etcd服务端的负载增加。在这种场景下，gRPC proxy 提供了 keepalive 心跳连接合并的机制，来降低服务端负载

## Leader选举优化点

避免同一时间多节点同时进行选举，导致选举失败，加入了随机数，使得每个节点发起选举的时间点不同

避免因网络原因导致某个节点发起多轮选举都得不到集群中其他节点的响应致使term无限增加，引入预投票机制。在选举之前，发送预投票请求，term不增加，当得到集群中半数节点的响应后，再将term+1，发送投票请求

合理调优心跳间隔、选举超时时间，避免频繁的发生leader选举切换，导致稳定性降低

## 总结

为了提升写吞吐量，将一批日志条目批量持久化到磁盘

通过 pipeline技术进行数据的复制，降低延迟

批量化的任务合并

任何问题都可以通过加一中间层来解决，添加中间层支持海量的客户端连接、对相同的请求进行合并，降低服务端的负载
