 高并发的本质，**就是海量的读和写**

# 如何应对高并发

## 架构原则：“4 要 1 不要”

### 1、数据要尽量少	

首先是指用户请求的数据能少就少。

其次，还要求系统依赖的数据能少就少，包括系统完成某些业务逻辑需要读取和保存的数据

### 2、请求数要尽量少

减少请求数最常用的一个实践就是合并 CSS 和 JavaScript 文件

合并请求

门面模式，为客户端访问的多个接口提供统一的接口

客户端限制单位时间内请求次数

按钮置灰，不可点击

浏览器、App端缓存

Nginx内容缓存

​		Nginx 会将多个并发请求合并为1条回源请求，并锁住所有的客户端请求，直到回源请求返回后，才会更新缓存，同时向所有客户端返回响应

### 3、路径要尽量短

缩短请求路径不仅可以增加可用性，同样可以有效提升性能（减少中间节点可以减少数据的序列化与反序列化），并减少延时

多个相互强依赖的应用合并部署在一起，把远程过程调用（RPC）变成 JVM 内部之间的方法调用

多机房部署，避免跨机房服务调用

将数据放在离用户近的地方，DNS用户就近接入，CDN

将频繁访问数据，放入缓存

### 4、依赖要尽量少

所谓依赖，指的是要完成一次用户请求必须依赖的系统或者服务，这里的依赖指的是强依赖。

要减少依赖，我们可以给系统进行分级，比如 0 级系统、1 级系统、2 级系统、3 级系统，0 级系统如果是最重要的系统，那么 0 级系统强依赖的系统也同样是最重要的系统

### 5、不要有单点

应用无状态化，可以任意扩容。但是要注意下游依赖所支撑的上限

存储服务本身很难无状态化，一般要通过冗余多个备份的方式来解决单点问题

**架构是一种平衡的艺术，而最好的架构一旦脱离了它所适应的场景，一切都将是空谈。**

## 流量削峰

### 排队

最容易想到的解决方案就是用消息队列来缓冲瞬时流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一端承接瞬时的流量洪峰，在另一端平滑地将消息推送出去。

排队，就是把“一步的操作”变成“两步的操作”，其中增加的一步操作用来起到缓冲的作用

### 答题

延缓请求，起到对请求流量进行削峰的作用，从而让系统能够更好地支持瞬时的流量高峰

### 分层过滤

层过滤其实就是采用“漏斗”的方式，对请求进行分层过滤，从而过滤掉一些无效的请求

大部分数据和流量在用户浏览器或者 CDN 上获取，这一层可以拦截大部分数据的读取

nginx代也可以缓存数据，拦截大部分数据的读取

应用服务层，本地缓存、远端缓存，对系统做好保护和限流

数据库层完成数据的强一致性校验

## 处理热点数据

1、优化

优化热点数据最有效的办法就是缓存热点数据，本地缓存、远端缓存（redis）、nginx缓存、多副本存放

2、限制

限制更多的是一种保护机制

对热点数据的key计算Hash，然后根据 Hash 做分桶，每个分桶设置一个处理队列，这样可以把热点商品限制在一个请求队列里，防止因某些热点数据占用太多的服务器资源，而使其他请求始终得不到处理

3、隔离

数据隔离

​	启用单独的 Cache 集群或者MySQL 数据库来放热点数据

系统隔离

​	系统隔离更多的是运行时的隔离，可以通过分组部署的方式和另外 99% 分开，让请求落到不同集群中

## 如何设计兜底方案

### 高可用建设

#### 架构阶段

避免系统出现单点、分组隔离，单元化部署、异步化、异地容灾

#### 编码阶段

错误捕获

异步线程

超时处理

限流保护

#### 测试阶段

自动化对比测试、beta测试

#### 发布阶段

多版本发布/分批发布

#### 运行阶段

实时监控报警

过载保护

自动降级

#### 故障发生

快速恢复

故障定位

### 降级

所谓“降级”，就是当系统的容量达到一定程度时，限制或者关闭系统的某些非核心功能，从而把有限的资源保留给更核心的业务

执行降级无疑是在系统性能和用户体验之间选择了前者

### 限流

当系统容量达到瓶颈时，通过限制一部分流量来保护系统，并做到既可以人工执行开关，也支持自动化保护的措施

限流既可以是在客户端限流，也可以是在服务端限流

限流的实现方式既要支持 URL 以及方法级别的限流，也要支持基于QPS 和线程的限流

### 拒绝服务

当系统负载达到一定阈值时，例如 CPU 使用率达到 90% 或者系统 load 值达到 2*CPU 核数时，系统直接拒绝所有请求

在最前端的 Nginx 上设置过载保护，当机器负载达到某个值时直接拒绝HTTP 请求并返回 503 错误码，在 Java 层同样也可以设计过载保护

## 提高性能

### 性能指标

服务端性能，一般用 QPS（Query Per Second，每秒请求数）来衡量，另一个就是响应时间（Response Time，RT），它可以理解为服务器处理响应的耗时

正常情况下响应时间（RT）越短，一秒钟处理的请求数（QPS）自然也就会越多

总 QPS =（1000ms / 响应时间）× 线程数量

### 影响因素

性能就和两个因素相关了，一个是一次响应的服务端耗时，一个是处理请求的线程数

要提升性能我们就要减少 CPU 的执行时间，另外就是要设置一个合理的并发线程数，通过这两方面来显著提升服务器的性能

### 如何发现瓶颈

JProfiler 和 Yourkit 这两个工具，它们可以列出整个请求中每个函数的 CPU 执行时间，可以发现哪个函数消耗的 CPU 时间最多，以便针对性地做优化

怎样简单地判断 CPU 是不是瓶颈呢？一个办法就是看当 QPS 达到极限时，服务器的CPU 使用率是不是超过了 95%，如果没有超过，那么表示 CPU 还有提升的空间，要么是有锁限制，要么是有过多的本地 I/O 等待发生。

### 如何优化系统

1、 减少序列化

序列化大部分是在 RPC 中发生的，因此避免或者减少 RPC 就可以减少序列化

可以将多个关联性比较强的应用进行“合并部署”，而减少不同应用之间的 RPC 也可以减少序列化的消耗。
所谓“合并部署”，就是把两个原本在不同机器上的不同应用合并部署到一台机器上，当然不仅仅是部署在一台机器上，还要在同一个 Tomcat 容器中，且不能走本机的 Socket，这样才能避免序列化的产生

2、Java 极致优化

对大流量的 Web 系统做静态化改造，让大部分请求和数据直接在 Nginx 服务器或者 Web 代理服务器（如 Varnish、Squid 等）上直接返回，而 Java 层只需处理少量数据的动态请求

直接使用 Servlet 处理请求，避免使用传统的 MVC 框架

直接输出流数据。使用 resp.getOutputStream() 而不是 resp.getWriter() 函数，可以省掉一些不变字符数据的编码，从而提升性能；数据输出时推荐使用 JSON 而不是模板引擎（一般都是解释执行）来输出页面

3、并发读优化

采用应用层的 LocalCache，即在系统的单机上缓存相关的数据

对于动态数据，会采用“被动失效”的方式缓存一定时间，失效后再去缓存拉取最新的数据

## 海量读请求

### 分散的读请求

当百万的 QPS 属于不同用户时，因缓存是集群化的，所有到达业务后台的请求会根据一定路由规则，分散到请求缓存集群中的某一个节点

假设一个节点最大能够支撑10WQPS，我们只需要在集群中部署 10 台节点即可支持百万流量

#### 1、架构尽量不要分层

​	读服务的业务逻辑都比较简单，性能主要消耗在网络传输上。读服务要尽可能和数据靠近，减少网络传输

​	比如数据前置，浏览器缓存、CDN缓存

#### 2、代码尽可能简单

​	精细化地按需打印日志

​	对于读取的内容要在存储处按需取，如果存储为 MySQL，则不要使用 select *，需要手动指定你要查询的字段。如果存储为 Redis，则使用 Redis 的 hash 结构存储数据

#### 3、缓存的懒加载模式

​	在初始的时候，所有数据都存储在数据库中。

​	当读服务接受请求时，会先去缓存中查询数据，如果没有查询到数据，就会降级到数据库中查询，并将查询结果保存在 Redis 中。

​	保存在 Redis 中的数据会设置一个过期时间，防止数据库的数据变更了，请求还一直读取缓存中的脏数据

存在的问题：

1、存在缓存穿透的风险

​		针对数据库中没有的数据，可以在缓存中设置一个占位符。在第二次请求处理时，读取缓存中的占位符即可识别数据库中没有此数据，然后直接返回给业务前台系统即可。
​	使用占位符虽然解决了穿透的问题，但也带来了另外一个问题。如果恶意请求不断变换请求的条件，同时这些条件对应的数据在数据库中均不存在，那么缓存中存储的表示无数据的占位符也会把整个缓存撑爆，进而导致有效数据被缓存清理策略清除或者整个读服务宕机。
​	对于此种恶意请求，就需要在业务上着手处理。对于请求的参数可以内置一些 token 或者一些验证数据，在读服务中前置进行校验并拦截，而不是透传到缓存或数据库中

2、缓存集中过期导致雪崩
	将设置的过期时间随机化，避免同一时间过期

3、懒加载无法感知实时变更

​	在缓存中设置过期时间，虽然可以让用户感知到数据的变更。但感知并不是实时的，会有一定延迟

​	修改数据时，删除缓存，订阅binlog，重写缓存

4、懒加载无法摆脱毛刺的困扰

​	使用懒加载的缓存过期方案，还有一个无法避免的问题，就是性能毛刺。当缓存过期时，读服务的请求都会穿透到数据库中，对于穿透请求的性能和使用缓存的性能差距非常大，时长是毫秒和秒级别的差异

#### 4、全量缓存

##### 基本概述

​	全量缓存是指将数据库中的所有数据都存储在缓存中，同时在缓存中不设置过期时间的一种实现方式

​	所有数据都存储在缓存里，读服务在查询时不会再降级到数据库里，所有的请求都完全依赖缓存。此时，因降级到数据库导致的毛刺问题就解决了

​	全量缓存对数据更新要求更加严格，要求所有数据库已有数据和实时更新的数据必须完全同步至缓存，不能有遗漏。一种有效的方案是采用订阅数据库的 Binlog 实现数据同步

​	将 Binlog 的中间件（Canal）挂载至目标数据库上，就可以实时获取该数据库的所有变更数据。对这些变更数据解析后，便可直接写入缓存里

**任何方案在带来某一方面的提升时，必然是在其他方面做出了一些取舍，架构其实是一门平衡的艺术**

##### **全量缓存存在的问题**			

第一个问题：提升了系统的整体复杂度

第二个问题：缓存的容量会成倍上升，相应的资源成本也大幅上升	

如何节省内存？

​		1、首先是存储在缓存中的数据需要经过筛选，有业务含义且会被查询的才进行存储。比如数据库常见的修改时间、创建时间、修改人、数据有效位等一些记录性字段可以不存储在缓存中

​		2、其次是存储在缓存中的数据可以进行压缩。采用 Gzip、Snappy 等常见的压缩算法进行处理，但压缩算法通常较消耗 CPU。

​		3、节约缓存的技巧	
​		技巧一：将数据按 JSON 格式序列化时，可以在字段上添加替代标识，表示在序列化后此字段的名称用替代标识进行表示
​		技巧二：如果你使用的缓存是 Redis 且使用了其 Hash 结构存储数据。其 Hash 结构的 Field 字段，也可以使用和上述 JSON 标识一样的模式，使用一个较短的标识进行代替

##### 其他优化点

###### 多机房实时热备

为了提升性能和可用性，可以将数据同步模块写入的缓存由一个集群变成两个集群.两套缓存集群可以分别部署到不同城市的机房或者同城市的不同分区。另外，读服务也相应地部署到不同城市或不同分区

此方案提升了可用性

###### 异步并行化

对于需要多次和存储交互的场景，可以采用异步并行化的方式——接收到一次读请求后，在读服务内部，将串行与存储交互的模式改为异步并行与存储进行交互

##### 如何保证一致性

###### Binlog 高效消费

1、全串行的方式进行消费

​	在消费时，对此 Binlog 文件使用 ACK 机制进行串行消费，每消费一条确认一条，然后再消费一条，以此重复

2、采用并行的方式提升吞吐量及扩展性

​	借用了 MQ 进行拆分。在 Binlog 处仍然进行串行消费，但只是 ACK 数据。ACK 后数据直接发送到 MQ 的某一个 Topic 里即可。因为只做 ACK 并转发至 MQ，不涉及业务逻辑，所以性能消耗非常小

​	为避免并行性带来的数据乱序的问题，将对同一条数据的修改记录都发送到同一个分区中，即根据数据的主键计算分区。

采用 Redis 的 Hash 结构进行局部更新

###### 最后的兜底，直接写入

​	一些关键场景在写完数据库后，主动将数据写入缓存中去。

​	但对于写入缓存可能出现的失败可以不处理，因为主动写入是为了解决缓存延迟的问题，主动写入导致的丢失数据由 Binlog 保障最终一致性

### 集中读请求

即热点数据读请求，当百万 QPS 都属于对同一条数据读取时，即使缓存是集群化的，同一个用户的请求都会被路由至集群中的某一个节点

热点查询是对相同的数据进行不断重复查询的一种场景。特点是次数多，但需要存储的数据少，因为数据都是相同的

#### 热点key探测

##### 京东[HotKey](https://gitee.com/mirrors/Tair.git)

###### etcd集群

​		存放配置热key的规则、worker地址

###### worker节点

​		通过滑动窗口探测热点key，将热点key推送给服务实例

###### 服务实例

​		启动从etcd拉取worker地址，建立连接，拉取热key规则，将符合规则的key上报给worker节点（对key计算hash，根据worker节点的数量取模）；接收worker节点推送的热key，将数据缓存再本机

##### [淘宝Tair](https://gitee.com/mirrors/Tair.git)

###### DataServer端

​		在DataServer上划分一块HotZone存储区域存放热点数据，每个HotZone都存储相同的读热点数据
对于Tair来说，每个key的访问都会落到同一数据节点DataServer上，由DataServer探测热点key，将热点推送其他节点的HotZone中

###### 客户端

​		客户端在第一次请求前初始化时，会获取配置HotZone的数据节点
​		客户端随机选择一个HotZone区域作为自身固定的读写HotZone区域（挑选前将数据节点地址打散）
​		在DataServer未发生变化的情况下，不会改变选择。即每个客户端只访问唯一的HotZone区域
​		客户端收到服务端反馈的热点Key信息后，至少在客户端生效N秒
​		客户端首先请求HotZone节点，如果数据不存在，则继续请求源数据节点，获取数据后异步将数据存储到HotZone节点里

##### [饿了么samaritan](https://github.com/samaritan-proxy/samaritan.git)

###### 设计前提

​			类似于Codis，都是中间层代理
​			所有的 Redis 请求都是经过代理 Samaritan，由代理层进行收集上报热点key
​			不️依赖任何外部组件，外部聚和组件的初衷是为了聚合不同机器的数据.基于局部可以代表整体的原则，那么单实例上的热 key 就可以代表全局的一个情况

###### 实现

​			每个代理会有一个全局的 Hotkey Collector
​			每个Redis的连接维护自己独立的 Counter，Counter 采用LFU 算法（只保留访问频次最高的key）
​			Collector 会定时地去收集每个 Counter 的数据并进行聚合，聚合的时候不会使用真实的计数，而是使用概率计数，并且为了适应访问模式的变化 counter 的值会随着时间衰减

###### 注意

​			当前的方案只能够快速定位系统中的热 key，但并没有真正解决热 key 本身带来的问题，仍旧需要业务方自行改造或者将那些热点 key 调度到单独的节点上

#### 利用应用内的前置缓存

应用内的缓存存储的均是热点数据。当应用扩容后，热点缓存的数量也随之增加。在采用了前置缓存后，在面对热查询时只需扩容应用即可。前端负载均衡器（如 Nginx 等）会将所有请求平均地分发到各应用中去

##### 首先是应用内缓存需要设置上限

​	必须设置容量的上限且设置容量满时的逐出策略。逐出策略可以是 LRU，将最少使用的缓存在容量满时清理掉，因为热点缓存需要存储的是访问次数多的数据。前置缓存也需要设置过期时间

##### 其次是根据业务对待延迟的问题

​	如果业务上可以容忍一定时间的延迟，可以在缓存数据上设置一个刷新时间即可

##### 要把控好瞬间的逃逸流量

​	在初始化时，瞬间出现热点查询或者缓存时间到期，那么所有的请求都会逃逸至后端加载最新的缓存，也有可能把后端缓存打挂

​	为了避免上述情况，在从后端加载数据时，通过加锁，只允许一个请求逃逸请求后端缓存

​	对于数据过期需要更新时，并不主动清理掉数据。其他非逃逸请求使用历史脏数据，而逃逸的那一个请求负责把数据取回来并刷新前置缓存

#### 降级兜底不可少

​	所部署的机器能够支持的 QPS 未必都能够大于当次的热点查询。对于可能出现的超预期流量，可以使用前置限流的策略进行应对

#### 其他前置策略

除了采用后端应用内的前置缓存进行应对外，在前端的架构里也有一些应对手段。比如在接入层（如 Nginx）进行前置缓存（nginx还可实现对请求的合并）、数据前置至离用户更近的 CDN 及开启浏览器缓存

#### 分散存储

将热点数据打散，分散到缓存集群中的各个节点

### 多维度富查询

1、为了满足和原有分库维度不一样的查询，最简单的方式是按新的维度异构一套数据

2、使用 ES 满足多维度查询

​	第一步需要做的便是数据异构，将数据库的数据同步至 ES 中

## 海量写请求

分库分表只解决了容量的问题，并没有解决写服务的高可用问题

写业务是指需要将用户传入的数据进行全部存储的一种场景，对于写入业务，当出现各种故障时，最重要的是保证系统可写入

### 如何保证随时可写入？

只要有可用存储即可写入。

引入写服务集群，当写请求到达时，将数据写入到写服务集群中，然后通过同步模块，将数据写入到目标库中。

### 如何维护可用列表？

可以通过自动感知或人工确认的方式维护可用的数据库列表

在写服务调用数据库写入时，可以设置一个阈值。如果写入某一台数据库，在连续几分钟内，失败多少次，则可以判定此数据库故障，并将此判定进行上报

当整个写服务集群里，超过半数都认为此数据库故障了，则可以将此数据库从可用列表中剔除

为了防止误剔除某一台只是发生网络抖动的数据库，可以在真正下线某一个机器前，增加一个报警，给人工确认一个机会。可以设置当多少时间内，人工未响应，即可自动下线

通过对写服务集群扩容，可以增加写业务的可用性，同时可以提高TPS。写服务集群扩容后，将写请求负载均衡的顺序随机写入升级为按权重写入，比如对新加入的机器设置更高的写入权重，让数据更快地在全部数据库里变得均衡

### 如何保证同步成功？

#### 1、同步模块

​	订阅写服务集群的binlog，根据分库分表规则，写入目标分库分表集群

#### 2、主动同步

​	在写入请求中主动触发同步模块进行迁移，同步模块在接收到请求后，立刻将数据同步至分库分表及缓存中

#### 3、兜底同步

​	主动触发同步模块的请求及同步模块本身运行都有可能出现异常，对于可能出现的异常情况，可以设计兜底策略进行处理

​	兜底的同步对于无状态存储中的数据按创建时间进行不断轮询，轮询会对超过设置的时间阈值（如 5S）仍未得到同步的数据进行主动同步

### 如何保证写入的可见性？

#### 1、写入成功，返回写入的全部数据

​	对于有时延要求的场景，当数据写入随机存储成功后，可以在请求返回前，主动地将数据写入缓存中，同时将此次写入的数据全部返回给前台。但此处并不强制缓存一定要写成功，缓存写入失败也可以返回成功。

#### 2、同步模块同步缓存

​	同步模块除了将数据同步至分库分表，也会写入到缓存中

#### 3、缓存可降级

​	因为主动写入缓存可能存在异常，导致数据未写入缓存，且主动数据同步和兜底同步是先写分库分表再通过 Binlog 刷新缓存，存在一定的延迟。

​	因此在查询时需要具备降级功能，当缓存中未查询到时，可以主动降级到数据库进行一次兜底查询，并将查询到的值存储至缓存中。

### 提高写性能

#### 1、将依赖的串行改并行

#### 2、依赖后置化

​		第一，在写入数据时，对于用户不太关心的数据信息，可以在重要的数据写入成功后，通过一个异步任务进行补齐。对于一些可以后置补齐的数据，可以在写请求完成时将原始数据写入一张任务表

​		第二，整个链路上会有较多外部接口，但大部分场景里，很多接口都是可以后置化的

#### 3、显式设置超时和重试

​	后置接口处理请求前，需要判断来自前置接口的请求是否已经超时，如果超时，直接返回请求超时，	后置接口的超时时间的设定应该考虑前置接口的超时时间

​	将超时时间设置为 TP999 和 Max 之间的值

​	自动重试只能设置读接口，设置自动重试是为了提高接口的可用性

#### 4、降级方式

​	1、对产生故障的依赖进行后置处理，比如需要风控的场景，写入的数据自己可见，其他人不可见，待风控故障恢复后再进行数据校验，校验通过后再允许所有人可见。通过有损+异步最终校验，也是一种常见的降级方案

​	2、对于需要写下游的场景，比如下单，库存故障时，可降级直接跳过库存扣减，但需要提示用户后续可能无货。修复故障后进行异步校验库存。

## 海量扣减请求

### 1、并发修改优化为串行修改

扣减请求，主要就是保证大并发请求时据库中的库存字段值不能为负数	

一种是在应用程序中通过事务来判断，即保证减后库存不能为负数，否则就回滚

一种办法是直接设置数据库的字段数据为无符号整数，这样减后库存字段值小于零时会直接执行 SQL 语句来报错

一种就是使用 CASE WHEN 判断语句，例如：UPDATE item SET inventory = CASE WHEN inventory >= xxx THENinventory-xxx ELSE inventory END

应对大并发的读库存，可以将库存放到缓存中，应用程序的本地缓存，也可以放到nginx中，当库存为0，不再请求后端服务

大并发扣减库存

如果秒杀商品的减库存逻辑非常单一，可以直接在缓存中减库存

如果有比较复杂的减库存逻辑，或者需要使用事务，你还是必须在数据库中完成减库存

由于 MySQL 存储数据的特点，同一数据在数据库里肯定是一行存储，因此会有大量线程来竞争InnoDB 行锁，而并发度越高时等待线程会越多，TPS会下降，响应时间会上升，数据库的吞吐量就会严重受影响。单个热点商品会影响整个数据库的性能

一个解决思路是遵循前隔离的原则，把热点商品放到单独的热点库中。但是会带来维护上的麻烦

一个解决思路是应用层做排队，按照商品维度设置队列顺序执行，这样能减少同一台机器对数据库同一行记录进行操作的并发度，同时也能控制单个商品占用数据库连接的数量，防止热点商品占用太多的数据库连接

在某些场景下有些多条 SQL（比如修改lastmodifytime修改时间字段） 是可以合并的，一定时间内只要执行最后一条 SQL 就行了，以便减少对数据库的更新操作

### 2、随机更新优化为顺序写

顺序写要比随机更新性能好

1、参数校验

2、开启事务

3、扣减明细插入任务数据库

​	任务库主要提供两个作用，一个是事务支持，其次是随机的扣减流水任务的存取;这两个功能均不依赖具体的路由规则，也是随机的、无状态的

4、插入失败，回滚事务

5、插入成功，进行缓存扣减

​		在Redis中的lua 脚本执行时，会按 SKU获取对应的剩余库存状态判断，如果此次扣减的数量大于剩余数量，直接返回错误提示数量不足；否则进行库存的扣减，返回成功

​		对于热门商品的扣减，往往集中到单台redis上，为避免海量请求超过redis负载，将库存按redis的数量进行平均等分，每一个缓存里均存储一等份即可。在处理秒杀请求时，不只是固定地命中某一个缓存分片，而是在每次请求时轮询命中缓存集群中的每一个缓存分片

6、缓存扣减失败，回滚

7、缓存扣减成功，事务提交，结束

缓存扣减失败，两种情况，第一种是扣减的数量不足，第二种就是缓存出现故障，缓存失败的可能性有很多，比如网络不通、调用缓存扣减超时、在扣减到一半时缓存突然宕机（故障 failover）了，以及在上述返回的过程中产生异常等。针对上述请求，都有相应的异常抛出，根据异常进行数据库回滚即可，最终任务库里的数据都是准的

## 预热

数据预热

​	模拟应用的访问，通过这种访问将数据加载到缓存和数据库中

应用预热

​	预先建立好连接，防止在高峰时建立连接

​	线程池预热，ThreadpoolExecutor提供了prestartAllCoreThreads方法可以预先启动核心线程 

​	JIT预热，JAVA代码是解释执行的，如果在高峰时同时做JIT编译，会消耗了大量的CPU，系统响应时间会拉长，通过JIT预热，保证代码可以提前充分编译

# 如何做需求分析和设计

不要想着产品经理给我产品设计文档（PRD）、线框图，我照着实现就可以了，这种想法有点狭隘

技术人员应该更多地参与到产品设计中

作为技术人，我该怎么做产品设计?首先，一定不要自己一个人闷头想，我们要学会“借鉴”，可以找几个类似的产品，看看他们是如何设计的，然后借鉴到我们的产品中

除了”借鉴”的思路之外，还可以通过产品的线框图、用户用例（user case ）或者叫用户故事（user story）来细化业务流程，挖掘一些比较细节的、不容易想到的功能点

用户用例，侧重情景化，其实就是模拟用户如何使用我们的产品，描述用户在一个特定的应用场景里的一个完整的业务操作流程。所以，它包含更多的细节，且更加容易被人理解

系统设计

面向对象设计聚焦在代码层面（主要是针对类），那系统设计就是聚焦在架构层面（主要是针对模块），两者有很多相似之处。很多设计原则和思想不仅仅可以应用到代码设计中，还能用到架构设计中

1、合理地将功能划分到不同模块

面向对象设计的本质就是把合适的代码放到合适的类中。合理地划分代码可以实现代码的高内聚、低耦合，类与类之间的交互简单清晰

系统设计实际上就是将合适的功能放到合适的模块中。合理地划分模块也可以做到模块层面的高内聚、低耦合，架构整洁清晰。

模块可以划分出多种，怎么判断哪种模块划分合理

通过看看是否符合高内聚、低耦合特性来判断，如果一个功能的修改或添加，经常要跨团队、跨项目、跨系统才能完成，那说明模块划分的不够合理，职责不够清晰，耦合过于严重

除此之外，为了避免业务知识的耦合，让下层系统更加通用，一般来讲，我们不希望下层系统（也就是被调用的系统）包含太多上层系统（也就是调用系统）的业务信息，但是，可以接受上层系统包含下层系统的业务信息

2、设计模块与模块之间的交互关系

在面向对象设计中，类设计好之后，我们需要设计类之间的交互关系。类比到系统设计，系统职责划分好之后，接下来就是设计系统之间的交互

比较常见的系统之间的交互方式有两种，一种是同步接口调用，另一种是利用消息中间件异步调用。第一种方式简单直接，第二种方式的解耦效果更好

3、设计模块的接口、数据库、业务模型

接口设计要符合单一职责原则，粒度越小通用性就越好

但是，接口粒度太小也会带来一些问题。比如，一个功能的实现要调用多个小接口，一方面如果接口调用走网络（特别是公网），多次远程接口调用会影响性能；另一方面，本该在一个接口中完成的原子操作，现在分拆成多个小接口来完成，就可能会涉及分布式事务的数据一致性问题

为了兼顾易用性和性能，我们可以借鉴facade（外观）设计模式，在职责单一的细粒度接口之上，再封装一层粗粒度的接口给外部使用

业务模型的设计

从代码实现角度来说，大部分业务系统的开发都可以分为 Controller、Service、Repository 三层。Controller 层负责接口暴露，Repository 层负责数据读写，Service 层负责核心业务逻辑，也就是这里说的业务模型

两种开发模式，基于贫血模型的传统开发模式和基于充血模型的DDD 开发模式

前者是一种面向过程的编程风格，后者是一种面向对象的编程风格

不管是 DDD 还是 OOP，高级开发模式的存在一般都是为了应对复杂系统，应对系统的复杂性

为什么要分 MVC 三层开发

分层能起到代码复用的作用、分层能起到隔离变化的作用、分层能起到隔离关注点的作用、分层能提高代码的可测试性、 分层能应对系统的复杂性

BO、VO、Entity 存在的意义是什么

针对 Controller、Service、Repository 三层，每层都会定义相应的数据对象，它们分别是 VO（View Object）、BO（Business Object）、Entity，例如 UserVo、UserBo、UserEntity

在实际的开发中，VO、BO、Entity 可能存在大量的重复字段，甚至三者包含的字段完全一样。在开发的过程中，我们经常需要重复定义三个几乎一样的类，显然是一种重复劳动

继承可以解决代码重复问题。我们可以将公共的字段定义在父类中，让VO、BO、Entity 都继承这个父类，各自只定义特有的字段

组合也可以解决代码重复的问题，可以将公共的字段抽取到公共的类中，VO、BO、Entity 通过组合关系来复用这个类的代码

不同分层之间的数据对象转化，比如BeanUtils、Dozer 等

要做架构升级，主要是要分析在预估的QPS下，整个系统的瓶颈会在什么地方，要针对这起瓶颈来重新设计架构方案

# 重构

重构是一种对软件内部结构的改善，目的是在不改变软件的可见行为的情况下，使其更易理解，修改成本更低

在保持功能不变的前提下，利用设计思想、原则、模式、编程规范等理论来优化代码，修改设计上的不足，提高代码质量

升级重构有两种常见的形式，一种是纯代码式的升级，另外一种是包含存储和代码的升级

包含存储和代码的重构升级是指在上述纯代码之外，将原有架构里的存储也一起升级。

存储升级有两种形式

第一种是将存储类型进行升级，比如将数据库升级为缓存，将原有的读接口从数据库切换至缓存

第二种是将一个表结构的存储升级为同类型存储的另外一个更加合理的表结构

纯代码重构的切换

​	通过灰度的方式，既可以做到纯代码的升级重构切换，又可以缩小因此可能带来的线上问题的影响范围

含存储重构的切换

​	含存储的重构切换有一个重要步骤便是数据迁移

​	对于含存储重构的切换，最简单的方法便是停服

​	切换架构

​		首先进行数据同步，在完成数据同步之后，便可以进行用户的灰度切量了，将用户逐步切换至升级重构的新版本

​       增量同步、全量同步、数据对比验证、

​	   用户切换

​			1、对于升级重构的系统涉及的所有用户进行分析并按等级划分，如果重构的模块是订单模块，可以将用户按历史以来的下单量、订单金额进行排序，订单量小、下单金额低的用户排在最前面

​			2、对上述用户进行数据分析，分析这些用户里哪些用户使用了较多的系统功能，使用功能较多的用户排序在前面。在切量时，使用系统功能最多的用户会优先进行切量

系统架构大体上类似，其中只有个别模块存在差异，但各个研发团队还是从零开始建设全部模块的方式，称为烟囱式架构

平台化是知识将重复模块进行融合，从降低技术重复的角度出发，从而提升效率

而中台化是在平台化之后，从业务复用的角度出发，进一步提升业务需求的效率

业务能力可视化：将业务逻辑可以直接在可视化平台上展现出来，业务方和产品经理不需要和研发来回沟通上周的时间来确认需求，可以极大地降低沟通时间，提升效率

业务能力配置化：上述可视化的业务流程中，有些节点时实心圆圈，有些是空心的圆圈。空心表示代码执行到此流程节点时会直接跳过，而实心表示会执行此流程节点。流程节点是为空心还是实心，是可配置的，此配置功能可以落地在上述介绍的可视化平台里

# 如何通过监控

## 微服务入口

1、次数监控

​	次数监控中被监控体就是次数，具体指微服务里各项代码逻辑运行的次数，可以是微服务对外提供接口的被调用次数、某一个方法被执行的次数

2、性能监控
	性能监控里的被监控体是性能，可以是微服务对外提供接口的性能、微服务依赖下游其他接口或存储的性能

3、可用率监控
	可用率里的被监控体是在指定时间里，代码执行成功的占比

1、需要设置调用次数报警，调用次数的报警阈值可以参考单机压测瓶颈值

2、按调用方设置调用次数监控

3、关于调用次数的阈值，需要设置调用次数的同环比监控

## 微服务自身

JVM、线程池数量、负载、cpu和内存的使用率

## 微服务依赖

并不是微服务内部的所有方法都要加监控，而是要挑选重点和可能存在问题的方法

如果是 Java 应用，监控的方式可以采用统一的 AOP 切面来实现。此外，也可以借助一些框架的功能统一拦截并进行监控，比如 MyBatis 里就提供用 Interceptor 拦截所有 SQL 的执行，在此处就可以添加统一的监控

# 日志收集

1、应用中开辟一个本地缓存，所有日志数据先存放在这个缓存中，然后后台线程通过异步的方式将缓存中的日志发送给服务端

2、设置缓存的大小，避免把内存撑爆

3、流量大时，可以配置采样比例控制日志的数据量

4、日志不需要永久保存，通常保存1/2个月的数据，可以结合公司的实际情况进行配置

# 联调阶段，依赖的接口没好怎么办？

编写Mock 接口服务，提供与正式服务的URI、出入参一样的接口，区别是主机名或者 URL 的前缀不一样

为了预防线上环境使用 Mock 服务，在服务启动时，如果判断当前的环境名称是prod，则判断 mock.apis 中是否有值，有值的话提示异常。

# 如何做写压测

方法：

第一种是采用模拟账号进行替换或数据修改（不可见）进行压测

第二种方式是采用压测数据打标 + 影子库的方式进行特殊处理

压测过程中有两方面重要的数据，一个是压测过程中的各项指标数据，另一个是压测的结果即服务所能够支撑的QPS。

压测过程的各项指标数据有：压测时机器的CPU 利用率的变化、内存的变化、进程里各线程的CPU 利用率、微服务依赖的存储的CPU利用率、内存使用率等。压测过程中监控这些数据是为了发现系统瓶颈点，并快速优化，进而提升微服务能够支撑的QPS

如果压测过程中，发现被压测应用的CPU 都被某一个或某一类线程消耗，同时通过堆栈信息，确定这个或这类线程的所有 CPU 消耗都集中在一个方法里

# BFF（Backend for Front）

BFF 不是一个架构，而是一个设计模式，它的主要职责是为前端设计出优雅的后台服务，即一个 API

一般而言，每个客户端都有自己的 API 服务，不同的客户端请求经过同一个网关后，它们都将分别重定向到为对应客户端设计的 API 服务中

API 层实现一下三点：


聚合：一个接口需要聚合多个后台服务返回的数据，并将数据返回给客户端。


分布式调用：一个接口可能需要依次调用多个后台服务，才能实现多个后台服务的数据修改。

装饰：一个接口需要重新装饰后台返回的数据，比如删除一些字段或者对某些字段进行封装，然后组成客户端需要的数据

API 服务其实就是一个 Spring Web 服务，它没有自己的数据库，主要职责是聚合、分布式调用及装饰数据，并通过 Feign 或者RPC调用后台服务

# 构建微服务高效法则

## 1、防备上游

第一个原则：增加接口调用鉴权 比如是否按照调用次序

第二个原则：接口里的入参需要是对象类型，而不是原子类型

第三个原则：接口的出入参不要设计为 Map<String,String> 等集合格式

第四个原则：入参需要增加条件限制和参数校验

第五个原则：写接口需要保证幂等性

第六个原则：接口返回的结果需要统一

第七个原则：返回的数据量必须要分页

第八个原则：所有的接口需要根据接口能力前置设置限流

## 2、做好自己

### 微服务 CPU 被打满如何排查

1、在 Linux 系统里，可以使用top 命令进行定位， top 命令可以按进程维度输出各个进程占用的 CPU 的资源并支持排序，同时还会显示对应的进程名称、进程 ID 等信息

2、top 命令支持查看指定进程里的各线程的 CPU 占用量，命令格式为：top -Hp 进程号。通过此方式便可以获得指定进程中最消耗 CPU 资源的线程编号、线程名称等信息

3、假设导致 CPU 飙升的应用是基于 Java 开发的，此时，便可以通过 Java 里自带的 jstack 导出该进程的详细线程栈信息

### 如何预防故障

部署层面
首先，微服务及存储需要双机房部署

其次，机房内至少部署两台及以上机器

再者，不同类型的接口需要单独部署,比如读服务和写服务

最后，至少要线程池隔离

代码层面

第一，不要基于 JSON 格式打印太多、太复杂的日志

第二，需要具有日志级别的动态降级功能

第三，for 循环不要嵌套太深，原则上不要超过三层嵌套

第四，多层嵌套需要有动态跳出的降级手段

第五，如果使用应用内的本地缓存，需要配置容量上限

## 3、怀疑下游

微服务会依赖很多其他微服务提供的接口、数据库、缓存，以及消息中间件等，这些接口及存储可能会因为代码 Bug、网络、磁盘故障、上线操作失误等因素引发线上问题

为了防止上述情况的发生，在构建微服务时，就需要预先考虑微服务所依赖的各项“下游”出现故障时的应对方案

### 对其他微服务的依赖

依赖后置、依赖并行化、设置超时和重试、服务降级等手段，来对它的依赖进行治理，进而保障服务的高可用

对于分布式事务，在不引入外部TCC 和 Saga 的成熟基础框架时，可以采用本地事务任务表的方法

### 对数据库的依赖

原则一：数据库一定要配置从库，且从库部署的机房需要与主库不同，从而保障数据库具备跨机房灾备的能力

原则二：在能够完成功能的前提下，使用的 SQL 要尽可能简单

原则三：在业务需求不断更新迭代的场景里，最好不要使用外键

原则四：表结构中尽可能不要创建一个长度为上千或上万的 varchar 类型字段，且用其来存储类似 JSON 格式的数据，因为这会带来并发更新的问题

### 对消息中间件的依赖

原则一：数据要先写入数据库或缓存后，再发送消息通知

原则二：发送的消息要有版本号*

原则三：消息的数据要尽可能全，进而减少消息消费方的反查

# 负载均衡

## 含义

将负载(访问的请求)“均衡”地分配到多个处理节点上。这样可以减少单个处理节点的请求量，提升整体系统的性能。

## 类别

代理类的负载均衡服务

以单独的服务方式部署，所有的请求都要先经过负载均衡服务，在 负载均衡服务中，选出一个合适的服务节点后，再由负载均衡服务，调用这个服务节点来实 现流量的分发。

四层负载LVS，七层负载nginx

一般会同时部署 LVS 和 Nginx 来做 HTTP 应用服务的负载均衡。也 就是说，在入口处部署 LVS，将流量分发到多个 Nginx 服务器上，再由 Nginx 服务器分发 到应用服务器上

客户端负载均衡服务

## 负载均衡策略

静态策略

在选择服务节点时，不会参考后端服务的实 际运行的状态

动态策略

依据后端服务的一些负载特性，来决定要 选择哪一个服务节点

Nginx 来说，我们要如何保证配置的服务节点是可用的呢?

淘宝开源的 Nginx 模块nginx_upstream_check_module了，这个模块可以 让 Nginx 定期地探测后端服务的一个指定的接口，然后根据返回的状态码，来判断服务是
否还存活。当探测不存活的次数达到一定阈值时，就自动将这个后端服务从负载均衡服务器 中摘除

如何动态更新nginx的up_stream

nginx-upsync-module

# 数据库

池化技术，数据库连接池

最小连接数

最大连接数

空闲时间

定时检测连接的可用性,，比如使用连接发送“select 1”的命令给数据库看是否会抛出异常(C3P0)

主从分离

分库分表

水平拆分

垂直拆分

任务库

​	优化随机更新为顺序写

​	异步将数据同步给真实的业务数据库

# 分布式系统设计策略

## 心跳检测

检测节点是否存活

 周期检测心跳机制、累计失效检测机制

## 高可用

经过设计来减少系统不能提供服务的时间 

### 主备（MS模式）

Active-Standby模式

当主机宕机时，备机接管主机的一切工作

在数据库高可用性方案中比较常用，如 MySQL、Redis等就采用MS模式实现主从复制。保证高可用

### 互备（MM模式）

两台主机同时运行各自的服务工作

一个系统存在多个master，每个master都具有 read-write能力

在数据库中通过双主，对外提供读写服务并且互相进行数据同步

### 集群

有多个节点在运行，同时可以通过主控节点分担服务请求

集群模式需要解决主控节点本身的高可用问题，一般采用主备模式。

### 脑裂问题

同时用两条心跳线路

引入仲裁机制，由第三方仲裁

Lease机制

## 容错性

IT系统对于错误包容的能力

保障分布式环境下相应系统的高可用或者健壮性

## 负载均衡

使用多台集群服务器共同分担计算任务，把网络请求分配到集群可用的不同服务器节点上，从而达到高可用性及较好的用户操作体验。

## 服务协调

主要用来解决分布式环境下多个进程之间的同步控制，有序的访问某种临界资源
基于缓存redis实现的分布式锁
		使用setnx加锁，并设置过期时间，超过时间自动释放，value值用来标识锁的持有者，释放锁时根据value来判断是否有资格释放锁
基于zookeeper实现的分布式锁
		创建一个目录，获取锁时在此目录下创建临时的顺序节点
		获取目录下的所有节点，如果自己是最小的节点，获取锁
		如果不是，设置监听比自己小的节点

## 服务削峰

​	为了应对短时间上亿用户的涌入，瞬间巨大的流量
​	削峰本质上，就是更多的延缓用户的请求，以及层层过滤用户的访问请求，遵从最后落地到数据库的请求尽量少的原则
​	消息队列削峰
​	流量削峰漏斗：层层削峰
​		CDN
​		缓存
​		后台系统
​		数据库系统

## 服务降级

​	保证核心或者基本服务的正常运行，可以将一些不重要或不紧急的任务进行服务的延时使用或者暂停使用
​	降级策略
​		缓存降级，使用缓存方式来降级部分读频繁的接口
​		读降级，直接禁止相关读的服务请求
​		写降级，直接禁止相关写的服务请求
​		延迟服务，如定时任务延迟处理，消息写入mq延迟处理

## 服务限流

​	通过对并发请求进行限速或者一个时间窗口内的请求数量尽心限速来保护系统，一旦达到限制的速率，则可以拒绝服务、排队或等待

### 	多维度限流

​		客户端
​			防重点击校验
​		nginx
​			限制单位时间内的请求数、限制同一时间的连接数
​		tomcat服务器
​			配置tomcat线程池、配置最大连接数、配置请求队列等参数
​		服务api接口
​			限制单位时间内的调用次数

### 	限流算法

​		计数器

 		固定时间窗口

​		滑动时间窗口
​		漏桶
​			请求会在桶内排队，请求会被匀速处理。服务器空闲时，理论上服务器可以直接处理一次洪峰，但是漏桶的机制是请求处理速率恒定，因此，前期服务器资源只能根据恒定的漏水速率逐步处理请求	

​			令牌桶

​			匀速产生令牌。服务器空闲时，是可以把令牌桶一下子装满 的，当洪峰来的时候可以直接拿取令牌，处理请求  

## 服务熔断

​	避免调用链中的异常请求，拖垮调用方甚至宕机
​	熔断器的工作机制主要是关闭、打开和半打开这三个状态之间的切换
​	在正常情况下，熔断器是关闭的
​	基于滑动窗口统计异常/超时的百分比，超过配置的阈值，熔断器打开；此时再发起请求，直接被熔断拦截
​	当熔断器打开一段时间后，会转为半打开状态。这时发送一个请求给服务端，如果能够正常地得到服务端的响应，则将状态置为关闭状态，否则设置为打开。



# 灰度发布

又名金丝雀发布，指在黑与白之间，能够平滑过渡的一种发布方式。
在其上可以进行 A/B testing， 即让一部分用户继续用产品特性 A，一部分用户开始用产品特性 B

灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调 整问题，以保证其影响度

服务分组或者版本号

# 代码坏味道

## 命名的坏味道

不精准的命名

命名过于宽泛，不能精准描述

命名要能够描述出这段代码在做的事情

好的名字应该描述意图，而非细节

用技术术语命名

解决：编写可维护的代码要使用业务语言，建立团队的词汇表

## 乱用英文的命名

违反语法规则的命名

不准确的英语词汇

英语单词的拼写错误

解决：制定代码规范，比如，类名要用名词，函数名要用动词或动宾短语；建立团队的词汇表；要经常进行代码评审

## 重复的代码、结构

复制粘贴的代码

结构重复的代码

if 和 else 代码块中的语句高度类似

解决：避免复制粘贴，提取一个新的函数出来，把公共的部分先统一掉。

## 长函数

把代码平铺直叙地摊在那里

需求变更只是每次增加了一点点

解决：分离关注点，避免多业务、多层次的代码写到一起

## 大类

职责不单一

字段未分组

解决：把不同的职责拆分开来；将不稳定的字段进行抽离

## 长参数

参数数量过多导致的长参数

标记参数导致的长函数

解决：把长参数封装成一个类；对于静态不变的参数，可以成为软件结构的一部分；根据标记参数，将函数拆分为多个函数

## 滥用控制语句

嵌套的代码

else语句

重复的switch

循环语句

解决：将循环中的内容封装成函数；不要使用else语句；多态取代条件表达式；以卫语句取代嵌套的条件表达式（卫语句（guard clauses）是一种改善嵌套代码的优化代码；编程规则：函数至多有一层缩进）

## 缺乏封装

过长的消息链

解决：隐藏委托关系；好的封装基于行为；以对象取代基本类型

## 变量的声明与赋值分离

变量一次性完成初始化，尽可能使用不变的量，在能够使用 final 的地方尽量使用 final

一次性完成变量的初始化

用声明式的方式进行集合的初始化

## 依赖混乱

## 不一致的代码

方案中的不一致，使用不同的程序库

命名的不一致

代码中的不一致，不同层次的代码耦合在了一起

解决：团队中使用同一的标准程序库，比如guava；分离关注点，分清楚代码应处的层次

## 落后的代码风格

引入Optional可以减少忽略空指针引起的问题

函数式编程

# 代码审查

实现方案的正确性

算法的正确性

代码的坏味道

及时评审

提升评审的频率，比如，每天评审一次

周期过长，累积的问题就会增多，造成的结果就是太多问题让人产生无力感

# 架构的目的

架构设计是为了解决软件复杂度带来的问题

通过熟悉和理解需求，识别系统复杂性所在的地方，然后针对这些复杂点进行架构设计

不是每个架构都具备高性能、高可用、高扩展等特点，而是要识别出复杂点然后有针对性地解决问题

复杂度的来源：高性能、高可用（计算和存储）、扩展性（伸缩、开闭原则）、安全性、成本

架构遵循的原则：合适原则、简单原则、演化原则

# 设计原则与思想

## 	单一职责原则

不要存在多于一个导致类变更的原因， 一个类只负责唯一职责。

不要设计大而全的类，要设计粒度小、功能单一的类。

如果一个类包含了两个或者两个以上业务不相干的功能，那我们就说它职责不够单一，应该将它拆分成多个功能更加单一、粒度更细的类

哪些情况就有可能说明类的设计不满足单一职责原则？

类中的代码行数、函数或者属性过多
类依赖的其他类过多，或者依赖类的其他类过多
私有方法过多
比较难给类起一个合适的名字
类中大量的方法都是集中操作类中的某几个属性

## 	里氏替换原则

​		在任何父类出现的地方可以用它的子类来替代。里式替换原则是用来指导，继承关系中子类该如何设计的一个原则。

​		里式替换原则，最核心的就是“design by contract，按照协议来设计”这几个字。父类定义了函数的“约定”（或者叫协议），那子类可以改变函数的内部实现逻辑，但不能改变函数原有的“约定”。这里的约定包括：函数声明要实现的功能；对输入、输出、异常的约定

## 	依赖倒置原则

​		 要依赖于抽象和接口，不要依赖于具体实现

## 	接口隔离原则

​		调用者不应该强迫依赖它不需要的接口

​		在设计接口时，不要设计出庞大膝肿的接口，因为实现这种接口时需要实现很多不必要的方法。

​		尽量设计出功能单一的接口，这样也能保证实现类的职责单一

## 	迪米特法则

也称最少知识原则，一个对象应该对其他对象保持最少的了解。简单来说，就是要求我们减低类间耦合

​	不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口

## 	 开放－封闭原则

​		程序要对扩展开放，对修改关闭。简单来说，当需求发生变化时，我们可以通过添加新的模块满足新需求，而不是通过修改原来的实现代码来满足新需求

## 组合原则

在软件复用时，要尽量先使用组合原则来实现，其次才考虑使用继承关系来实现

采用组合原则时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能

## DRY 原则

Don’t Repeat Yourself

不要写重复的代码，重复包括实现逻辑重复、功能语义重复和代码执行重复。

## KISS 原则

尽量保持简单

如何写出满足 KISS 原则的代码？

不要使用同事可能不懂的技术来实现代码

不要重复造轮子，要善于使用已经有的工具类库

## YAGNI 原则

英文全称是：You Ain’t Gonna Need It。	直译就是：你不会需要它。这条原则的核心思想就是：不要做过度设计

# 如何发现代码质量问题？

## 常规checklist

目录设置是否合理、模块划分是否清晰、代码结构是否满足“高内聚、松耦合”？
是否遵循经典的设计原则和设计思想（SOLID、DRY、KISS、YAGNI、LOD 等）？
设计模式是否应用得当？是否有过度设计？
代码是否容易扩展？如果要添加新功能，是否容易实现？
代码是否可以复用？是否可以复用已有的项目代码或类库？是否有重复造轮子？
代码是否容易测试？单元测试是否全面覆盖了各种正常和异常的情况？
代码是否易读？是否符合编码规范（比如命名和注释是否恰当、代码风格是否一致等）？

## 业务需求checklist

代码是否实现了预期的业务需求？
逻辑是否正确？是否处理了各种异常情况？
日志打印是否得当？是否方便 debug 排查问题？
接口是否易用？是否支持幂等、事务等？
代码是否存在并发问题？是否线程安全？
性能是否有优化空间，比如，SQL、算法是否可以优化？
是否有安全漏洞？比如输入输出校验是否全面？

# 如何快速地改善代码质量

## 命名

a.命名多长最合适？

在足够表达其含义的情况下，命名当然是越短越好。于一些默认的、大家都比较熟知的词，我比较推荐用缩写

对于作用域比较小的变量，我们可以使用相对短的命名，比如一些函数内的临时变量。相反，对于类名这种作用域比较大的，我更推荐用长的命名方式。

 b.利用上下文简化命名

c.命名要可读、可搜索

什么是命名可读。这里所说的“可读”，指的是不要用一些特别生僻、难发音的英文单词来命名

命名可搜索。我们在 IDE 中编写代码的时候，经常会用“关键词联想”的方法来自动补全和搜索

在命名的时候，最好能符合整个项目的命名习惯。大家都用“selectXXX”表示查询，你就不要用“queryXXX”；大家都用“insertXXX”表示插入一条数据，你就要不“addXXX”

d.如何命名接口和抽象类

对于接口的命名，一般有两种比较常见的方式。一种是加前缀“I”，表示一个 Interface。比如 IUserService，对应的实现类命名为 UserService。另一种是不加前缀，比如UserService，对应的实现类加后缀“Impl”，比如 UserServiceImpl。

对于抽象类的命名，也有两种方式，一种是带上前缀“Abstract”，比如
AbstractConfiguration；另一种是不带前缀“Abstract”。实际上，对于接口和抽象类，选择哪种命名方式都是可以的，只要项目里能够统一就行

## 注释

a.注释到底该写什么

注释的内容主要包含这样三个方面：做什么、为什么、怎么做。

函数和变量如果命名得好，确实可以不用再在注释中解释它是做什么的

b.注释是不是越多越好

类和函数一定要写注释，而且要写得尽可能全面、详细

函数内部的注释要相对少一些，一般都是靠好的命名、提炼函数、解释性变量、总结性注释来提高代码的可读性。

## 代码风格

a.类、函数多大才合适?

对于函数代码行数的最大限制，网上有一种说法，那就是不要超过一个显示屏的垂直高度

函数的代码行数不要超过一屏幕的大小，比如 50 行。类的大小限制比较难确定。



b.一行代码多长最合适？

一行代码最长不能超过 IDE 显示的宽度

需要滚动鼠标才能查看一行的全部代码，显然不利于代码的阅读



c.善用空行分割单元块

对于比较长的函数，如果逻辑上可以分为几个独立的代码块，在不方便将这些独立的代码块抽取成小函数的情况下，为了让逻辑更加清晰，除了用总结性注释的方法之外，我们还可以使用空行来分割各个代码块。

d.四格缩进还是两格缩进？

至于到底应该是两格缩进还是四格缩进，只要项目内部能够统一就行了

个人比较推荐使用两格缩进，这样可以节省空间

不管是用两格缩进还是四格缩进，一定不要用 tab 键缩进。因为在不同的 IDE 下，tab 键的显示宽度不同

e.大括号是否要另起一行？

比较推荐，将括号放到跟语句同一行的风格，节省代码行数

大括号跟上一条语句在同一行，还是另起新的一行，只要团队统一、业内统一、跟开源项目看齐就好了，没有绝对的优劣之分。

g.类中成员的排列顺序

在 Java 类文件中，先要书写类所属的包名，然后再罗列 import 引入的依赖类



在类中，成员变量排在函数的前面。成员变量之间或函数之间，都是按照“先静态（静态函数或静态成员变量）、后普通（非静态函数或非静态成员变量）”的方式来排列的


成员变量之间或函数之间，还会按照作用域范围从大到小的顺序来排列，先写 public成员变量或函数，然后是 protected 的，最后是 private 的。

## 关于编码技巧

将复杂的逻辑提炼拆分成函数和类

通过拆分成多个函数或将参数封装为对象的方式，来处理参数过多的情况

函数中不要使用参数来做代码执行逻辑的控制

函数设计要职责单一

移除过深的嵌套层次，方法包括：去掉多余的 if 或 else 语句，使用 continue、
break、return 关键字提前退出嵌套，调整执行顺序来减少嵌套，将部分嵌套逻辑抽象成函数

用字面常量取代魔法数

用解释性变量来解释复杂表达式，以此提高代码可读性

## 统一编码规范

# 如何根据功能需求，写代码

1、划分职责进而识别出有哪些类

根据需求描述，把其中涉及的功能点，一个一个罗列出来，然后再去看哪些功能点职责相近，操作同样的属性，可否应该归为同一个类

2、定义类及其属性和方法

识别出需求描述中的动词，作为候选的方法，再进一步过滤筛选。类比一下方法的识别，我们可以把功能点中涉及的名词，作为候选属性，然后同样进行过滤筛选

3、定义类与类之间的交互关系

泛化（Generalization）可以简单理解为继承关系

实现（Realization）一般是指接口和实现类之间的关系

组合（Composition）也是一种包含关系。A 类对象包含 B 类对象，B 类对象的生命周期跟依赖 A 类对象的生命周期，B 类对象不可单独存在

依赖（Dependency）是一种比关联关系更加弱的关系，包含关联关系。不管是 B 类对象是 A 类对象的成员变量，还是 A 类的方法使用 B 类对象作为参数或者返回值、局部变量，只要 B 类对象和 A 类对象有任何使用关系，我们都称它们有依赖关系

4、 将类组装起来并提供执行入口

类定义好了，类之间必要的交互关系也设计好了，接下来我们要将所有的类组装在一起，提供一个执行入口

这个入口可能是一个 main() 函数，也可能是一组给外部用的 API 接口。通过这个入口，我们能触发整个代码跑起来

# 如何实现代码的可测试性

1、 什么是代码的可测试性？

所谓代码的可测试性，就是针对代码编写单元测试的难易程度

对于一段代码，如果很难为其编写单元测试，或者单元测试写起来很费劲，需要依靠单元测试框架中很高级的特性，那往往就意味着代码设计得不够合理，代码的可测试性不好



2、编写可测试性代码的最有效手段

依赖注入是编写可测试性代码的最有效手段

通过依赖注入，我们在编写单元测试的时候，可以通过 mock 的方法解依赖外部服务



3、常见的 Anti-Patterns

常见的测试不友好的代码有下面这 5 种

代码中包含未决行为逻辑

所谓的未决行为逻辑就是，代码的输出是随机或者说不确定的，比如，跟时间、随机数有关的代码滥用



可变全局变量

全局变量是一种面向过程的编程风格，有种种弊端。实际上，滥用全局变量也让编写单元测试变得困难



滥用静态方法

静态方法跟全局变量一样，也是一种面向过程的编程思维。

在代码中调用静态方法，有时候会导致代码不易测试。主要原因是静态方法也很难 mock

只有在这个静态方法执行耗时太长、依赖外部资源、逻辑复杂、行为未决等情况下，我们才需要在单元测试中 mock 这个静态方法

如果只是类似Math.abs() 这样的简单静态方法，并不会影响代码的可测试性，因为本身并不需要mock



使用复杂的继承关系

相比组合关系，继承关系的代码结构更加耦合、不灵活，更加不易扩展、不易维护。实际上，继承关系也更加难测试



高度耦合的代码

如果一个类职责很重，需要依赖十几个外部对象才能完成工作，代码高度耦合，那我们在编写单元测试的时候，可能需要 mock 这十几个依赖的对象



Code Review 的内容

代码结构是否合理

代码是否容易理解

业务是否正确

异常考虑是否全面

是否有隐藏的 bug

线程是否安全

性能是否满足业务需求

是否符合编码规范

# 容量保障

互联网软件系统容量的概念，即“单位时间内软件系统能够承载的最大业务量”

容量保障，就是用各种方法保证软件系统的容量充足，预防容量隐患的重要工作

## 容量保障的目标是什么？如何度量？

### 目标

第一，以尽可能小的成本确保系统当前和未来的容量充足，即容量规划；
第二，解决已知的容量问题，预防未知的容量问题，即容量治理

### 度量

1、服务等级协议（SLA）

SLA 就是对服务可用性的一个保证，它可以细分为 SLI 和 SLO。

其中，SLI 定义测量的具体指标，如 QPS、带宽等；SLO 定义服务提供功能的期望状态，如 QPS 99 线≤100ms

SLA 用一个简单的公式来描述就是：SLA = SLO + 后果。 这里的后果指的是不满足 SLO情况下的补偿协议，比如赔款、延长使用期，等等。

2、QPS/TPS

QPS（Queries per Second）指的是每秒查询率，是一台服务器每秒能够响应的查询次数，一般针对读请求居多。
TPS（Transactions per Second）指的是每秒处理的事务数，一般针对写请求居多

通常说“系统容量是否足够”，一般就是指系统或服务能否在可接受的响应时间和成功率下支撑目标 QPS/TPS

在实际工作中，我们有时可能无法直接给出 QPS/TPS 的目标值，比如说，针对某个将要上线的大促活动，业务方预估的活动热度往往是以业务指标的形式呈现的，如PV（Page View）、UV（Unique View）、DAU（Daily Active User）等，我们需要将其转换为 QPS/TPS，才能作为容量保障可实施的技术目标

首先，A万 PV 是分布在整个活动期间的，但不一定是均匀分布的。假设我们根据二八原则，也就是 80% 的 PV 分布在 20% 的时间内，4.8 小时中将有 800 万 PV，平均每秒463PV。

然后再根据页面中接口的调用情况，计算出接口调用次数，就可以得出接口的预估QPS

3、 用户体验

有些容量问题尽管没有影响可用性，但会导致用户操作时响应延迟，页面打开缓慢等体验问题

用户体验完全可以作为容量保障更高级的目标

### 技术人员看待容量的视角

1、系统可用时长

2、关键路径正常

3、性能可接受范围

## 怎样科学实施容量测试？

容量测试是验证手段，不是测试手段。我们应该先努力设计和建造出满足容量要求的服务，再通过容量测试去验证它

### 确定容量测试的范围

针对容易产生容量风险的服务重点考虑进行容量测试

1、关键路径上的核心服务

2、有明显流量峰值特征的服务

3、对响应时间敏感的服务

4、占用资源大的服务

5、曾经发生过容量事故的服务

6、高峰期已经存在容量隐患的服务

7、新上线对容量情况未知的服务

### 科学实施容量测试

1、测试方案设计

测试背景、测试目标、测试模型和数据、测试计划、监控指标、止损策略

2、测试方案评审

在测试方案评审中，相关研发人员和测试人员都要参与评审，确保测试模型和数据合理，没有遗漏关键信息

3、测试准备

撰写测试脚本，准备测试数据。准备完毕后，调试脚本和数据，确保能够正常执行，服务无异常

准备的测试数据尽可能贴近真实的业务场景

4、测试执行

容量测试一般在线上执行居多，除非在线下能构建出完全对等的环境，否则即便将服务规模和硬件资源等比例缩放，容量的表现情况也不一定是线性的

容量测试是循序渐进的过程，逐步对目标服务施加压力，期间需要严密监控各项指标，一旦出现异常，应确保无风险的情况下才能继续施压。在达到容量目标值后，可以同时进行适当的摸高（在更高压力下维持一段时间）和脉冲（模拟瞬时的高并发场景），或对限流进行验证等工作

5、测试反馈

容量测试结束后，要有明确结论，总结测试过程中的各项指标和数据，与各方确认数据结论是否正常以及是否达到预期，编写测试报告，输出结论

6、持续跟进

容量测试不是单纯的测完就好，暴露的问题需要有效跟进，并在一定时间内跟踪解决，改进后确定时间再次进行验收，确保改进措施有效。

## 容量指标分析

响应时间应更关注分位线，我们常说的 TP95、TP99 或 95 线、99 线这类称呼
就是指分位线

响应时间越短越好，是建立在场景正确，服务无异常的基础上的，加强全方位的指标监控

CPU 利用率低只能说明 CPU 不忙，并不能说明在其他地方没有容量瓶颈

压不上去，不一定就是服务不行，有可能是施压端自身出现了瓶颈

当遇到容量指标抖动的情况，不要忽视它，尽可能找到根因

## 容量治理的三板斧：扩容、限流与降级

### 扩容的实践要点

要建立服务性能优化胜过简单扩容的意识，努力通过性能优化腾出容量，避免不经思考直接扩容

扩容要联动系统整体资源共同规划，不能只关注服务资源，还应同时关注对底层资源的影响

### 限流的实践要点

限流就是在控制成本的情况下对服务容量的一种保护措施

提前设置合理的限流对系统进行过载保护，是更主动的方式

与业务场景紧密相关的三大流量控制方式

流量整形： 指不管流量到达的速率多么不稳定，在接收流量后，都将其匀速输出的过程，即“乱进齐出”

容忍突发流量： 指的是限流策略允许流量在短时间内突增，且在突增结束后不会影响后续流量的正常限流

平滑限流： 指的是在限流周期内流量分布均匀，比如限制 10 秒内请求次数不超过
1000，平滑限流应做到分摊到每秒不超过 100 次请求

固定窗口无法容忍突发流量，但它实现简单，资源消耗少

如果你的应用服务经常需要应对诸如大促场景这样的突发流量，那么使用令牌桶算法进行限流往往会更适合

根据不同的限流位置，限流可以划分为网关层限流、接入层限流、应用层限流、数据库层限流等

### 降级的实践要点

降级是从系统功能角度出发，人为或自动地将某些不重要的功能停掉或者简化，以降低服务负载，这部分释放的资源可以去支持更核心的功能

降级与限流有明显的区别，前者依靠牺牲一部分功能或体验保住容量，而后者则是依靠牺牲一部分流量来保住容量

## 全链路压测

全链路压测本质上也是一种容量测试的手段，因此容量测试需要解决的问题也是全链路压测面对的重点问题，比如**数据隔离、压测场景置信度、对正常业务的影响**等

全链路压测遵循与用户访问相同的流量轨迹，因此会涉及大量的服务链路，我们需要保证压测流量在这些链路中流动时的完整性和可识别性。此外，有别于**单链路压测只需要制造局部流量，全链路压测需要制造大规模的整体流量**

### 应该如何建设

#### 三项改造工作

##### 数据隔离

常见的数据隔离方式有两种，分别是：逻辑隔离和物理隔离

逻辑隔离是指通过数据打标的方式区分真实数据和压测数据，在各实体（如用户、商户、订单等）中添加压测类型标识。比如针对用户这个实体，可以设置一个用户类型字段，其他系统或服务可以根据这个字段硬编码走相应的隔离逻辑

逻辑隔离实现简单，容易理解，但难以标准化，因为具体的字段设置是由业务技术方决定的。上游系统如果需要同时识别多种数据的压测标识时，会比较困扰。

物理隔离
它的做法是先通过在压测流量中进行打标的方式，区分真实请求和压测请求，再将压测请求所涉及的数据存储到另一个与真实表数据结构对等的表中（俗称影子表）

涉及两个重点工作，一是在压测流量中打标，二是建立影子表

通常情况下，流量入口大多是 HTTP 请求，压测流量标识可以置于 HTTP Header 中，进入内网后，相关中间件需要将 HTTP Header 中的压测流量标识转移至内部请求

要确保压测流量标识能够一路透传（传输过程中不对标识进行改变）至数据层不
丢失；最后，数据层，如数据库中间件通过对压测流量标识的识别，将数据写入影子表，完成整个物理隔离的全过程

建立影子表的过程

1、针对某张真实表建立相应的影子表，表名可以通过增加前缀或后缀区分，比如原表名为User，影子表名可以设定为 User_T，其他影子表也都基于 _T 这个后缀建立。
2、将真实表的数据进行脱敏，部分 id 类字段需要进行偏移，以免增长后与真实表冲突，比如真实的订单号都是以 1 开头，那么影子表中的订单号可以偏移为以 9 开头。
3、脱敏和偏移后的数据导入到影子表中。
4、进行完整性检查（数据量、表结构等内容），确保数据无误。

##### 中间件改造

对消息队列需要进行改造，当接收到带有压测请求标识的生产者推送消息时，需要将压测标识转存至数据中，以免丢失，当异步服务消费数据时，再将该标识恢复至请求体或上下文中继续传递

##### 应用服务改造

应用服务也需要进行一定的改造，确保压测请求能被反复执行，并且不影响真实场景

绕开限制逻辑： 比如系统针对短时间内反复下单的用户将进行限制，这个逻辑针对压测流量需要放开

Mock 逻辑： 有些对外交互的服务是不太方便发起大量真实请求的，比如支付和清结算等，这些功能可以在识别到压测流量后走 Mock 服务，也就是模拟一个正常的返回，而不是直接调用真实服务

#### 两项压测工作

##### 压测模型构建

业务模型（压测场景、业务模型、数据）

​	读请求
​		读请求不会对数据造成污染，因此可以直接使用真实请求和数据进行回放

​	写请求

​		写请求一般需要单独构造压测模型，并做好数据隔离和清理工作

压测模型构建的核心要点是，要利用好生产环境的各种信息来帮助我们构建贴近真实业务的压测模型

生产环境中的请求的依赖关系、调用比例、数据特征都是我们构建压测模型的素材，将这些数据抽取出来再进行精加工，即可得到贴合实际的压测模型

##### 压测流量构建

全链路压测对于压测流量构造的技术选型主要取决于流量的规模

如果规模不大，传统的压测工具是可以支持的，如 JMeter、Locust、nGrinder 等

如果是大规模流量乃至超大规模流量（百万请求量级），成本就会比较高。对于后者，可以考虑自研一套压测平台

## 怎样做好大促活动的容量保障工作

### 明确背景与目标

找到产品和业务方，了解大促的业务背景，最好能拿到 PRD（产品需求文档）
和具体的业务指标，先了解一下到底要举办哪些活动，活动的策略是什么，希望达到的效果是什么，活动周期和预估访问量是多少，等等

一定要获得量化的业务指标，没有也要让业务方拍个脑袋出来

最后，针对各类活动场景，拆解出容量保障的侧重点，说白了，就是将业务目标转换为容量保障的技术目标

### 重点链路梳理

1、同步链路

同步链路指的是，链路上的服务是强依赖的，调用方需要等待被调用方执行完成后才能继续工作

2、异步链路

异步链路与同步链路的概念正好相反，链路上的服务没有强依赖关系，调用方不需要等待被调用方执行完成，可以继续执行后续工作，被调用方在另一个线程内执行完成后，通过回调通知调用方

3、旁支业务链路

平时流量不大的“小”业务在大促场景下，流量是否会放大，是否会有叠加

4、高并发链路

类似秒杀和红包雨这类链路，可能瞬间会产生极高的并发量，这类链路要尽量做到集群物理隔离，分层过滤流量，再通过容量测试去检验效果

如何来梳理高并发链路

首先我们需要识别出高并发的“爆点” 在哪？也就是说，哪个环节，哪个接口，承受了最高的并发量

识别出爆点后，下面就要从两方面考虑：爆点的逻辑是否合理，能不能减少爆点的容量压力？比如其中的有些是否必要、是否可以缓存预热、是否可以异步执行等等

确定了合理的逻辑后，就可以规划容量测试方案了，方案需要尽可能与真实场景保持一致

最后，根据容量测试的结论，对整体链路进行合理的限流，预防容量事故的发生

梳理工作的思路都是类似的，先识别其中的容量风险（爆点），再看能不能通过优化链路设计减少容量压力，并结合容量测试不断接近目标，最后通过限流等手段合理控制容量风险

### 服务架构治理

#### 新服务和新功能的架构治理

对新服务和新功能的架构治理，倾向于通过绘制各种架构图（调用关系图、部署图、时序图，等等）的方式在架构评审时使用

调用关系图：调用关系图表示服务之间主要模块的交互关系，以及与外部系统的交互关系。绘制调用关系图的目的是为了完整识别出服务的上下游和外部依赖，这是容量保障工作所需要的基础信息

部署图：部署图描述服务部署的物理拓扑结构，在容量评估时我们需要知道服务的部署情况，来判断容量保障的范围

时序图：时序图描述服务对象之间的调用顺序，它可以帮助我们快速识别出同步调用和异步调用，在链路梳理时很有用

#### 存量服务的架构治理

由于时间成本的约束，从性价比的角度考虑，可以优先梳理历史上发生过的事故和冒烟

另一个切入点是代码评审，通过对已有代码进行审查，发现潜在的隐患，同时制定改进策略。评审工作由架构师牵头，联合研发与测试一同进行

代码逻辑：逻辑是否合理，有没有冗余的逻辑，有没有嵌套过深的逻辑

代码调用：调用是否恰当，是否存在重复调用

代码规范：代码可读性是否良好、可维护性是否良好

依赖管理：依赖关系是否严格，弱依赖是否明确

数据库使用：数据库字段是否合理，数据库连接池大小是否合适

中间件使用：Redis是否有热点key，MQ中的数据是否可以清除，有没有过度堆积的情况



### 大促流量预估

在梳理出核心场景链路后，对各服务进行流量预估时，特别要注意对下游服务的流量预估，一方面需要从核心场景入手计算，另一方面还要考虑到上游其他服务的依赖

上游负责制，简单说，所有依赖方应负责预估对被依赖方的调用量，并及时通知下游进行保障。对于调用量很高，且非常关键的调用方，下游服务甚至可以切割出一个物理隔离的集群，专门为这个调用方服务，以降低容量相互影响的可能性

### 大促容量测试

除了常态的全链路测试工作外，针对大促场景需要进行多次单链路压测，每次单链路压测都包含压测模型构建、压测数据准备、压测方案和计划准备、影响范围预估，以及监控和预案等步骤

在单链路压测通过后，需要再和全链路压测叠加进行混压，方能确保全局容量不出问题

### 大促容量保障预案

扩容、限流、降级等预案减少和避免损失

预案编写应非常具体，避免歧义，还要落实到具体的人，负责人、服务对象、执行条件、预案内容与操作步骤、影响面等五大要素
完善预案覆盖率是保证业务快速恢复的有效手段，为保障预案是可执行的，平时要进行演练，而且是频繁的演练，通过演练提升人员操作熟练度，减少误判

### 大促容量保障体系

首先，为规避一些单场景容量评估存在疏漏的情况，在各个大促场景的容量预估完成后，组织了一次全
局评审，全局评审的目的是拉齐各方之间的信息，查遗补漏，确实能发现一些单场景梳理缺失的信息

其次，在全场景混压前 1 小时，安排了一个简单的流程通气会，会议的内容是串讲压测流程，并现场答疑，答疑结束后立刻进入实战

最后，在大促活动完成后，应该及时组织复盘，总结经验，将不足之处尽可能多的暴露出来，以免将来再犯

## 建设经济实用型的容量保障体系？

### 方法一：粗放式保障

抓大放小，去做那些性价比高的事情，适当牺牲掉一些细节，这就是粗放式保障的思维

具体怎么去实施？或者说，哪些环节可以粗放式保障

在服务容量的观测能力方面，对容量指标的监控告警就可以是粗放式的，你可以针对服务的各项容量指标划定一个警戒值，比如 CPU 利用率≥80%，响应时间 99 线≥500ms，成功率≤95% 等，超过警戒值后，监控系统直接发出告警

对于那些趋势型的指标监控，还有带有业务语义的监控，实现的成本高，还要理解业务逻辑，就可以次要考虑

粗放式的容量保障并不是无谓地放弃一些工作，而是有策略地选择性价比高的“大头”去优先保障，以较低的成本堵住最严重的容量风险

### 方法二：善用云服务商的收费机制

云服务商也提供了弹性伸缩的服务，通过设置伸缩规则，在业务流量增长时能够自动增加实例，以保证计算力充足，反之则自动缩减实例。弹性伸缩是可以计量收费的

# 服务拆分维度

​	业务维度

​	功能维度

​	资源维度

​		高频高并发场景

​			商品详情	

​			优惠计算

​			热点数据、热点隔离

​		低频突发场景

​			秒杀

​			服务隔离

​		IO密集

​		CPU密集

用户维度

​	2C

​	2B

​	用户端

​	采购端

​	运营端

前后台业务

​	买家业务

​	卖家业务

​	运营业务

# 接口版本控制

RPC接口，消费方指定版本号，请求指定版本好的服务提供方

HTTP接口，Header中指定版本号，业务网关根据根据版本号路由到指定的服务

# 设计模式

对接口编程而不是实现编程,优先使用对象组合而不是继承

## 基石

封装

继承

多态

分类

## 创建型模式

怎么样创建对象
将对象的创建和使用分离
降低系统耦合度

单例

原型

工厂方法

抽象工厂

建造者

## 结构型模式

代理

适配器

装饰

门面

## 行为型模式

模板

策略

命令

职责链

状态

观察者

迭代者

# CAP理论

CAP 理论含义是，一个分布式系统不可能同时满足一致性（C:Consistency)，可用性（A: Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的2个

 C一致性 

​	分布式系统当中的一致性指的是所有节点的数据一致，或者说是所有副本的数据一致
A 可用性

 Reads and writes always succeed. 也就是说系统一直可用，而且服务一直保持正常
P 分区容错性 

系统在遇到一些节点或者网络分区故障的时候，仍然能够提供满足一致性和可用性的服务
在分布式系统当中，CAP三个特性我们是无法同时满足的，必然要舍弃一个。三者舍弃一个，显然排列组合一共有三种可能

# BASE理论

什么是BASE理论
BASE：全称：Basically Available(基本可用)，So state（软状态）,和 Eventually consistent（最终一致性）三个短语的缩写

BASE是对CAP中一致性和可用性权衡的结果，BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性

①Basically Available(基本可用)
基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性

②Soft state（软状态）
软状态指的是允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同
节点的数据副本之间进行数据同步的过程中存在延迟。

③Eventually consistent（最终一致性）
最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。

# 异地多活

异地就是指地理位置上不同的地方，多活就是指不同地理位置上的系统都能够提供业务服务

## 同城异区

将业务部署在同一个城市不同区的多个机房

应对机房级别故障的最优架构

## 跨城异区

业务部署在不同城市的多个机房，而且距离最好要远一些

有效应对这类极端灾难事件。跨城异地距离较远带来的网络传输延迟问题，给异地多活架构设计带来了复杂性。导致数据的不一致性

保证核心业务的异地多活

保证核心数据最终一致性

采用多种手段同步数据

存储系统自带的同步、消息队列同步，使用不同的网络连接，可以一个走内网、一个走公网

二次读取，接口访问和同步通道使用不同的网络连接

# 基于共识算法（Raft）的跨机房部署

## 两地三中心

一城市部署两个数据中心，另一城市部署一个数据中心。每个数据中心部署一个数据节点

能满足单个数据中心级的容灾，但是不能满足城市级容灾

## 三地五中心

5 个节点基本平分到了 3 个城市的数据中心，2个中心部署2个副本，1个中心部署1个副本

任何一个城市网络出现了问题，raft 协议都能正常运行，也是普遍使用的较高容灾级别的部署方式

# 数据安全

数据篡改

​	数据签名

请求重放

​	基于时间戳的解决方案、Token，请求幂等

存储安全

​	加密存储，明文数据不落盘，需知道明文的敏感数据加密存储，无需知道明文的数据使用hash存储

敏感信息

​	避免将用户隐私信息明文用于传输展示，服务端数据脱敏

Sql 注入

​	采用预编译Sql、数据检查

XSS 跨站攻击

​	输入输出检查，尽可能把检查逻辑放入后端以防绕过

CSRF 跨站请求

​	验证码、验证Referer、CSRF Token

# 后台系统的归类和共性

##  读业务（资讯类业务系统）

产品

​	微博、头条、知乎、抖音、快手

目的

​	尽可能的保证读的可用性和用户体验

功能要求

​	高可用，保证系统可读（即使不能写，但是需要可以浏览）

​	高性能（比如TP99要在100ms以内）

​	支持QPS非常高

实现

​	读业务的代码尽可能的简单

​	架构尽量不要分层，分层之后多一次网络传输

​	CQRS原则，将读写分离，分开优化

数据前置

​	浏览器，减少请求的次数，降低了延迟

​	CDN，缩短了传输距离，降低了延迟

缓存穿透

​	校验key的合法性，拦截无效请求

​	不存在的key存储空值，但是造成空间的浪费

​	布隆过滤器拦截

缓存集中过期导致雪崩

​	过期时间加上一个随机值，避免同一时间过期

​	多级缓存、前置缓存

nginx缓存

​	开启缓存功能，设置过期时间

​	开启请求合并功能，对于相同的请求只调用一次后台服务

数据变更导致的无法实时感知

​	采用Binlog方式，Canal+Mq

缓存数据压缩

​	将数据按照json格式序列化时，可以在字段上添加较短的替代标志，表示序列化后此字段的名称可以用替代标志进行表示；如果使用了redis中的Hash结构存储，也可以采取上述方式进行存储

热点数据的查询

​	缓存的主从，增加从节点 

​	为了应对热点数据的查询，增加从节点，并发性能并不会翻倍增加，而且造成了资源浪费

读服务中添加前置缓存模块

​	设置缓存容量的上限及逐出策略

​	数据变更感知的问题

​		定期刷新，有一定的延迟

​		实时感知，采用Binlog的方式在变更时主动刷新

控制穿透到缓存、数据库的流量

​		对于相同的查询请求，只有其中一个可以穿透到数据库，防止重复查询

流量超出预期，对读服务进行限流

## 写服务（外卖、打车）

保证写的高可用，支持海量数据的写入

分库分表

​	按用户分库

​	按照业务记录的唯一标志分库

分库分表中中间件

​	代理模式

​	内嵌模式

多维度查询

​	异构数据，通过Binlog方式，将数据导入ES

## 扣减服务（库存）

保障扣减的高可用，保证数据一致性，抗住高并发

纯数据库实现扣减

​	扣减库存、插入流水记录

基于缓存实现缓存扣减

​	1、Redis只支持单条命令的原子性，需要借助Lua实现批量扣减的原子性

​	2、Lua脚本中，首先判断数量是否满足，满足的话就扣减数量、记录流水

​	3、Lua扣减成功后，异步的将扣减内容保存至数据库

​	4、运营后台修改库存时需要同步增加至Redis

​	Redis宕机，扣减成功但是流水记录失败；异步刷新数据库失败；导致出现丢数据、少卖的情况。为了保证不出现少卖的情况需要做对账、异常处理等设计

缓存+数据库构建高可靠的扣减

1、扣减明细插入任务数据库（顺序插入；配置多个任务库，分摊写压力）
2、Redis扣减库存（运营后台修改库存需实时同步至Redis）
3、扣减成功，事务提交；扣减失败，事务回滚
4、异步将任务数据库的数据导入正式数据库

极端情况下，扣减成功后，还没给扣减服务返回响应，Redis就宕机，导致事务回滚，出现丢数据、少卖的情况。

应对热点缓存扣减*

引入秒杀库存分发器，将秒杀商品的库存平均分配到每个缓存分片

应对秒杀流量

扣减服务设置单机限流，实时并且精准；远程的集群限流多一次网络传输而且并不精准

引入兜底限流配置中心，推送每个分片在每台应用中的限流值，避免缓存分片被流量打垮

# 高并发系统分类

## 高并发读

策略1：加缓存

本地缓存或Memcached/Redis集中式缓存

MySQL的Master/Slave

CDN静态文件加速（动静分离）

策略2：并发读

异步RPC

Google的“冗余请求”

客户端同时向多台服务器发送请求，哪个返回得快就用哪个，其他的丢弃，但这会让整个系统的调用量翻倍

策略3：重写轻读

把计算逻辑从“读”的一端移到了“写”的一端

多表的关联查询：宽表与搜索引擎

策略4：读写分离（CQRS架构）

分别为读和写设计不同的数据结构

定时任务定期把业务数据库中的数据转换成适合高并发读的数据结构

## 高并发写

策略1：数据分片

数据库的分库分表

JDK的ConcurrentHashMap实现

Kafka的partition

ES的分布式索引

策略2：任务分片

Map/Reduce

Tomcat的1+N+M网络模型

策略3：异步化

LSM树（写内存+Write-Ahead日志）

凡是不阻碍主流程的业务逻辑，都可以异步化，放到后台去做

策略4：批量

Kafka的百万QPS写入

# 配置中心

## 开源实现

### Apollo

1、用户在Portal操作配置发布，
2、Portal调用Admin Service的接口操作发布
3、Admin Service发布配置后，发送ReleaseMessage给各个Config Service
4、客户端从Config Service拉取配置信息

客户端实现

本地模式

从本地文件读取文件配置信息；注册监听器ConfigChangeListener，监听配置的变更

远端模式

先从Config Service拉取配置信息，然后启动定时器，周期性的从远端拉取配置信息。
启动长轮询，获取发生变更的namespace，客户端获取到namespace后，会立即请求Config Service获取该namespace的最新配置。获取到配置信息后会保存到本地文件

### Nacos

客户端获取配置时优先从本地文件获取，如果获取不到，发送Http请求从远端获取，获取之后调用ConfigFilterChainManager的doFilter方法

注册监听器，监听配置的变化；后台通过长轮询获取变更的配置信息，并调用注册的监听器

客户端实现

拉取模式

Client主动拉取配置信息，服务端不需要对Client进行管理，但是Client不能及时获取更新的数据

推送模式

服务端主动推送最新的配置信息给Client，服务端需要对Client进行管理，服务端的复杂度上升

但是可以保证Client及时获取到最新的配置信息

Client连接比较多时，服务端可能出现瓶颈

对于配置下发敏感且Client数不是太大推荐使用推送模式

当Client数在万级以上推荐使用拉取模式，或者加入中间层，分担服务端的压力

# 注册中心

## 开源实现

### Eureka（AP)

#### EurekeServer

AP模型，实现最终一致性

高可用通过部署多个服务节点实现

每个节点存储全量的服务信息，各个节点都是平等的，无主从之分，每个节点会同步本节点的数据到其他节点

每台Server都需要存储全量的服务数据，Server的内存明显会成为瓶颈，扩容的效果不明显

如果在15分钟内80%的客户端节点都没有正常的心跳，那么Eureka就认为客户端与注册中心发生了网络故障，注册中心进入自我保护机制

在一定时间内没有接收到某个微服务节点的心跳，将会注销该微服务节点(默认90s)

#### 服务消费方

周期性向服务端主动pull服务数据（默认30s）

存在实时性不足以及无谓的拉取性能消耗的问题

#### 服务提供方

启动时，向EurekeServer注册自己的信息

周期性向EurekeServer发送心（默认30s）跳以续约自己的信息

### Zookeeper（CP）

当服务注册数、订阅数超过一定数量，不堪重负

写单点，不能水平扩展

按业务垂直拆分ZK集群

依赖ZK的Session活性心跳机制以及结合临时节点机制

### Nacos（AP/CP）

注册中心分别实现了AP和CP模型，根据namespaceId和serviceName判断选择CP还是AP模型（DelegateConsistencyServiceImpl）

AP模型实现，最终一致性

1、本地存储

2、触发数据变更事件，通过UDP协议向消费方推送变更的服务

3、定时向其他的注册中心同步变更的数据，最终每个Nacos server的数据都保持一致性

CP模型实现	RAFT协议

### Sofa-registry（AP）

支持海量的客户端，引入了中间层Session-Server，它是无状态的，很容易扩容

SessionServer承载客户端的连接，负责服务的发布和订阅
SessionServer也会对发布的服务进行保存，避免每次都请求DataServer
为了保持一致性，当DataServer上的服务信息发生了变更，及时推送变更的服务给SessionServer

DataServer负责数据存储；数据按照dataInfoId一致性哈希算法存储，支持多副本备份。当DataServer扩容时，对数据进行迁入迁出

## 技术要点

### 模型选择CP还是AP

AP可以保证可用性，但不保证消费方能拉取到最新的数据

CP可以保证一致性，但当集群中的多数节点宕机了不能保证可用性

### 服务发布大量注册写请求

将注册的时间随机化，避免同一时间注册

注册中心分片，将注册请求分散到其他节点

添加中间层，也可以缓存服务，合并重复的注册请求，再将合并后的请求发送给注册中心

服务提供方将注册中心的集群地址打散，避免注册请求都发送到同一台注册中心服务器

### 如何处理海量连接

添加中间层

### 如何处理连接超时

遍历扫描

动态分组（时间轮、桶）

### 注册中心容灾考虑

服务调用链路弱依赖注册中心，只有服务发布、下线、扩缩容等必要时依赖

客户端设计考虑容灾，缓存拉取的服务信息

把所有服务清单列表同步一份到配置中心，注册中心发生故障后，可以从配置中心拉取数据

### 服务状态检测

探活，不仅是探测服务提供方是否宕机，还要探测是否可以对外提供正常的服务

服务提供方主动上报心跳，注册中心周期性检测服务提供方是否在一段时间内发送心跳

服务消费方主动向服务提供方发起健康检查，根据一段时间内的成功响应的比率去判断下次是否继续向其发送请求

### 如何优雅下线服务

1、通知注册中心，此服务已下线，从服务列表中删除

2、注册中心通知消费方此服务已经下线，从服务列表中剔除，不再给服务提供者发送请求

3、通知服务提供者被迫下线。服务提供者接受到下线请求后，不再接受后续的请求，并且等待一定的时间将已经接受的请求处理完毕，如果在规定的时间未处理完成，则强制关闭

### 避免通知风暴

如果多个服务集群同时上线或者发生波动时，注册中心推送的消息就会更多，会占用过多的带宽资源

对变更的信息合并压缩

分批间隔发送

### 保护策略

注册中心的自我保护

​		避免发生网络抖动，导致大量的心跳超时，进而剔除大量的服务

客户端自我保护

​		注册中心因为高负载，推送了异常数据。客户端监测服务的可用节点数量下降超过一定阈值，进入自我保护，放弃使用新推送的服务注册信息

灵活方便的扩缩容

方便支撑海量的客户端连接和海量的服务存储

对于海量客户端连接的问题，可以通过添加中间层解决，中间层无状态可以灵活扩容

### 对于海量存储

1、可以将注册中心集群进行分组，服务提供方注册时随机选择某一个分组进行注册，每一组的集群负责存储一部分的服务注册请求

2、基于一致性 Hash 做了数据分片，每台注册中心节点只存储一部分的数据，随着数据规模的增长，只要添加注册中心节点即可

# 分布式事务

## 解决方案

​	seata

​		AT

​		TCC

​		SAGA

​	ServiceComb Pack

​		TCC

​		SAGA

​	RocketMq半消息

​			1、先发送半prepare请求，Broker写消息到HalfTopic

​			2、执行事务操作，事务提交成功，发送commmit请求，将half消息保存到真实队列中，表示消费者可以消费

​		3、Broker定时扫描半消息，然后回调客户端提供的事务回查方法，判断对应的事务是否已经提交成功

​	本地事务表+消息中间件

​		    1、业务操作数据和事务信息存储到同一数据库

​			2、事务提交成功时，将消息发送到消息中间件。若发送成功，将本地事务表中的状态修改为已发送；若发送失败，则重试；后端开启定时任务，扫描本地事务表，将未发送的消息发送到消息中间件。事务提交失败，则将事务回滚		

​	单机事务处理

​			避免分布式事务，所有的事务操作在单台机器通过本地事务上执行，然后定时将事务操作复制到对应的真实的数据库中

## 分布式事务理论

TCC（Try-Confirm-Cancel））

概念

​	一阶段：Try（检测预留资源）

​	二阶段：Confirm（真正的业务操作提交）/Cancel（预留资源释放）

注意事项

​	空回滚：Try未执行，Cancel执行了

​		Try 超时（丢包），分布式事务回滚，触发 Cancel

​		允许空回滚

​	悬挂：Cancel 比 Try 先执行

​		Try 超时（拥堵），分布式事务回滚，触发 Cance，拥堵的 Try 到达

​		拒绝空回滚后的Try 操作

幂等控制

FMT 模式（（Framework-managed transaction）

框架管理事务，托管事务的所有操作，一阶段和二阶段操作均由框架自动生成

一阶段：执行用户SQL，并且保存一条事务日志，包含修改前的数据快照和修改后的数据快照

二阶段：框架自动生成“提交/回滚”操作。提交:删除事务日志；回滚：如果当前的数据是和事务日志中记录的修改后的快照相同，则根据修改前的数据快照进行还原

## 优化点

二阶段异步执行

# 链路追踪

最小的业务代码入侵系统

日志的记录收集对服务器的性能不会产生影响

丰富的查询统计功能

SkyWalking

zipkin

# Service Mesher

servicecomb-mesher（华为）

Mosn（蚂蚁）

Istio

Linkerd

# 分布式ID

特性

​	全局唯一

​	有序

​	具有业务意义

​	保证高可用

​	对数据库友好，占用空间不要太长

实现

​	UUID

​	Redis

​	Zookeeper顺序节点

​	Mongodb

​	Mysql自增ID

​	Snowflake（twitter）

​			整体长度通常是 64 （1 + 41 + 10+ 12 = 64）位，适合使用 Java 语言中的 long 类型来存储；头部是 1 位的正负标识位。紧跟着的高位部分包含 41 位时间戳，通常使用 System.currentTimeMillis()。后面是 10 位的 WorkerID，标准定义是 5 位数据中心 + 5 位机器 ID，最后的 12 位就是单位毫秒内可生成的序列号数目

​	Leaf-segment（美团）

​	tinyid（滴滴）

​	[seqsvr（微信）](https://www.infoq.cn/article/wechat-serial-number-generator-architecture/)

疑问：真的需要集中式的全局唯一Id吗？

# 分布式任务调度

## ElasticJob

使用 jar 的形式提供分布式任务的协调服务

引入分片的概念，将一个任务分解成多个独立的子任务

借助quartz框架，实现Job接口，注入自定义的ElasticJob，实现分布式任务调度

每个任务都有自己的Leader节点，创建Leader节点的服务器为主服务器，在任务开始调度的时候，需要为每个任务节点分配执行的分片项。分片仅可能发生在下次任务触发前

提供了失效转移和错过任务重新执行的功能，但是只适用于运行耗时较长且间隔较长的场景

运维平台

​	 读取作业注册中心展示作业配置和状态，展现作业运行轨迹及执行状态，或更新作业注册中心数据修改作业配置

[Saturn](https://github.com/vipshop/Saturn)

# 限流、熔断、降级 

## [SDS(Service Downgrade System)](https://github.com/didi/sds)

轻量级、简单、易用的限流、熔断、降级系统
sds-client做为客户端，提供了限流、熔断和数据统计等功能
sds-admin作为Server端主要是为了配置降级策略、提供丰富的仪表盘并且保存客户端上传的统计数据

## [Sentinel](https://github.com/alibaba/Sentinel)

以方法为单位，基于滑动窗口，统计过去一段时间内的调用信息，比如调用次数、异常次数、超时时间等等，执行降级策略

## 注意事项

注意数据一致性的问题和用户体验的问题

## 资源隔离

服务分组

线程资源隔离，服务端按请求分队列隔离；调用方工作线程隔离，不同的服务调用使用不同的线程

信号量限制调用的并发数量，无法控制超时时间

## 服务熔断

可熔断的服务

​	非核心、非必要的服务

熔断触发

​	主动触发

​	被动触发

## 降级

有损提供服务，服务柔性可用

浏览计数返回假数据、推荐搜索返回兜底数据，用户不易感知

## 断路器设计

当对下游服务的调用异常量达到设定阈值后，打开断路器，触发熔断，在熔断期内，不会调用调用下游服务，直接调用降级方法；超过了熔断器，尝试调用下游服务，如果服务响应正常，则关闭断路器，否则断路器继续打开状态

状态流转

CLOSE关闭状态

异常量达到阈值，OPEN打开状态

OPEN状态超过一定时间后转为HALF状态

尝试调用下游服务，调用成功，则改为CLIOSE状态，否则处于OPEN状态

# 反向代理

描述

代理服务器，反向代理隐藏了真实的服务端，帮客户端把请求转发到真实的服务器

意义

承载海量连接

保证内网的安全

缓存加速Web请求

负载均衡

分类

硬件反向代理

F5

Netscale

A10

软件反向代理

LVS

主要用于多服务器的负载均衡，工作在网络层

Nginx（七层负载）

模块化

核心模块

标准HTTP模块

可选HTTP模块

第三方模块

事件驱动

异步

单进程

master进程

 监视工作进程的状态

当工作进程死掉后启动新进程

处理信号和通知工作进程

worker进程

处理客户端请求

从主进程处获得信号做相应的处理

非阻塞

  reload

 nginx -s reload

不重启nginx动态加载配置

启动新的woker进程，等待旧的的进程将待处理的请求处理完成或者超过一定时间，将旧的进程关闭。新的进程依次接收请求进行处理

dyups

动态配置upstream

https://github.com/yzprofile/ngx_http_dyups_module

# RPC设计

## 关键特性

### 服务注册/发现

​			选择CP还是AP。在设计超大规模集群服务发现系统的时候，舍弃强一致性，更多地考虑系统的健壮性.
最终一致性才是分布式系统设计中更为常用的策略

### 健康检测

​			判断服务的可用性
​			“连续”心跳失败次数必须到达某一个阈值。如果节点的心跳日志只是间歇性失败，也就是时好时坏，这样，失败次数根本没到阈值，调用方会觉得它只是“生病”了，并且很快就好了。那怎么解决呢？
​			基于滑动窗口，统计成功次数的百分比，可用率低于某个阈值时，进行降权处理

### 路由策略

​			在负载均衡之前，选择符合发送规则的节点
​			IP路由、参数路由

### 集群策略

​			快速失败
​			故障转移（重试）
​			失败自动恢复，后台记录失败请求重选节点定时重发，通常用于消息通知操作

### 负载均衡策略

​			随机
​			轮询
​			本地服务优先
​			一致性哈希
​			最少连接
​			最小耗时
​			自适应（根据服务的处理能力，智能地控制发送给每个服务节点的请求流量）
​				收集运行信息，包括服务器的CPU核数、负载、内存等指标
​				收集耗时数据，如平均耗时、TP99、TP9999

### 异常超时重试

​			业务逻辑必须是幂等的
​			重试时，去掉之前有问题的节点
​			在每次触发重试之前，我们需要先判定下这个请求是否已经超时，如果超时了会直接返回超时异常，否则我们需要重置下这个请求的超时时间
​			加个重试异常的白名单，用户可以将允许重试的异常加入到这个白名单中
​			时间轮实现超时

### 优雅启动

​			启用预热
​				让刚启动的服务提供方应用不承担全部的流量，而是让它被调用的次数随着时间的移动慢慢增加，增加其权重
​			延迟暴露
​				应用启动完成后再将服务注册
​				在服务提供方启动后，接口注册到注册中心前，模拟调用逻辑，从而使 JVM 指令能够预热起来，并且也可以事先预加载一些资源，只有等所有的资源都加载完成后，最后才把接口注册到注册中心

### 优雅关闭，服务下线

​			服务提供方通知注册中心，注册中心通知订阅该服务的消费方；涉及两次网络通信，不能保证实时性
​			服务提供方主动通知消费方
​			实现
​				服务方注册关闭的钩子（Runtime.addShutdownHook ），捕获关闭事件
​				开启关闭标志，通知消费方此服务已下线，并且拒绝接受新的请求
​				等待一定的时间，保证已经接受的请求处理完成
​				对请求进行计数，接入请求计数加1，处理完计数减1，计数为0，唤醒阻塞中的线程，进行资源的释放

### 熔断限流

​			限流（服务方的自我保护）
​				单机限流
​					每个服务节点独立去执行限流逻辑
​				集群限流
​					依赖一个限流服务。调用端在发出请求时，先调用限流服务，如果达了限流阈值，返回限流异常
​			熔断（调用方的自我保护）
​				避免调用链中的异常请求，拖垮调用方甚至宕机
​				熔断器的工作机制主要是关闭、打开和半打开这三个状态之间的切换
​				在正常情况下，熔断器是关闭的
​				基于滑动窗口统计异常/超时的百分比，超过配置的阈值，熔断器打开；此时再发起请求，直接被熔断拦截
​				当熔断器打开一段时间后，会转为半打开状态。这时发送一个请求给服务端，如果能够正常地得到服务端的响应，则将状态置为关闭状态，否则设置为打开。

### 		业务分组（流量隔离）

​			不同的调用方，调用流量不同。当某个调用方的流量突增时，可能影响到其他调用方，导致整体的可用性降低
​			为业务方划分不同的分组，每个业务方只能调用指定分组内的服务
​			根据业务的重要程度的不同划分核心服务和非核心服务，当核心服务的流量突增时，为保证核心服务的可用性，可以将其他业务分组下的服务移到核心服务分组下
​			由于分组之后，业务调用方所能调用的服务减少，出错的概率就升高了，保证调用方的高可用，在调用方配置多个分组，并将配置的分组区分下主次分组，只有在主分组上的节点都不可用的情况下才去选择次分组节点，只要主分组里面的节点恢复正常，就把流量都切换到主分组

## 基础

### RPC通信流程

​			发布 RPC 服务，并将服务注册到服务注册中心上
​			当引用这个服务的应用启动时，会从服务注册中心订阅到相应服务的元数据信息。
​			当服务引用方拿到地址以后，就可以从中选取地址发起调用了

### 可扩展且向后兼容的协议

### 序列化

​			实现
​				JSON序列化
​				JDK 原生序列化
​				Protobuf
​				Hessian
​			优化点
​				对象尽量简单，没有太多的依赖关系，属性不要太多
​				入参对象与返回值对象体积不要太大，更不要传太大的集合
​				对象不要有复杂的继承关系

### 通信模型

​			Reactor模型
​			Proactor模型

### 动态代理

​			JDK动态代理
​			CGLib动态代理
​		功能点扩展
​			SPI机制

### 线程模型

​			IO线程池
​			业务线程池
​				接口级别的线程池
​				方法级别的线程池

### 调用方式

​			oneway
​			同步
​			异步
​				Future
​				Callback

其他		
		方法级别的缓存，减少请求次数
		请求合并，提高了吞吐量，但是增加了延迟

## 开源项目

​	octo-rpc（美团）
​	motan（微博）
​	dubbo（阿里）
​	sofa-rpc(蚂蚁)

# DDD领域驱动设计

DDD 包括战略设计和战术设计两部分

战略设计主要从业务视角出发，建立业务领域模型，划分领域边界，建立通用语言的限界上下文，限界上下文可以作为微服务设计的参考边界

战术设计则从技术视角出发，侧重于领域模型的技术实现，完成软件开发和落地，包括：聚合根、实体、值对象、领域服务、应用服务和资源库等代码逻辑的设计和实现

如何划定领域模型和微服务的边界

第一步：在事件风暴中梳理业务过程中的用户操作、事件以及外部依赖关系等，根据这些要素梳理出领域实体等领域对象。

第二步：根据领域实体之间的业务关联性，将业务紧密相关的实体进行组合形成聚合，同时确定聚合中的聚合根、值对象和实体。

第三步：根据业务及语义边界等因素，将一个或者多个聚合划定在一个限界上下文内，形成领域模型。限界上下文之间的边界就是未来微服务的边界

领域具体指一种特定的范围或区域。领域可以进一步划分为子领域。我们把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围。领域可以通过细分为子域的方法，来降低研究的复杂度

决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力

没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域

还有一种功能子域是必需的，但既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，它就是支撑域。

在公司领域细分、建立领域模型和系统建设时，我们就要结合公司战略重点和商业模式，找到核心域了，且重点关注核心域。

通用语言定义上下文含义，限界上下文则定义领域边界

什么是通用语言？

在事件风暴过程中，通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言。它可以解决交流障碍这个问题

限界上下文可以拆解为两个词：限界和上下文。限界就是领域的边界，而上下文则是语义环境。通过领域的限界上下文，我们就可以在统一的领域边界内用统一的语言进行交流

领域专家、架构师和开发人员的主要工作就是通过事件风暴来划分限界上下文。限界上下文确定了微服务的设计和拆分方向，是微服务设计和拆分的主要依据

## 实体和值对象

实体

在 DDD 中有这样一类对象，它们拥有唯一标识符，且标识符在历经各种状态变更后仍能保持一致

领域模型中的实体是多个属性、操作或行为的载体

实体和值对象是组成领域模型的基础单元

在代码模型中，实体的表现形式是实体类，这个类包含了实体的属性和方法，通过这些方法实现实体自身的业务逻辑。

实体以 DO（领域对象）的形式存在，每个实体对象都有唯一的 ID

DDD 是先构建领域模型，针对实际业务场景构建实体对象和行为，再将实体对象映射到数据持久化对象

值对象

通过对象属性值来识别的对象，它将多个相关属性组合为一个概念整体

DDD 提倡从领域模型设计出发，而不是先设计数据模型

 聚合和聚合根：怎样设计聚合

聚合

在 DDD 中，实体和值对象是很基础的领域对象。实体一般对应业务对象，它具有业务属性和业务行为；而值对象主要是属性集合，对实体的状态和特征进行描述

聚合就是由业务和逻辑紧密关联的实体和值对象组合而成的，聚合是数据修改和持久化的基本单元，每一个聚合对应一个仓储，实现数据的持久化

聚合有一个聚合根和上下文边界，这个边界根据业务单一职责和高内聚原则，定义了聚合内部应该包含哪些实体和值对象，而聚合之间的边界是松耦合的。

聚合在 DDD 分层架构里属于领域层，领域层包含了多个聚合，共同实现核心业务逻辑。聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚

聚合根

聚合根的主要目的是为了避免由于复杂数据模型缺少统一的业务规则控制，而导致聚合、实体之间数据不一致性的问题

如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，它不仅是实体，还是聚合的管理者

首先它作为实体本身，拥有实体的属性和业务行为，实现自身的业务逻辑

其次它作为聚合的管理者，在聚合内部负责协调实体和值对象按照固定的业务规则协同完成共同的业务逻辑

最后在聚合之间，它还是聚合对外的接口人，以聚合根 ID 关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。

怎样设计聚合

第 1 步：采用事件风暴，根据业务行为，梳理出在业务中发生这些行为的所有的实体和值对象

第 2 步：从众多实体中选出适合作为对象管理者的根实体，也就是聚合根。判断一个实体是否是聚合根，你可以结合以下场景分析：是否有独立的生命周期？是否有全局唯一 ID？是否可以创建或修改其它对象？是否有专门的模块来管这个实体

第 3 步：根据业务单一职责和高内聚原则，找出与聚合根关联的所有紧密依赖的实体和值对象。构建出 1 个包含聚合根（唯一）、多个实体和值对象的对象集合，这个集合就是聚合

第 4 步：在聚合内根据聚合根、实体和值对象的依赖关系，画出对象的引用和依赖模型。

第 5 步：多个聚合根据业务语义和上下文一起划分到同一个限界上下文内

聚合的一些设计原则

聚合用来封装真正的不变性，而不是简单地将对象组合在一起

设计小聚合

 通过唯一标识引用其它聚合

在边界之外使用最终一致性

通过应用层实现跨聚合的服务调用

## 领域事件：解耦微服务的关键

在事件风暴（Event Storming）时，我们发现除了命令和操作等业务行为以外，还有一种非常重要的事件，这种事件发生后通常会导致进一步的业务操作，在 DDD 中这种事件被称为领域事件

领域事件是 DDD 的一个重要概念，在设计时我们要重点关注领域事件，用领域事件来驱动业务的流转，尽量采用基于事件的最终一致，降低微服务之间直接访问的压力，实现微服务之间的解耦，维护领域模型的独立性和数据一致性

DDD 分层架构包含用户接口层、应用层、领域层和基础层

DDD 分层架构有一个重要的原则：每层只能与位于其下方的层发生耦合

DDD 分层架构模型就属于严格分层架构，任何层只能对位于其直接下方的层产生依赖

在严格分层架构中，领域服务只能被应用服务调用，而应用服务只能被用户接口层调用，服务是逐层对外封装或组合的

# 设计模式

面向对象编程是一种编程范式或编程风格。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石 

面向对象分析就是要搞清楚做什么，面向对象设计就是要搞清楚怎么做。两个阶段最终的产出是类的设计，包括程序被拆解为哪些类，每个类有哪些属性方法、类与类之间如何交互等等

封装、抽象、继承、多态

封装也叫作信息隐藏或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式（或者叫函数）来访问内部信息或者数据

抽象讲的是如何隐藏方法的具体实现，让调用者只需要关心方法提供了哪些功能，并不需要知道这些功能是如何实现的。

抽象作为一种只关注功能点不关注实现的设计思路

定义类的方法的时候，也要有抽象思维，不要在方法定义中，暴露太多的实现细节

继承是用来表示类之间的is-a 关系,继承最大的一个好处就是代码复用,但是过度使用继承，继承层次过深过复杂，就会导致代码可读性、可维护性变差。多用组合少用继承

多态是指，子类可以替换父类，在实际的代码运行过程中，调用子类的方法实现，多态特性能提高代码的可扩展性和复用性、

抽象类具有哪些特性

抽象类不允许被实例化，只能被继承

抽象类可以包含属性和方法

子类继承抽象类，必须实现抽象类中的所有抽象方法

接口都有哪些特性

接口不能包含属性
接口只能声明方法，方法不能包含代码实现
类实现接口的时候，必须实现接口中声明的所有方法

抽象类既然属于类，也表示一种 is-a 的关系。相对于抽象类的 is-a 关系来说，接口表示一种 has-a 关系，表示具有某些功能。对于接口，有一个更加形象的叫法，那就是协议（contract）

抽象类更多的是为了代码复用，而接口就更侧重于解耦。接口是对行为的一种抽象，相当于一组协议或者契约。解决解耦问题，隔离接口和具体的实现，提高代码的扩展性。

 如何理解“高内聚、松耦合”？

高内聚，就是指相近的功能应该放到同一个类中，不相近的功能不要放到同一类中

松耦合指的是，在代码中，类与类之间的依赖关系简单清晰。即使两个类有依赖关系，一个类的代码改动也不会或者很少导致依赖类的代码改动

# 性能测试

在性能市场上，我们总要用具有普适性的指标说明，而不是用混乱的体系。对于TPS中T一定要有一个清楚的定义

用 TPS 来承载“并发”这个概念。并发数是 16TPS，就是 1 秒内整个系统处理了 16 个事务

性能测试分析的能力阶梯视图

1、工具操作：包括压力工具、监控工具、剖析工具、调试工具

2、数值理解：包括上面工具中所有输出的数据

3、趋势分析、相关性分析、证据链分析：就是理解了工具产生的数值之后，还要把它们的逻辑关系想明白

4、最后才是调优：有了第 3 步之后，调优的方案策略就有很多种了，具体选择取决于调优成本和产生的效果

性能分析思路

瓶颈的精准判断
TPS 曲线

之前有很多人在描述性能测试的过程中，说要找到性能测试中曲线上的“拐点”。我也有明确说过，大部分系统其实是没有明确的拐点的

当响应时间增加了，但是TPS 增加得却没有那么多时，可以是认定是拐点，性能瓶颈在加剧，越往后就越明显

准确说所有的系统都有性能瓶颈，只看我们在哪个量级在做性能测试了

TPS 随着压力的变化而变化，那就是有关系。不管压力增不增加，TPS 都会出现曲线趋势问题，那就是无关

响应时间是用来判断业务有多快的，而 TPS 才是用来判断容量有多大的

响应时间的曲线

随着线程的增多，响应时间也在增加

到压到一定数量线程时，TPS 基本上达到上限。

响应时间随着线程数的增加而增加了，系统的瓶颈显而易见地出现了

线程递增

在线程递增的过程中，出现了抖动，可能什么原因？

1、资源的动态分配不合理，像后端线程池、内存、缓存等等

2、数据没有预热

线程递增的策略

1、场景中的线程递增一定是连续的，并且在递增的过程中也是有梯度的

2、场景中的线程递增一定要和 TPS 的递增有比例关系，而不是突然达到最上限

根据响应时间线程递增的幅度

0-50ms 	1

50-100ms	1-3

100-200ms	3-5

200-500ms	5-10

性能衰减的过程

只要每线程每秒的 TPS 开始变少，就意味着性能瓶颈已经出现了。

但是瓶颈出现之后，并不是说服务器的处理能力会下降，应该说 TPS 仍然会上升，在性能不断衰减的过程中，TPS 就会达到上限

响应时间的拆分

在性能分析中，响应时间的拆分通常是一个分析起点。因为在性能场景中，不管是什么原因，只要系统达到了瓶颈，再接着增加压力，肯定会导致响应时间的上升，直到超时为止

如果想知道每个系统消耗了多长时间，就需要链路监控工具来拆分时间了

构建分析决策树

从压力工具中，只需要知道 TPS、响应时间和错误率三条曲线，就可以明确判断瓶颈是否存在

再通过分段分层策略，结合监控平台、日志平台，或者其他的实时分析平台，知道架构中的哪个环节有问题，然后再根据更细化的架构图一一拆解下去

操作系统分析决策树

cpu ：top pidstat

内存：top pidstat

IO：iostat

网络：iftop

进程：pidstat top -Hp

线程：jstack

堆：jmap

场景的比对

​	当你觉得系统中哪个环节不行的时候， 又没能力分析它，你可以直接做该环节的增加。一方面是压测机器的增加，一方面是服务节点的增加

如何录制脚本

性能测试工具的脚本编写能力分为两类，一个是录制，另一个是手工编写

编写 JMeter 脚本

1.1 创建线程组

重要参数

Number of Threads(users)： JMeter 中的线程数，也可以称之为用户数。这个线程数是产生 TPS 的，而一个线程产生多少 TPS，取决于系统的响应时间有多快。用 TPS 这个概念来承载系统的负载能力，而不是用这里的线程数

Ramp-up Period(in seconds)：递增时间，以秒为单位。指的就是上面配置的线程数将在多长时间内会全部递增完

Loop Count ：指的是一个线程中脚本迭代的次数。这里你需要注意，这个值和后面的Scheduler 有一个判断关系

Delay Thread creation until needed： JMeter 所有的线程是一开始就创建完成的，只是递增的时候会按照上面的规则递增。如果选择了这个选项，则不会在一开始创建所有线程，只有在需要时才会创建

Duration：线程会在多少秒之后结束，可以说和loopcount是互斥的关系，如果设置了 Loop Count 为 100，而响应时间是 0.1 秒，那么Loop Count * iteration duration(这个就是响应时间) = 100 * 0.1 = 10秒，即便设置了 Scheduler 的 Duration 为 100 秒，线程仍然会以 10 秒为结束点

2、编写 HTTP 脚本时，要注意以下几点：
1、要知道请求的类型，我们选择的类型和后端接口的实现类型要是一致的。
2、业务的成功要有明确的业务判断（在下面的 TCP 中，我们再加断言来判断）。

3、判断问题时，请求的逻辑路径要清晰

3、断言

断言指的就是服务器端有一个业务成功的标识，会传递给客户端，客户端判断是否正常接收到了这个标识的过程

在做脚本时，断言是必须要加的

## 关联和断言

### 关联

现在做性能测试的，有很多都是单纯的接口级测试，这样一来，关联就用得更少了。因为接口级的测试是一发一收就结束了，不需要将数据保存下来再发送出去

什么样的数据需要关联呢

1、数据是由服务器端生成的
2、数据在每一次请求时都是动态变化的
3、数据在后续的请求中需要再发送出去。

比如常见的 Session ID 就是一个典型的需要关联的数据

工作中常用的添加json提取器

$.提取的值 、下一个请求中的变量名



断言

断言就是判断服务端的返回是不是正确的

断言是根据需要来设计的，而设计断言的前提就是完全理解这个逻辑

对于 HTTP 协议来说，我们在性能分析中，主要关心的部分就是传输字节的大小、超时的设置以及压缩等内容。在编写脚本的时候，要注意 HTTP 头部，至于 Body 的内容，只要能让业务跑起来即可

## 在JMeter中如何设置参数化数据

以 JMeter 的 CSV Data Set Config 为例

参数释义

“Allow quoted data?”：False 和 True。它的含义为是否允许带引号的数据

Recycle on EOF? ：False、True 和 Edit。False 是指在没有参数的时候不循环使用；True 是指在没有参数的时候循环使用。Edit 是指在没有参数的时候会根据定义的内容来调用函数或变量

Stop thread on EOF?：False、True 和 Edit。含义和上面一致

Sharing mode : All threads、Current thread group、Currentthread、Edit

参数是在所有线程中生效，在当前线程组生效，还是在当前线程中生效。选择了 Edit 之后，会出现一个输入框，就是说这里并不是给引用函数和参数使用的，而是要自己明确如何执行 Sharing mode

## 做参数化前,要考虑什么

### 1、参数化数据应该用多少数据量？

根据业务场景计算参数化数据量

场景一	单线程内可循环使用的数据 比如用户填入登录名和密码登录

做脚本时考虑的是，有多少线程（Thread）就配置多少用户，让每个线程在同一个用户上多次循环执行

在这样的场景中，有多少线程就需要准备多少用户数据，即线程数=用户数

场景二  不可循环使用的数据，比如说电商系统，用同一个用户账号不停循环购买商品，就不符合业务场景

做脚本时，在压力测试工具中模拟出来的线程的每一次迭代来代表一个用户

不可循环使用的数据，在这样的场景中，就需要考虑场景的 TPS 和持续时间了

用户数据的计算方法 tps*持续时间（秒）

场景三	在一个线程之中，可以循环使用固定条目的数据，比如电商网站查看商品列表

这样的场景没有固定的条数限制，只能根据实际的业务判断
所以在配置参数之前，我们需要先判断这个参数是什么类型的数据

### 2、参数化数据从哪里来？

第一类
用户输入的数据在后台数据库中已存在，比如我们上面示例中的用户数据。
这类数据必须查询数据库之后再参数化到工具中
1、存在后台数据库中
2、需要用户主动输入
3、用户输入的数据会和后台数据库中的数据做比对
第二类
用户输入的数据在后台数据库中不存在。在业务流中，这些数据会 Insert 或 Update 到数据库中
1、数据库中原本不存在这些数据；
2、在脚本执行成功后会将这些数据 insert 或 update 到数据库中；
3、每个用户输入的数据可能相同，也可能不同，这取决于业务特点。

这类数据必须通过压力工具做参数化，要满足生产环境中数据的分布,也要满足性能场景中数据量的要求

### 3、参数多与少的选择对系统压力有什么影响？

参数取得过多，对系统的压力就会大；参数取得过少，不符合真实场景中的数据量，则无法测试出系统真实的压力

### 4、参数化数据在数据库中的直方图是否均衡？

对于参数化数据来说，如果数据取自于数据图，我们通常要检查一下数据库中的数据直方图。 

对于直接从生产上拿的数据来说，数据的分布更为精准。

但是对于一些在测试环境中造的数据，则一定要在造数据之后，检查下数据分布是否与生产一致

首先分析业务逻辑，某一类型的业务数据量是否在合理范围内

然后我们过滤掉不合理的数据即可

## 性能测试场景：如何进行场景设计

基准性能场景

什么是基准场景？基准场景就是对单接口或者单业务的测试

基准性能场景是为了测试出单业务的最大容量，以便在混合容量场景中判断哪个业务对整体容量最有影响

首先，我们要列出自己要测试的业务比例、业务目标 TPS 和响应时间指标

一定要知道，接口的TPS再高，都无法说明容量场景的情况，除非这个服务只有这一个接口

不要用所谓的”最大 TPS 拐点“这样的描述来说明 TPS 曲线，性能的衰减是逐步的，在最大 TPS 出现之前，就已经可以判断瓶颈是否出现了

容量性能场景

如果在你的项目中，有特定的业务日，那就要根据业务日的业务比例，重新做一个针对性的场景

关注业务的容量测试TPS和最大TPS，如果两者接近，将来业务扩展了，这两个业务将会先受到影响

## 性能测试场景：如何理解业务模型

回放的逻辑，就是可以在每一个业务产品和基础架构的层面做接口的回放，甚至我们可以直接在数据库中回放 SQL。这些手段，都是为了模拟生产的业务模型

生产数据统计
首先我们从生产环境取出数据，粒度到秒级，取出所有业务的交易量数据

业务量级按天统计的生成图

从这样的数据中取出业务量最高的一天，再以小时为单位统计出业务量比例

可以看出哪个小时的业务量最大

如果需要更细的数据，我们可以以分钟为单位看一下这个小时内的业务量分布

再以小时为单位做出百分比图

1、通用业务场景模型。就是将这一天的所有业务数加在一起，再将各业务整天的交易量加在一起，计算各业务量的比例。
3、哪个小时的业务量比较大，建立此小时的业务模型。将此消失的业务比例直接拿出来用。

收集线上日志，通过Elk分析业务比例

## 如何进行监控设计

### 监控设计步骤

首先，你要分析系统的架构。在知道架构中使用的组件之后，再针对每个组件进行监控

其次，监控要有层次，要有步骤。应该是先全局，后定向定量分析

最后，通过分析全局、定向、分层的监控数据做分析，再根据分析的结果决定下一步要收集什么信息，然后找到完整的证据链

架构图

做性能监控之前，先画一个最简单的架构图，看一下架构中各有什么组件，各有什么服务，将这些列下来，再找对应的监控手段和方式，看哪种手段和方式在性能测试过程中成本最低，效率最高

监控设计

1、我们要对整个架构做分层
2、在每一个层级上列出要监控的计数器
3、寻找相应的监控工具，实现对这些计数器的监控。如果一个工具做不到，在定位过程中考虑补充工具
4、要做到对每层都不遗漏

在企业中，我们也是首先考虑快速的监控实现。但是，还要一点要考虑，就是监控的持久有效性，能一直用下去。所以，在快速实现了之后，在必要时，会做一些二次开发，定制监控

什么是全局监控呢

OS 查看的第一层

DB 层（MySQL）

1、连接报表

2、临时表报表

3、线程表

4、Innodb缓存池报表

5、Innodb锁报表

6、Innodb数据、页、行报表

7、基本信息

8、索引报表

9、操作报表

10、查询和排序报表

11、查询缓存报表

12、表锁报表

13、表信息报表

定向监控

OS 层之定向监控细化 1

DB 层之定向监控

1、连接报表

SELECT COUNT(*) FROM information_schema.processlist

2、临时表报表

SELECT * FROM INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO

3、线程表

select * from performance_schema.threads;

4、Innodb缓存池报表

select * from INFORMATION_SCHEMA.INNODB_BUFFER_POOL_STATS\G;

5、Innodb锁报表

select * from INFORMATION_SCHEMA.INNODB_LOCKS;

6、Innodb数据、页、行报表

7、基本信息

8、索引报表

9、操作报表

10、查询和排序报表

11、查询缓存报表

12、表锁报表

13、表信息报表

监控工具

1、性能分析中，TPS 和响应时间的曲线是要有明显的合逻辑的趋势的。如果不是，则要降线程， 增加 Ramp-up 来让 TPS 趋于平稳。
2、我们要对曲线的趋势敏感，响应时间的增加不可以过于陡峭，TPS 的增幅在一开始要和线程数对应。
3、当 TPS 和响应时间曲线抖动过于强烈，要想办法让曲线平稳下来，进而分析根本原因，才能给出线上的建议配置



# 全链路压测

重要的原则

线上系统是不允许有脏数据的
压测流量落影子库，正常流量落正常库
压测不能把系统压崩溃
压测是模拟流量最高峰时用户的操作行为，所以综合性的流量模型需要跟实际情况相符

# 其他资料

[微信红包系统是如何应对高并发的](http://www.52im.net/thread-2548-1-1.html)

[Feed流系统设计-总纲](https://zhuanlan.zhihu.com/p/72882547)

[现代IM系统中的消息系统架构 - 架构篇](https://zhuanlan.zhihu.com/p/65119683)

[system-design](https://www.hiredintech.com/system-design)

[程序员转型架构师，推荐你读这几本书](https://developer.aliyun.com/article/721088?spm=a2c6h.14164896.0.0.1fe999bdnE8TZn)

[架构师之路](https://z.itpub.net/stack/detail/10131)

[架构师之路17年精选80篇]https://mp.weixin.qq.com/s/CIPosICgva9haqstMDIHag

GIT

git init ：初始化本地库



工作区  暂存区 本地库

git add 文件

git commit -m "注释" 文件



日志展示

git log

git log --pretty=oneline

git log --oneline

git reflog

reset命令：前进或后退历史版本

git reset --hard 索引（log命令中显示的索引），本地库指针移动的同时，重置工作区、重置暂存区

git diff 将工作区中的文件和暂存区中的文件进行比对 

git status

git branch -v 查看分支

git branch  新分支名称 ：创建分支

git checkout 分支名称：切换分支

git merge  分支名称：合并分支

解决冲突

1、避免不同的人对一个文件进行修改

2、在修改之前，先执行pull，再push



高并发系统设计的三种通用方法：Scale-out（横向扩展）、缓存和异步

分层有什么好处

分层的设计可以简化系统设计，让不同的人专注做某一层次的事情。

再有，分层之后可以做到很高的复用。

最后一点，分层架构可以让我们更容易做横向扩展

分层架构的不足	

增加了代码的复杂度

如果我们把每个层次独立部署，层次间通过网络来交互，那么多层的架构在性能上会有损耗

如何提升系统性能

性能优化原则

首先，性能优化一定不能盲目，一定是问题导向的。

其次，性能优化也遵循“八二原则”

再次，性能优化也要有数据支撑

最后，性能优化的过程是持续的

性能的度量指标

平均值、最大值、分位值

一般我们度量性能时都会同时兼顾吞吐量和响应时间，比如我们设立性能优化的目标时通常会这样表述：在每秒 1 万次的请求量下，响应时间 99 分位值在 10ms 以下

高并发下的性能优化

1、提高系统的处理核心数

2、减少单次任务响应时间

CPU 密集型系统中，需要处理大量的 CPU 运算，那么选用更高效的算法或者减少运算次数就是这类系统重要的优化手段

IO 密集型系统指的是系统的大部分操作是在等待 IO 完成，这里 IO 指的是磁盘 IO 和网络IO。我们熟知的系统大部分都属于 IO 密集型，比如数据库系统、缓存系统、Web 系统

系统怎样做到高可用

高可用系统设计的思路

1、系统设计

预先考虑如何自动化地发现故障，发生故障之后要如何解决

还需要掌握一些具体的优化方法，比如failover（故障转移）、超时控制以及降级和限流

发生 failover 的节点可能有两种情况：

是在完全对等的节点之间做 failover。

是在不对等的节点之间，即系统中存在主节点也存在备节点，Paxos，Raft

除了故障转移以外，对于系统间调用超时的控制也是高可用系统设计的一个重要考虑方面

降级是为了保证核心服务的稳定而牺牲非核心服务的做法

限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统



限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统

系统运维

可以从灰度发布、故障演练两个方面来考虑如何提升系统的可用性。

为了提升系统的可用性，重视变更管理尤为重要

除了提供必要回滚方案，以便在出现问题时快速回滚恢复之外，另一个主要的手段就是灰度发布

灰度发布指的是系统的变更不是一次性地推到线上的，而是按照一定比例逐步推进的

故障演练指的是对系统进行一些破坏性的手段，观察在出现局部故障时，整体的系统表现是怎样的，从而发现系统中存在的，潜在的可用性问题



如何让系统易于扩展

拆分是提升系统扩展性最重要的一个思路，它会把庞杂的系统拆分成独立的，有单一职责的模块。相对于大系统来说，考虑一个一个小模块的扩展性当然会简单一些。将复杂的问题简单化

存储层的扩展性

存储拆分首先考虑的维度是业务维度

再次拆分是按照数据特征做水平的拆分

我们不能随意地增加节点，因为一旦增加节点就需要手动地迁移数据，成本还是很高的。所以基于长远的考虑，我们最好一次性增加足够的节点以避免频繁地扩容。

业务层的扩展性

一般会从三个维度考虑业务层的拆分方案，它们分别是：业务纬度，重要性纬度和请求来源纬度

按业务维度

把相同业务的服务拆分成单独的业务池，每个业务依赖独自的数据库资源，不会依赖其它业务的数据库资源。

当某一个业务的接口成为瓶颈时，我们只需要扩展业务的池子，以及确认上下游的依赖方就可以了，这样就大大减少了扩容的复杂度。

按业务的重要性

根据业务接口的重要程度，把业务分为核心池和非核心池。

我们可以优先保证核心池的性能，当整体流量上升时优先扩容核心池，降级部分非核心池的接口，从而保证整体系统的稳定性。

按请求的来源

根据接入客户端类型的不同做业务池的拆分

比如说，服务于客户端接口的业务可以定义为外网池，服务于小程序或者 HTML5 页面的业务可以定义为 H5 池，服务于内部其它部门的业务可以定义为内网池





如何保证质量

1、代码开发

单元测试、代码review、静态代码扫描

2、测试保障

功能测试、性能测试、高可用测试、稳定性测试

3、线上质量

灰度发布、紧急回滚、故障演练、线上监控和巡检

质量评价标准：

可维护性、

可读性：要看代码是否符合编码规范、命名是否达意、注释是否详尽、函数是否长短合适、模
块划分是否清晰、是否符合高内聚低耦合等等；code review 是一个很好的测验代码可读性的手段

可扩展性：表示我们的代码应对未来需求变化的能力；代码的可扩展性表示，我们在不修改或少量修改原有代码的情况下，通过扩展的方式添加新的功能代码

 简洁性：尽量保持代码简单。代码简单、逻辑清晰，也就意味着易读、易维护

可复用性：尽量减少重复代码的编写，复用已有的代码

可测试性：代码可测试性的好坏，能从侧面上非常准确地反应代码质量的好坏。代码的可
测试性差，比较难写单元测试，那基本上就能说明代码设计得有问题



如何才能写出高质量的代码？

向对象设计思想、设计原则、设计模式、编码规范、重构技巧

面向对象中的继承、多态能让我们写出可复用的代码

编码规范能让我们写出可读性好的代码

设计原则中的单一职责、DRY、基于接口而非实现、里式替换原则等，可以让
我们写出可复用、灵活、可读性好、易扩展、易维护的代码



设计模式可以让我们写出易扩展的代码

持续重构可以时刻保持代码的可维护性等等

