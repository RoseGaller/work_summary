# **性能问题归根结底是某个资源不够**

# 如何获取性能数据

## 获取系统性能数据

nmon

按 C 键可加入 CPU 面板；按 M 键可加入内存面板；按 N 键可加入网络；按 D 键可加入磁盘

数据采集

nmon -f -s 5 -c 12 -m /home/qgc/Desktop/
-f 参数:生成文件
-T 参数:显示资源占有率较高的进程
-s 参数:-s 10表示每隔10秒采集一次数据
-c 参数:-s 10表示总共采集十次数据
 -m 参数:指定文件保存目录

## 获取JVM性能数据

jvisualvm

-Dcom.sun.management.jmxremote.port=9005
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

CPU分析

统计方法的执行次数和执行耗时，用于分析哪个方法执行时间过长

内存分析

以通过内存监视和内存快照等方式进行分析，进而检测内存泄漏问题，优化内存使用情况

线程分析

查看线程的状态变化，以及一些死锁情况

JMC

jcmd PID JFR.start
cmd  PID JFR.dump  filename=recording.jfr
jcmd PID JFR.stop

## 获取单个请求的调用链耗时

arthas中的trace命令

## 获取web接口的性能数据

ab

wrk

# 怎样写代码能够让CPU执行得更快？

CPU 缓存分为数据缓存与指令缓存，对于数据缓存，我们应在循环体中尽量操作同一块内存上的数据，由于缓存是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时也有性能提升

对于指令缓存，有规律的条件分支能够让 CPU 的分支预测发挥作用，进一步提升执行效率

对于多核系统，如果进程的缓存命中率非常高，则可以考虑绑定 CPU 来提升缓存命中率

多线程并行访问不同的变量，这些变量在内存布局是相邻的（比如类中的多个变量），此时 CPU 缓存就会失效，通过填充缓存行来解决

# 如何提升内存分配的效率？

不同的 C 库内存池，都有它们最适合的应用场景，例如 TCMalloc 对多线程下的小内存分配特别友好，而 Ptmalloc2 则对各类尺寸的内存申请都有稳定的表现，更加通用

内存池管理着堆内存，它的分配速度比不上在栈中分配内存。只是栈中分配的内存受到生命周期和容量大小的限制，应用场景更为有限。然存，它比内存池中的堆内存分配速度快很多！如果有可能的话，尽量在栈中分配分配

# 如何根据业务场景选择合适的锁？

互斥锁能够满足各类功能性要求，特别是被锁住的代码执行时间不可控时，它通过内核执行线程切换及时释放了资源，但它的性能消耗最大

如果能够确定被锁住的代码取到锁后很快就能释放，应该使用更高效的自旋锁，它特别适合基于异步编程实现的高并发服务

如果能区分出读写操作，读写锁就是第一选择，它允许多个读线程同时持有读锁，提高了并发性

当并发访问共享资源，冲突概率非常低的时候，可以选择无锁编程

不管使用哪种锁，锁范围内的代码都应尽量的少，执行速度要快。在此之上，选择更合适的锁能够大幅提升高并发服务的性能！

# 导致性能慢的原因

## 重试

控制重试的次数

控制重试的间隔

控制超时时间

识别重试的成功率，减少不必要的重试，通过熔断、快速失败等实现

## 公共资源争抢

定时任务、监控软件征用资源

降低资源争用

错峰，错开定时任务运行的时间

避免资源的共享

## 网络抖动

网络延时的最大值与最小值的差值

工具

traceroute -n -T www.xxx.com

ping www.xxx.com

MTR

尽量减少或避免太远的访问，多机房部署时，尽量同机房调用

## 缓存失效

进程内或进程外缓存失效，穿透到数据库，增加网络延迟

## 网络的延迟

传输延时

处理延时

网络传输慢

网络抖动

数据大

应用程序处理慢

# 常用命令

周期性的对I/O信息、内存信息、cpu使用情况采样
sar -u 1 3 cpu
sar -r 1 3 内存
sar -b 1 3 IO
sar -n 1 3 网络
sar -d 1 3 磁盘

ll /proc/${PID}/fd | wc -l

ll /proc/${PID}/task | wc -l （效果等同pstree -p | wc -l）

就能知道进程打开的句柄数和线程数。

sysctl配置的内核参数

dmesg系统级别异常

lsof查看打开文件

# 性能优化切入点

## 复用优化

重复的代码可以提取成公用的方法

缓冲

常见于对数据的暂存，然后批量传输或者写入，用来缓解不同设备之间频繁地、缓慢地随机写，主要针对的是写操作

缓存

进程内缓存（堆内缓存）

Ehcache

Caffeine

Guava Cache

JCache

进程外缓存

主要针对的是读操作

多级缓存

计算结果缓存

数据缓存

池化

对对象进行保存，用时从池中获取，用完再放回池子。降低对象的创建、销毁的开销

线程池

数据库连接池

对象池

## 结果集优化

Protobuf

可读性低，体积小，共并发场景可提高效率

json

可读性高，体积大

开启压缩，降低传输内容的大小

批量处理

## 高效的实现

高性能的网络通信框架Netty

## 算法优化

高效的算法实现

空间换时间，加快处理速度

## 计算优化

并行执行

多机执行

多进程执行

多线程

多协程执行

变同步为异步

多步骤执行彼此无依赖，多线程执行

非核心步骤稍后执行，消息队列异步执行

懒性加载

分屏加载

延迟加载

## 资源冲突优化

锁的优化

锁消除

JIT编译器，可以消除某些对象的加锁操作

无锁

减小锁的粒度

减少锁的持有时间

锁分级

synchronize的锁升级，偏向锁、轻量级锁、重量级锁

锁分离

读写锁

乐观锁

悲观锁

自旋锁

在用户态代码中完成加锁与解锁操作。被锁住的代码执行时间很短，
可以用自旋锁取代互斥锁。

互斥锁

由操作系统内核实现，增加上下文的切换开销

java加锁的方式

​	synchronized

​	普通方法加锁时锁的对象是this

​	静态方法加锁时锁的对象是Class对象

## JVM优化

G1垃圾收集器

**确认是不是内存本身就分配过小** jmap -heap 10765

**找到最耗内存的对象**jmap -histo:live 10765 | more

## 数据库优化

## 操作系统优化

## 架构优化

## Java代码优化法则

使用局部变量代替堆上分配

减少变量的作用范围

访问静态变量直接使用类名

字符串拼接使用StringBuilder

集合初始化时指定初始大小

遍历Map使用EntrySet，使用KeySet时获取的Key的集合，需要执行一次get操作

自增推荐使用LongAddr

不用异常控制流程

不在循环中使用try catch

合理使用PreparedStatement，使用预编译队对SQL的执行会提速，防止SQL注入

日志打印的注意事项

减少事务作用范围

使用位运算代替乘除运算



# 性能定律

帕累托法则

也被称为 80/20 法则、关键少数法则，或者八二法则

大约 80% 的用户使用集中在 20% 的程序功能

80% 的开发时间往往用在最精髓的 20% 代码上

80% 的代码演进和改动发生在大约 20% 的代码上

差不多有 80% 的错误发生在大约 20% 的代码上

 80％的流量将在总时间段的特定 20％内发生

程序的 80% 的时间是在运行大约 20% 的代码。

阿姆达尔定律

用于衡量处理器进行并行处理时总体性能的提升度

优先加速占用时间最多的模块，因为这样可以最大限度地提升加速比

# 操作系统性能瓶颈

## CPU

### 多处理器和 NUMA

现在的 CPU 普遍采用多处理器（Socket）来提高 CPU 性能，每个处理器都有自己可以直接访问的本地内存（Local Memory）

当 CPU 处理器访问本地内存时，会有较短的响应时间（称为本地访问 Local Access）。而如果需要访问外地 / 远程内存时候，就需要通过互联通道访问，响应时间就相比本地内存变慢了（称为远端访问 Remote Access）。所以 NUMA（Non-Uniform MemoryAccess）就此得名

对每一个进程和线程而言，当它运行在某一个处理器上时，它所对应的内存使用默认的分配方案是——优先尝试在请求线程当前所处的处理器的本地内存上分配。如果本地内存不足，才会分配到外地 / 远程内存上去

### 多核结构和多级缓存

处理器内部一般都是多核（Core）架构。CPU 的缓存通常分成了三个级别：L1、L2 和 L3

级别越小就越接近 CPU，速度更快，同时容量也越小。L1 和 L2 一般在核的内部

L3 缓存是三级缓存中最大的一级，同时也是最慢的一级,在同一个处理器内部的核会共享同一个 L3 缓存

### 超线程（Hyperthreading，HT）

一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程

当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器

具有 HT 超线程技术的处理器进行线程切换的成本很低

### CPU 的性能监测工具

top

uptime

查看过去的 CPU 负载情况

 mpstat 和 pidstat

查看每个核还有每个进程的情况

vmstat 

显示虚拟内存、内核线程、磁盘、系统进程、I/O 模块、中断等信息。

perf

性能剖析工具

### 常见的性能问题的表现

中断

各个核上的中断不平衡，导致有的核会超载而导致系统响应缓慢，但是其他的核反而空闲

超载

多核之间的平衡没做好

CPU做了无用功

应用程序本身的设计需要优化

空闲

太多的分支预测错误

太多内存数据操作

### CPU性能指标

CPU负载情况

CPU使用率

系统CPU

用户CPU

IO等待CPU

注意：使用率和负载的关系往往不是线性的

中断

软中断

硬中断

上下文切换

平均负载

理想情况下，平均负载等于逻辑 CPU 个数，这表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了

CPU 缓存的命中率

### CPU超标量

超标量（superscalar）CPU架构是能够在相同的CPU主频下实现更高的CPU吞吐率（throughput）

超标量处理器运用了指令级并行运算，将一条指令分解为多个步骤（周期），并且每一个周期时间相同，允许多条指令同时存在，提升了指令并行性，进而提升性能

### 工具集

CPU 上下文切换

vmstat来查询系统总体的上下文切换情况

pidstat 给它加上 -w 选项，可以查看每个进程上下文切换的情况

性能调优利器--火焰图

mpstat

用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标

 pidstat -u 5 1

用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。

查看哪个函数导致了CPU使用率升高:perf top -g -p PID(-g 开启调用关系分析)

pidstat -p PID显示了指定进程的cpu使用率

execsnoop：监控短时进程；包括进程 PID、父进程 PID、命令行参数

perf

以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题

perf record(提供了保存数据的功能) 和 perf report(对保存后的数据解析展示)

perf top，实时显示占用 CPU 时钟最多的函数或者指令，可以用来查找热点函数，

 perf top -g -p PID，查看进程调用关系

 ps aux | grep PID 根据PID从所有进程中查找

pstree：以树形结构显示程序和进程之间的关系

短时进程导致的CPU过高

execsnoop：查看进程的父进程 PID 以及它的命令行参数

软中断导致CPU使用率升高

查看 /proc/softirqs文件中各种类型软中断的变化情况，确定到底是哪种软中断出的问题（watch -d cat /proc/softirqs）（NET_RX（网络接收））

发现是网络接收中断导致的问题，那就可以继续用网络分析工具 sar 和 tcpdump 来分析（sar -n DEV 1：查看系统的网络收发情况；tcpdump -i eth0 -n tcp port 80：抓取eth0上的包， -i eth0 选项指定网卡eth0，并通过 tcp port 80 选项指定TCP协议的 80 端口）

THP(透明大页)

一个2M的连续物理内存，用一个页表项来映射更大的内存，这样可以减少Page Fault，因为需要的页数少了，这也会提升 TLB 命中率

grep -i HugePages /proc/meminfo(查看系统分配的大页)

echo never > /sys/kernel/mm/transparent_hugepage/enabled（禁用大页）

是否启用大页需要考虑业务数据的局部性

中断

软中断是用来处理硬中断在短时间内无法完成的任务的

硬中断执行时间短，如果发生频率不高，一般不会给业务带来明显影响

追踪瞬时系统快照

sysctl -w kernel.sysrq = 1（启用sysrq所有功能）

echo t > /proc/sysrq-trigger（当前的任务快照保存下来）

dmesg（查看快照信息）

如何让CPU执行更快

将进程或者线程和CPU进行绑定，提升多核CPU下的缓存命中率

提升指令缓存的命中率

CPU含有分支预测器，如果分支预测器可以预测接下来要在哪段代码执行（比如 if 还是 else 中的指令）
，就可以提前把这些指令放在缓存中，CPU 执行时就会很快。

提升数据缓存的命中率

遍历访问数组时，按照内存布局顺序访问将会带来很大的性能提升

填充缓存行，默认64字节，消除伪共享

## 磁盘IO

### 文件系统

Linux 文件系统为每个文件都分配两个数据结构，索引节点（indexnode）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。

索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等，索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。

目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存

磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成

磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。超级块，存储整个文件系统的状态；索引节点区，用来存储索引节点；数据块区，则用来存储文件数据

### 虚拟文件系统

为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS

VFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节

文件系统三种类型

第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS

第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统

第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。

### 文件系统 I/O

缓冲 I/O 与非缓冲 I/O

缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件

非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存

直接 I/O 与非直接 I/O

直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件

非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘

阻塞 I/O 和非阻塞 I/O

阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务

非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果

同步和异步 I/O

同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得I/O 响应

异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序

### Linux 磁盘 I/O 的工作原理

#### 磁盘类型

按存储介质分

1、机械磁盘

机械磁盘主要由盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据

如果 I/O 请求刚好连续，那就不需要磁道寻址，自然可以获得最佳性能

机械磁盘的最小读写单位是扇区，一般大小为 512 字节

2、固态磁盘

通常缩写为 SSD，由固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多

固态磁盘的最小读写单位是页，通常大小是 4KB、8KB 等

存在“先擦除再写入”、写放大的限制

按接口分

比如可以把硬盘分为 IDE（Integrated Drive Electronics）、SCSI（Small Computer SystemInterface） 、SAS（Serial Attached SCSI） 、SATA（Serial ATA） 、FC（FibreChannel） 等

不同的接口，往往分配不同的设备名称。如果是多块同类型的磁盘，就会按照a、b、c 等的字母顺序来编号

#### 块设备

在 Linux 中，磁盘实际上是作为一个块设备来管理的，也就是以块为单位读写数据，并且支持随机读写

每个块设备都会被赋予两个设备号，分别是主、次设备号

主设备号用在驱动程序中，用来区分设备类型

次设备号则是用来给多个同类设备编号

#### 通用块层

与虚拟文件系统 VFS 类似，为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备

通用块层除了为文件系统和应用程序，提供访问块设备的标准接口外，还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率

四种调度算法

第一种 NONE ，更确切来说，并不能算 I/O 调度算法。因为它完全不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理

第二种 NOOP ，是最简单的一种 I/O 调度算法。它实际上是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘

第三种 CFQ（Completely Fair Scheduler），也被称为完全公平调度器，是现在很多发行版的默认 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求

最后一种 DeadLine 调度算法，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理，DeadLine 调度算法，多用在 I/O 压力比较重的场景，比如数据库等

#### IO栈

##### 1、文件系统层

包括虚拟文件系统和其他各种文件系统的具体实现

为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据

##### 2、通用块层

包括块设备 I/O 队列和 I/O 调度器

对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层

##### 3、设备层

包括存储设备和相应的驱动程序

负责最终物理设备的 I/O 操作

Linux优化IO的方式

为了优化文件访问的性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，以减少对下层块设备的直接调用

为了优化块设备的访问效率，会使用缓冲区，来缓存块设备的数据

### IO性能指标

#### 磁盘性能指标

使用率

指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈

饱和度

指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求

IOPS（Input/Output Per Second）

指每秒的 I/O 请求数

在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能

吞吐量

指每秒的 I/O 请求大小

在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能

响应时间

指 I/O 请求从发出到收到响应的间隔时间

#### 文件系统

存储空间的使用情况，包括容量、使用量以及剩余空间等

索引节点的使用情况，它也包括容量、使用量以及剩余量等

缓存

页缓存

目录项缓存

索引节点缓存

各个具体文件系统（如 ext4、XFS 等）的缓存

### 如何对磁盘进行基准测试

推荐用性能测试工具 fio ，来测试磁盘的 IOPS、吞吐量以及响应时间等核心指标

在基准测试时，一定要注意根据应用程序 I/O的特点，来具体评估指标

测试出，不同 I/O 大小（一般是 512B 至 1MB 中间的若干值）分别在随机读、顺序读、随机写、顺序写等各种场景下的性能情况

### 磁盘 I/O 观测

观测每块磁盘的使用情况

iostat 是最常用的磁盘 I/O 性能观测工具

提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats

进程 I/O 观测

pidstat，加上 -d 参数，就可以看到进程的 I/O 情况

iotop，是类似于 top 的工具，可以按照 I/O大小对进程排序，然后找到 I/O 较大的那些进程

#### 查看思路

可以先用 top ，来观察 CPU 和内存的使用情况，主要是sys、 iowait、Buffer/Cache  ；查看buffer/cache占用，建议使用pcstat或者hcache

然后再用 iostat ，来观察磁盘的 I/O 情况

使用 pidstat 加上 -d 参数，查看每个进程的 I/O 情况

通过strace -p pid查看进程的系统调用write情况

怎样知道哪里在写文件

借助bcc 软件包的下filetop工具

主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称

ps -efT | grep 线程Id	：根据线程Id找到对应的进程

opensnoop工具，可以动态跟踪内核中的open 系统调用，可以得到写入文件的具体路径

strace默认是不开启线程追踪的，而写文件是由子线程执行的，所以看不到write调用

可以通过pstree查看进程的线程信息，再用strace跟踪。或者，通过strace -fp pid 跟踪所有线程

通过lsof -p  pid查看进程打开了哪些文件

### 应用程序优化

追加写代替随机写，减少寻址开销

充分利用系统缓存，降低IO次数

可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统

频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数

在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，减少系统调用次数

cgroups来限制进程 / 进程组的 IOPS 以及吞吐量，避免I/O被某个应用完全占用

零拷贝

降低上下文切换开销和内存的拷贝次数

磁盘文件拷贝到pagecache，pagecache拷贝到socket缓冲区，socket缓冲区拷贝到网卡
如果网卡支持SG-DMA技术，可以省略socket缓冲区的拷贝，直接从cache拷贝到网卡

### 文件系统优化

第一，你可以根据实际负载场景的不同，选择最适合的文件系统

比如 Ubuntu 默认使用ext4 文件系统，而 CentOS 7 默认使用 xfs 文件系统

相比于 ext4 ，xfs 支持更大的磁盘分区和更大的文件数量

但是 xfs 文件系统的缺点在于无法收缩，而 ext4 则可以

第二，在选好文件系统后，还可以进一步优化文件系统的配置选项

包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等

第三，可以优化文件系统的缓存

比如，你可以优化 pdflush 脏页的刷新频率（比如设置 dirty_expire_centisecs 和
dirty_writeback_centisecs）以及脏页的限额（比如调整 dirty_background_ratio 和
dirty_ratio 等）

最后，在不需要持久化时，你还可以用内存文件系统 tmpfs，以获得更好的 I/O 性能

### 磁盘优化

最简单有效的优化方法，就是换用性能更好的磁盘，比如用 SSD 替代 HDD

根据磁盘和应用程序的模式选择合适的IO调度算法，SSD通常选用NOOP算法，数据库选用deadline

为应用程序的数据，设置单独的磁盘

在顺序读比较多的场景下，增大磁盘预读的数据

可以调整磁盘队列的长度/sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）

最后，要注意，磁盘本身出现硬件错误，也会导致 I/O 性能急剧下降

可以查看 dmesg 中是否有硬件 I/O 故障的日志

 还可以使用 badblocks、smartctl 等工具，检测磁盘的硬件问题

用 e2fsck 等来检测文件系统的错误

如果发现问题，你可以使用 fsck 等工具来修复

### 命令工具集合

df

文件系统的磁盘空间使用情况， -h 选项，可以获得更好的可读性

加上 -i 参数，查看索引节点的使用情况。除了文件数据，索引节点也占用磁盘空间。如果小文件过多，会出现索引节点空间不足的情况

free、vmstat

观察页缓存的大小

free 输出的 Cache，是页缓存和可回收 Slab 缓存的和，可以从/proc/meminfo ，直接得到它们的大小

内核使用 Slab 机制，管理目录项和索引节点的缓存，proc/meminfo 只给出了Slab 的整体大小，具体到每一种 Slab 缓存，还要查看 /proc/slabinfo 这个文件。

iostat

查看每个磁盘的IO使用率、队列长度、队列等待时间、处理时间

-d 选项是指显示出 I/O 的性能指标

-x 选项是指显示出扩展统计信息

pidstat

pidstat -d查看各进程的IO情况

filetop

主要跟踪内核中文件的读写情况，并输出线程ID、读写大小、读写类型以及文件名称

ps -efT | grep tid：根据线程id查找进程

opensnoop

动态跟踪内核中的open系统调用，找出文件的路径

iotop

按照 I/O大小对进程排序

strace

分析系统调用,-p指定进程号，查看进程正在读写的文件

-f 跟踪所有线程读写

-tt 在每行输出的前面，显示毫秒级别的时间

-T 显示每次系统调用所花费的时间

-e 控制要跟踪的事件和跟踪行为,比如指定要跟踪的系统调用名称

lsof

查看进程打开文件列表,-p指定进程号

## 网络

### 修改TCP缓冲区

发送缓冲区

缓冲区动态调节功能默认是打开的

net.ipv4.tcp_wmem = 4096        16384   4194304

接收缓冲区

net.ipv4.tcp_moderate_rcvbuf = 1；打开自动调节功能

接收缓冲区的调节是通过 tcp_mem 配置完成的net.ipv4.tcp_mem = 88560     118080  177120(当 TCP 内存小于第 1 个值时，不需要进行自动调节；在第 1 和第 2 个值之间时，内核开始调节接收缓冲区的大小；大于第 3 个值时，内核不再为 TCP 分配新内存，此时新连接是无法建立的)

关键点

保证缓冲区的动态调整上限达到带宽时延积（带宽与时延的乘积），而下限保持默认的 4K 不变即可；而对于内存紧张的服务而言，调低默认值是提高并发的有效手段

调大 tcp_mem 的上限可以让 TCP 连接使用更多的系统内存，这有利于提升并发能力（tcp_wmem 和 tcp_rmem 的单位是字节，而 tcp_mem 的单位是页面大小。而且，千万不要在 socket 上直接设置SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能）

### 三次握手

client发送syn（syn_send）

client收不到server的ack，会重发syn

net.ipv4.tcp_syn_retries = 2

server响应ack/syn（syn_recv）

收到syn，将其加入到半连接队列
net.ipv4.tcp_max_syn_backlog = 16384

查看SYN_RECV连接数
netstat -n -p | grep SYN_RECV | wc -l

积压的半连接个数超过阈值，新的 SYN 包就会被丢弃。防止SYN Flood 攻击
net.ipv4.tcp_syncookies = 1

serve收不到client返回ack，进行重试发送ack/syn
net.ipv4.tcp_synack_retries = 2

查看队列溢出导致 SYN 被丢弃的个数
netstat -s | grep "SYNs to LISTEN"

client响应ack

server收到ack，产生全连接，放入全连接队列，长度是由 listen(sockfd, backlog) 这个函数里的 backlog 控制的
net.core.somaxconn = 16384

当服务器中积压的全连接个数超过该值后，新的全连接就会被丢弃掉，发送reset来通知 Client不要再重试
设置net.ipv4.tcp_abort_on_overflow = 0，表明不再发送 reset ，给client重试的机会

查看全连接队列溢出的连接个数：netstat-s|grep"listenqueue"

### TFO（TCP fast open）

绕过三次握手，直接发送数据

启用fastopen：net.ipv4.tcp_fastopen=3

### 四次挥手

主动关闭方发送FIN，进入FIN_WAIT_1状态

对端收到FIN，响应ACK，进入CLOSE_WAIT状态

主动关闭方收到ACK，进入FIN_WAIT_2状态

迟迟收不到对端的 FIN 包，那就会一直处于这个状态，于是就会一直消耗系统资源

tcp_fin_timeout，默认为 60s，超过这个时间后就会自动销毁该连接

对端调用close方法，发送FIN，进入LAST_ACK状态

如果迟迟收不到ack，会重试发送FIN

FIN 报文的重发次数仍由 tcp_orphan_retries 参数控制。

主动方收到FIN，响应ACK，进入TIME_WAIT状态

TIME_WAIT 状态会持续一段时间，默认存活60秒

成功发送ACK给被动方

让之前的连接发送的报文在网络中失效

net.ipv4.tcp_max_tw_buckets = 10000，限制该状态的最大个数

net.ipv4.tcp_tw_reuse = 1，net.ipv4.tcp_timestamps=1重用占用的端口号

### tcp重传

tcpdump -s 0 -i eth0 -w tcpdumpfile（保存进出某个网卡的数据包）
 tshark -r tcpdumpfile -R tcp.analysis.retransmission（tshark工具过滤TCP重传包）

TCP tracepoint

cd /sys/kernel/debug/tracing/events/
echo 1 > tcp/tcp_retransmit_skb/enable (开启Tracepoint 来追踪 TCP重传)

 cat trace_pipe（查看TCP重传事件）

echo 0 > tcp/tcp_retransmit_skb/enable(关闭此功能)

### 慢启动优化

增大 init_cwnd （初始拥塞窗口）可以显著地提升网络性能

### 启用BBR拥塞算法

net.ipv4.tcp_congestion_control=bbr

### 超时重传优化

改进RTO 的初始值，这可以显著节省业务的阻塞时间

### 延迟确认

针对 TCP ACK 的一种优化机制，也就是说，不用每次请求都发送一个 ACK，而是先等一会儿（比如 40ms），如果这段时间内，正好有其他包需要发送，那就捎带着ACK一起发送过去。当然，如果一直等不到其他包，那就超时后单独发送 ACK

延迟发送数据（Nagle算法）

TCP 协议中用于减少小包发送数量、合并TCP小包的一种优化算法，目的是为了提高实际带宽的利用率

### DNS优化

对DNS解析的结果进行缓存（dnsmasq）

对DNS解析的结果进行预取

使用HTTPDNS代替常规的DNS解析

基于DNS的全局负载均衡

### 常用工具

dstat

CPU使用率、磁盘 I/O吞吐（读写）、 网络吞吐（收发包）、内存分页统计、系统中断、上下文切换

dstat -tcp

lis

处于监听状态的连接数

act

处于连接状态的连接数

syn

处于三次握手阶段的连接数

tim

time_wait状态的连接数

clo:close-wait状态的连接数

ss  -natp（看 TCP 连接的详细信息）

nstat -z | grep -i  drop(查看系统的网络状态)

tcpdump

TCP Tracepoints

ss -nipt

iperf、netperf、pktgen

sar检查网络状况，观察PPS每秒收发的报文数，还可以观察BPS每秒收发的字节数

Netperf

网络性能的测量工具，主要针对基于 TCP 或 UDP 的传输

Netstat

可以显示与 IP、TCP、UDP 和 ICMP 协议相关的统计数据，提供 TCP连接列表，TCP 和 UDP 监听，进程内存管理的相关报告，一般用于检验本机各端口的网络连接情况

Traceroute

以帮我们知道，数据包从我们的计算机到互联网远端的主机，是走的什么网络路径

### 网络的性能指标

第一个指标是可用性

ping 命令其实就是向远端的机器发送 ICMP 的请求数据包，并等待接收对方的回复。通过请求和应答返回的对比，来判断远端的机器是否连通

第二个指标是响应时间

第三个指标是网络带宽容量

第四个指标网络吞吐量

指在某个时刻，在网络中的两个节点之间，端到端的实际传输速度

第五个指标网络利用率

指网络被使用的时间占总时间的比例

## 内存

### PageCache

#### 时间局部性原理

刚被访问的数据在短时间内再次被访问的概率很高；

#### 空间局部性原理

刚被访问的数据的附近的数据将来也会被访问

#### 预读

磁盘读取不是按需读取，而是按页读取，

 read 方法只读取了 0-32KB 的字节，但内核会把其后的 32-64KB 也读取到 PageCache

#### 不适合的场景

大文件传输

大件中某一部分内容被再次访问到的概率其实非常低，大文件长期长期占用pagecache，热点小文件无法充分使用pagecache，甚至挤出pagecache

#### LRU

避免大量只读一次的文件涌入Active链表，将Active链表上的热数据冲刷出去

第一次读取文件后，文件内容都是inactive的，放到inactive链表。

第二次读取这些内容后，会把它放在active链表上

回收时优先把inactive list的部分page给回收掉，为了维持inactive/active的平衡，就需要把active 链表的部分page给demote到inactive 链表上

#### 查看脏页的积压、回写情况

cat /proc/vmstat | egrep "dirty|writeback"

#### 回收方式

直接回收

后台回收

#### 查看回收行为

sar -B 1

pgscank/s : kswapd(后台回收线程) 每秒扫描的 page 个数

pgscand/s: Application 在内存申请过程中每秒直接扫描的 page 个数

pgsteal/s: 扫描的 page 中每秒被回收的个数

%vmeff: pgsteal/(pgscank+pgscand), 回收率，越接近100说明系统越安全，越接近0说明系统内存压力越大

#### 观察应用的page cache

lsof可以查看某个应用打开的文件

fincore可以看这些文件有多少内容在pagecache。

#### 优化

避免直接内存回收，及早地触发后台回收来

​	vm.min_free_kbytes = 4194304，可以增大这个配置选项来及早地触发后台回

避免脏页积压过多

​	sar -r 来观察系统中的脏页个数

​	vm.dirty_background_bytes = 0
​	vm.dirty_background_ratio = 10
​	vm.dirty_bytes = 0
​	vm.dirty_expire_centisecs = 3000
​	vm.dirty_ratio = 20

避免热点PageCache被回收

​	mlock(2) 防止被回收以及被 drop

​	madvise(2) 告诉内核来立即释放这些 Page Cache

### 缓存和缓存命中率

缓存（也就是cache）是 CPU 与内存之间的临时数据交换器，是为了解决两种速度不匹配的矛盾而设计的

随着多核 CPU 的发展，CPU 缓存通常分成了三个级别：L1、L2、L3。一般而言，每个核上都有 L1 和 L2 缓存。L1 缓存其实分成两部分：一个用于存数据，也就是 L1d Cache，另外一个用于存指令，L1i Cache

要采用多级缓存，并逐级增加缓存大小，就是为了提高各级缓存的命中率，从而最大限度地降低直接访问内存的概率

### 缓存一致性

缓存一致性协议解决了缓存内容不一致的问题，但同时也造成了缓存性能的下降

为了达到数据访问的一致，就需要各个处理器和内核，在访问缓存和写回内存时遵循一些协议，这样的协议就叫缓存一致性协议。常见的缓存一致性协议有 MSI、MESI 等

### 内存带宽和延迟

内存对性能的制约包括三个方面

第一个方面就是内存的使用大小

采用高效的，使用内存少的算法和数据结构

第二个方面是内存访问延迟

各级缓存，都是为了降低内存的直接访问，从而间接地降低内存访问延迟的

尽量降低数据和程序的大小，那么各级缓存的命中率也会相应地提高

第三个方面就是内存带宽

单位时间内，可以并行读取或写入内存的数据量，通常以字节 / 秒为单位表示

### 内存的分配

应用程序向操作系统申请内存时，系统会分配内存，这中间总要花些时间，因为操作系统需要查看可用内存并分配

如果空闲内存不够，系统需要采取措施回收内存，这个过程可能会阻塞

应用申请内存块的大小不定且申请频繁操作时，会造成大量的内存碎片，这些内存碎片会导致系统性能下降

内存池，就是提前申请分配一定数量的、大小仔细考虑的内存块留作备用

### NUMA 的影响

NUMA 包含多个处理器（或者节点），它们之间通过高速互连网络连接而成。每个处理器都有自己的本地内存，但所有处理器可以访问全部内存，访问远端内存的延迟远远大于本地内存访问

部署应用程序时，最好将访问相同数据的多个线程放在相同的处理器上。根据情况，有时候也需要强制去绑定线程到某个节点或者 CPU 核上

### 工具

free

显示总的内存、使用的内存、空闲内存等

Linux下的 /proc 文件系统

根据进程的 ID 来查看每个进程的详细信息，包括分配到进程的内存使用

vmstat

对操作系统的虚拟内存、进程、CPU 等的整体情况进行监视

cachestat 

查看整个系统缓存的读写命中情况

cachetop

查看每个进程的缓存命中情况

pcstat 

查看文件在内存中的缓存大小以及缓存比例

如果 RES 太高而 SHR 不高，那可能是堆内存泄漏；如果 SHR 很高，那可能是 tmpfs/shm 之类的数据在持续增长，如果 VIRT 很高而 RES 很小，那可能是进程不停地在申请内存，但是却没有对这些内存进行任何的读写操作，即虚拟地址空间存在内存泄漏。	
通过 pmap 我们能够清楚地观察一个进程的整个的地址空间，包括它们分配的物理内存大小，这非常有助于我们对进程的内存使用概况做一个大致的判断。比如说，如果地址空间中[heap]太大，那有可能是堆内存产生了泄漏；再比如说，如果进程地址空间包含太多的 vma（可以把 maps 中的每一行理解为一个 vma），那很可能是应用程序调用了很多 mmap 而没有 munmap；	

pmap 同样也是解析的 /proc 里的文件，具体文件是 /proc/[pid]/maps 和 /proc/[pid]/smaps，其中 smaps 文件相比 maps 的内容更详细，可以理解为是对 maps 的一个扩展

### 伪共享

以缓存行为单位进行存储，哪怕你修改了缓存行中一个很小很小的数据，它都会将整个缓存行刷新

在 JDK8 以上的版本，通过开启参数 -XX:-RestrictContended，就可以使用注解 @sun.misc.Contended 进行补齐，来避免伪共享的问题

### HugePage

使用较少的映射表来管理大内存，而将页增大的技术

HugePage 也伴随着一些副作用，比如竞争加剧，在一些大内存的机器上，开启后在一定程度上会增加性能

### TLB

### 预先加载

 JVM 的 -XX:+AlwaysPreTouch 参数，JVM 会在启动的时候，就把所有的内存预先分配

# 优化方向

## 架构优化

调用方式

同异步调用

RPC框架选型

MQ选型

序列化方式

多路复用器

架构优化

动静分离

集群优化

数据隔离

服务拆分

异步驱动

负载均衡

程序优化

配置优化

并行优化

缓存优化

锁优化

## 网络优化

提升吞吐量

提升连接并发数

事件驱动

多路复用

降低连接成本

降低时延

TCP握手优化

重试次数

超时时间

复用策略

缓冲区队列

套接字缓冲区优化

滑动窗口

读写缓冲区

系统调整策略

网络专线

CDN优化

## 硬件优化

CPU

数据缓存优化

数据指令优化

内存

应用内存池

磁盘

磁盘缓存

零拷贝

IO优化

SSD固态硬盘

# 如何高效的传输文件

基于用户缓冲区传输文件时，过多的内存拷贝与上下文切换次数会降低性能。

零拷贝技术在内核中完成内存拷贝，天然降低了内存拷贝次数。它通过一次系统调用合并了磁盘读取与网络发送两个操作，降低了上下文切换次数

零拷贝技术基于 PageCache，而 PageCache 缓存了最近访问过的数据，提升了访问缓存数据的性能，同时，为了解决机械磁盘寻址慢的问题，它还协助 IO 调度算法实现了 IO 合并与预读

零拷贝有一个缺点，就是不允许进程对文件内容作一些加工再发送，比如数据压缩后再发送

高并发场景下，为了防止 PageCache 被大文件占满后不再对小文件产生作用，大文件不应使用 PageCache，进而也不应使用零拷贝技术处理

绕过 PageCache 的 IO 称为直接 IO。对于磁盘，异步 IO 只支持直接 IO

直接 IO 的应用场景并不多，主要有两种：第一，应用程序已经实现了磁盘文件的缓存，不需要 PageCache 再次缓存，引发额外的性能消耗。比如 MySQL 等数据库就使用直接IO；二，高并发下传输大文件

如果网卡支持SG-DMA技术，可以省去socket缓冲区的拷贝

# nginx优化

## 协议

改为HTTP/2协议

## 压缩

对于文本文件使用 Brotli 压缩算法

提前在磁盘中压缩好，然后通过 add_header 等指令在响应头部中告诉客户端该如何解压

## 提高内存使用率

Nginx 上多使用小于 256KB 的小内存，TCMalloc 的性能要远高于 Linux 默认的 PTMalloc2 内存池

## 限速

limit_conn 可以限制并发连接

limit_req 可以基于 leacky bucket 漏斗原理限速

## Nginx如何防止流量打穿缓存

开启Nginx 的合并回源功能：proxy_cache_lock on;

开启过期缓存功能：proxy_cache_use_staleupdating;

开启Nginx 的合并回源功能。Nginx 会将多个并发请求合并为1条回源请求，并锁住
所有的客户端请求，直到回源请求返回后，才会更新缓存，同时向所有客户端返回响应。

## 长、短连接

与客户端使用短连接

http模块：keepalive_timeout 0;

与下游 Web 服务使用长连接

proxy_http_version 1.1;

proxy_set_headerConnection ""; 清除header中的Connection，默认是close

## 反向代理的意义

承载海量的连接

保证内网的安全

负载均衡

缓存加速Web请求

隐藏目标服务器

## 减少磁盘IO

Sendfile零拷贝

SSD固态硬盘

AIO异步读写

增大error_log级别

关闭access_log

压缩access_log

批量、压缩写入

syslog代替本地IO

​	在另一个服务器上搭建 rsyslog 服务，然后配置 Nginx 通过 UDP 协议，将 access.log 日志文件从网络写入到 rsyslog 中，这完全移除了日志磁盘 IO

## TCP协议优化

### 减少报文的往返次数

​	启用fast_open

​	增大初始网络拥塞窗口

### 提高硬件资源的利用效率

​	启用TCP_DEFER_ACCEPT 功能，epoll_wait 并不会返回仅完成三次握手的连接，只有连
接上接收到的 TCP 数据报文后，它才会返回 socket，这样 Worker 进程就将原本 2 次切
换就降为 1 次了，虽然会牺牲一些即时性，但提高了 CPU 的效率。

### 高网络效率

打开 tcp_nopush、tcp_nodelay 功能，将小报文合并后批量发送，减少 IP 与 TCP 头部的占比

### 网络快速容错

建立连接

tcp_syn_retries

tcp_synack_retries

关闭连接

fin_timeout

tcp_retries1

tcp_retries2

lingering

端口复用

tcp_tw_reuse

tcp_tw_recycle

## CPU优化

优化cpu缓存的亲和性，将进程和cpu绑定

使用优秀的正则表达式库，可以提供更好的执行性能，比如PCRE

升级为万兆网卡

启用reuseport多个进程监听相同的端口，内核确保只有一个进程被唤醒，不会有惊群问题*

# 应用层协议优化

## 启用缓存避免发送请求

将文件大小和修改时间拼接为一个字符串作为文件摘要，将其与服务端的进行比较，相同则返回304，不同则返回最新的资源

## 降低请求次数

减少重定向的次数

合并请求

延迟发送请求

## 降低数据的大小

无损压缩

指压缩后不会损失任何信息，可以完全恢复到压缩前的原样文本文件、二进制可执行文件都会使用这类压缩方法

有损压缩

通过牺牲质量来提高压缩比，主要针对的是图片和音视频

 Brotli 无损压缩算法替换 gzip

# 哪里使用缓存提升性能

浏览器缓存

CDN 缓存

nginx缓存

应用本地缓存

​	一般缓存一些数据量不大、更新频率较低的数据，分布式环境下很难实现各个服务器的同步更新

分布式缓存

# 如何提升内存分配效率

分配内存时，如果在满足功能的情况下，可以在栈中分配的话，就选择栈

Ptmalloc2 则对各类尺寸的内存申请都有稳定的表现，更加通用

TCMalloc适用于分配 256KB 以下的内存，特别是在多线程环境下

# Tomcat调优

## 调整线程池参数

maxThreads

最大线程数

minSpareThreads

最小活 跃线程数

maxIdleTime

空闲线程等待时间

 maxQueueSize

最大的等待请求数，超过则请求拒绝

prestartminSpareThreads

是否在启动时就生成 minSpareThreads 个线程

## 调整连接器参数

禁用AJP连接器

acceptCount

acceptorThreadCount

maxConnections

compression

## 调整IO模式

Tomcat8以后的版本默认使用NIO模式

当Tomcat并发性能有较高要求或者出现瓶颈时，可以尝试使用APR模式，从操作系统级别解决异步IO问题

## 动静分离

可以使用Nginx+Tomcat相结合的部署方案，Nginx负责静态资源访问
Tomcat负责Jsp等动态资源访问处理(因为Tomcat不擅⻓处理静态资源)

# 网络数据传输慢，问题出在哪

netstat/ss命令查看网络收发包

1、如果客户端上的接收队列 RecvQ 不为零，则客户端应用程序是性能瓶颈

2、如果服务器上的发送队列 SendQ 为零，则服务器应用程序是性能瓶颈

3、如果客户端的接收队列 RecvQ 为零，而服务器的发送队列 SendQ 为非零，则网络本身是性能瓶颈


