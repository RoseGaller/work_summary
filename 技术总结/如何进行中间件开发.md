# 基础

## 集合

### array

数组是用于储存多个相同类型数据的集合，要求内存空间必须连续。根据下标获取数据的效率高

顺序添加元素的效率高，中间插入、删除元素的效率低下，涉及到元素的移动

适用场景：常用于滑动窗口、时间轮、环形缓冲区、窃取队列用数组、Disruptor中的环形数组

### linklist

内存的申请无须连续性

在尾部插入元素快

在中间插入或删除元素，都需要先遍历链表，再修改前后指针

LinkedList无限容量，容易OOM

### blockqueue

ArrayBlockingQueue

数组实现，可以循环利用，读写使用一把锁

PriorityBlockingQueue

优先队列，按照优先级存放元素，通过堆实现

LinkedBlockingDeque

初始化时，指定队列大小，不可修改，读写使用不同的锁

VariableLinkedBlockingQueue（[DynamicTp](https://dynamictp.cn/) ）

对LinkedBlockingDeque进行了扩展， capacity 不是 final 类型，可以动态修改

MemorySafeLinkedBlockingQueue（[DynamicTp](https://dynamictp.cn/) ）

继承VariableLinkedBlockingQueue，可以设置最大空闲内存，如果可用内存超过最大空闲内存，不再插入元素，避免出现OOM

org.dromara.dynamictp.common.queue.MemorySafeLinkedBlockingQueue#MemorySafeLinkedBlockingQueue(int)

```
public static final int THE_16_MB = 16 * 1024 * 1024;
```

```java
public MemorySafeLinkedBlockingQueue() {
    this(THE_16_MB);
}
```

```java
public MemorySafeLinkedBlockingQueue(final int maxFreeMemory) {
    super(Integer.MAX_VALUE);
    this.maxFreeMemory = maxFreeMemory;
}
```

```java
public void put(final E e) throws InterruptedException {
    if (hasRemainedMemory()) {
        super.put(e);
    }
}
```

```java
public boolean hasRemainedMemory() {
    if (MemoryLimitCalculator.maxAvailable() > maxFreeMemory) {
        return true;
    }
    throw new RejectedExecutionException("No more memory can be used.");
}
```

```java
public class MemoryLimitCalculator {
    
    private static volatile long maxAvailable;
    
    private static final ScheduledExecutorService SCHEDULER = Executors.newSingleThreadScheduledExecutor();
    
    static {
        refresh(); //立即获取最大空闲内存
        SCHEDULER.scheduleWithFixedDelay(MemoryLimitCalculator::refresh, 50, 50, TimeUnit.MILLISECONDS); //定时刷新
        Runtime.getRuntime().addShutdownHook(new Thread(SCHEDULER::shutdown));//关闭定时器
    }
    
    private static void refresh() {
        maxAvailable = Runtime.getRuntime().freeMemory();
    }
    
    public static long maxAvailable() {
        return maxAvailable;
    }
}
```

### map

HashMap

无序，基于数组+链表+红黑树实现，非线程安全

出现哈希冲突时，会引入一个链表，由于链表的查询性能低，又引入了红黑树

loadFactor表示加载因子，默认为 0.75。当存储的元素数量超过loadFactor*容量，进行双倍扩容。

TreeMap

按序存储，红黑树，非线程安全

ConcurrentSkipListMap

跳表，线程安全

ConcurrentHashMap

数组+链表或者红黑树实现

使用链表可以节省空间，查询慢

使用红黑树查询、更新快，但是占用空间大

在时间和空间上权衡，默认当链表长度大于8时，并且数组长度大于64时，转为红黑树

并发度为数组的长度，当table[i]为空时，使用cas，不为空使用synchronized

扩容时会使用多个线程分段扩容，分段的个数根据cpu核数计算（redis中也使用了分段迁移的思想，redis只是把迁移分散到其他的操作中，例如在处理完客户端的命令请求后，会判断如果有需要迁移的数据，就会进行帮助迁移）

## 锁

当一个线程拿不到锁的时候，有以下两种基本的等待策略:

策略1:放弃CPU，进入阻塞状态，等待后续被唤醒，再重新被操作系统调度。

 策略2:不放弃CPU，空转，不断重试，也就是所谓的“**自旋**”。

synchronized

如果获取不到锁，先自旋;如果自旋还拿不到锁，再阻塞

cas

对于悲观锁，认为数据发生并发冲突的概率很大，读操作之前就上锁。

synchronized关键字，ReentrantLock都是悲观锁的典型

对于乐观锁，认为数据发生并发冲突的概率比较小，读操作之前不上锁。等到写操作的时候，再判断数据在此期间是否被其他线程修改了。如果被其他线程修改了，就把数据重新读出来，重复该过程; 如果没有被修改，就写回去。判断数据是否被修改，同时写回新值，这两个操作要合成一个原子操作， 也就是CAS 

ReentrantLock



ReentrantReadWriteLock

采用“悲观读”的策略，当第一个读线程拿到锁之后， 第二个、第三个读线程还可以拿到锁，使得写线程一直拿不到锁，可能导致写线程“饿死

StampedLock

引入了“乐观读”策略，读的时候不加读锁，读出来发现数据被修改了，再升级为“悲观 读”，避免写线程被饿死

## 池化

### 线程

创建执行线程有两种方法: 

扩展Thread 类

实现Runnable 接口

在Java 中，可以创建两种线程: 守护线程、非守护线程

应用程序的所有非守护线程均已结束执行，无论是否有正在运行的守护线程，Java程序都会结束执行

### jdk线程池

核心参数

corePoolSize:在线程池中始终维护的线程个数

maxPoolSize:在corePooSize已满、队列也满的情况下，扩充线程至此值

keepAliveTime/TimeUnit:空闲线程，销毁所需要的时间，总线程数收缩回corePoolSize

blockingQueue:线程池所用的队列类型

threadFactory:线程创建工厂，可以自定义，有默认值Executors.defaultThreadFactory() 

RejectedExecutionHandler:corePoolSize已满，队列已满，maxPoolSize 已满，最后的拒绝策略

执行步骤

步骤一:判断当前线程数是否大于或等于corePoolSize。如果小于，则新建线程执行;如果大于， 则进入步骤二

步骤二:判断队列是否已满。如未满，则放入;如已满，则进入步骤三

步骤三:判断当前线程数是否大于或等于maxPoolSize。如果小于，则新建线程执行;如果大于， 则进入步骤四。

 步骤四:根据拒绝策略，拒绝任务

如果队列是无界的，将永远没有机会走到步骤三，也即maxPoolSize没有使用

使用 ThreadPoolExecutor 有个问题，Spring 容器关闭的时候可能任务队列里的任务还没处理完，有丢失任务的风险

注册钩子函数，Runtime.getRuntime().addShutdownHook(new Thread(“是否即可关闭线程池”));

#### 监控告警

1、线程池活跃度告警。活跃度 = activeCount / maximumPoolSize，当活跃度达到配置的阈值时，会进行事前告警。

2、队列容量告警。容量使用率 = queueSize / queueCapacity，当队列容量达到配置的阈值时，会进行事前告警。

3、拒绝策略告警。当触发拒绝策略时，会进行告警。

4、任务执行超时告警。重写 ThreadPoolExecutor 的 afterExecute() 和 beforeExecute()，根据当前时间和开始时间的差值算出任务执行时长，超过配置的阈值会触发告警。

5、任务排队超时告警。重写 ThreadPoolExecutor 的  beforeExecute()，记录提交任务时时间，根据当前时间和提交时间的差值算出任务排队时长，超过配置的阈值会触发告警

### 扩展

**尽量用最多的资源更快的执行任务，缩短任务等待的时间**

#### Tomcat中的线程池

步骤一:判断当前线程数是否大于或等于corePoolSize。如果小于，则新建线程执行;如果大于， 则进入步骤二。

**步骤二:当前线程数等于最大线程数，放入队列；当前正在执行的任务数小于当前线程数，放入队列；当前线程数少于最大线程数，开启新线程，如已满，则进入步骤三。**

步骤三:判断当前线程数是否大于或等于maxPoolSize。如果小于，则新建线程执行;如果大于， 则进入步骤四。

步骤四:根据拒绝策略，拒绝任务。

```
StandardThreadExecutor实现了接口Executor
```

org.apache.catalina.core.StandardThreadExecutor#startInternal

```java
protected void startInternal() throws LifecycleException { //启动
    this.taskqueue = new TaskQueue(this.maxQueueSize); //内置任务队列
    TaskThreadFactory tf = new TaskThreadFactory(this.namePrefix, this.daemon, this.getThreadPriority());
  //继承JDK自带线程池
    this.executor = new ThreadPoolExecutor(this.getMinSpareThreads(), this.getMaxThreads(), (long)this.maxIdleTime, TimeUnit.MILLISECONDS, this.taskqueue, tf);
    this.executor.setThreadRenewalDelay(this.threadRenewalDelay);
    if (this.prestartminSpareThreads) { //预启动核心线程
        this.executor.prestartAllCoreThreads();
    }

    this.taskqueue.setParent(this.executor);
    this.setState(LifecycleState.STARTING);
}
```

```java
public void execute(Runnable command) {//执行任务
    if (this.executor != null) {
        try {
            this.executor.execute(command); //jdk线程池
        } catch (RejectedExecutionException var3) {
            if (!((TaskQueue)this.executor.getQueue()).force(command)) { //再次放入队列
                throw new RejectedExecutionException("Work queue full.");
            }
        }

    } else {
        throw new IllegalStateException("StandardThreadPool not started.");
    }
}
```

org.apache.tomcat.util.threads.ThreadPoolExecutor#execute(java.lang.Runnable)

```java
public void execute(Runnable command) {
    this.execute(command, 0L, TimeUnit.MILLISECONDS);
}
```

```java
public void execute(Runnable command, long timeout, TimeUnit unit) {
    this.submittedCount.incrementAndGet(); //当前正在执行的任务数+1

    try {
        super.execute(command); //调用JDK的execute方法
    } catch (RejectedExecutionException var9) {
        if (!(super.getQueue() instanceof TaskQueue)) {
            this.submittedCount.decrementAndGet();
            throw var9;
        }

        TaskQueue queue = (TaskQueue)super.getQueue();

        try {
            if (!queue.force(command, timeout, unit)) { //再次放入队列
                this.submittedCount.decrementAndGet();
                throw new RejectedExecutionException("Queue capacity is full.");
            }
        } catch (InterruptedException var8) {
            this.submittedCount.decrementAndGet();
            Thread.interrupted();
            throw new RejectedExecutionException(var8);
        }
    }

}
```

org.apache.tomcat.util.threads.TaskQueue

```
TaskQueue继承LinkedBlockingQueue，put、take使用单独锁，put是尾部追加，take是从头部获取
```

```java
private ThreadPoolExecutor parent = null; //成员变量
```

```java
public boolean offer(Runnable o) {
    if (this.parent == null) { //不为空，ThreadPoolExecutor
        return super.offer(o);
    } else if (this.parent.getPoolSize() == this.parent.getMaximumPoolSize()) {
        return super.offer(o); //线程池已经达到最大线程数，放入队列
    } else if (this.parent.getSubmittedCount() < this.parent.getPoolSize()) {
        return super.offer(o); //当前执行任务数少于活跃线程数，说明线程执行挺快，放入队列
    } else { //任务开始出现积压，线程数尚未达到最大，返回false，开启新线程执行任务，否则放入队列
        return this.parent.getPoolSize() < this.parent.getMaximumPoolSize() ? false : super.offer(o);
    }
}
```

#### Motan中的线程池

com.weibo.api.motan.core.StandardThreadExecutor

继承JDK的线程池，设计思路借鉴于Tomcat

队列ExecutorQueue，继承LinkedTransferQueue，能保证更高性能，相比与LinkedBlockingQueue有明显提升

```java
public StandardThreadExecutor(int coreThreads, int maxThreads, long keepAliveTime, TimeUnit unit,
      int queueCapacity, ThreadFactory threadFactory, RejectedExecutionHandler handler) {
  //初始化
   super(coreThreads, maxThreads, keepAliveTime, unit,
         new ExecutorQueue(), //任务队列，继承自LinkedTransferQueue
         threadFactory, handler);
   ((ExecutorQueue) getQueue()).setStandardThreadExecutor(this);
	//提交的任务数，用于限制队列的长度
   submittedTasksCount = new AtomicInteger(0);
   
   // 最大并发任务限制： 队列长度数 + 最大线程数 
   maxSubmittedTaskCount = queueCapacity + maxThreads; 
}
```

```java
public void execute(Runnable command) {
   int count = submittedTasksCount.incrementAndGet();

   // 超过最大的并发任务限制，进行 reject
   // 依赖的LinkedTransferQueue没有长度限制，因此这里进行控制 
   if (count > maxSubmittedTaskCount) {
      submittedTasksCount.decrementAndGet();
      getRejectedExecutionHandler().rejectedExecution(command, this);
   }

   try {
      super.execute(command);
   } catch (RejectedExecutionException rx) {
			//再次尝试放入队列
      if (!((ExecutorQueue) getQueue()).force(command)) {
         submittedTasksCount.decrementAndGet();
         getRejectedExecutionHandler().rejectedExecution(command, this);
      }
   }
}
```

com.weibo.api.motan.core.ExecutorQueue#force

```java
public boolean force(Runnable o) {
   if (threadPoolExecutor.isShutdown()) {
      throw new RejectedExecutionException("Executor not running, can't force a command into the queue");
   }
   // forces the item onto the queue, to be used if the task is rejected
   return super.offer(o);
}
```

```java
public boolean offer(Runnable o) {
   int poolSize = threadPoolExecutor.getPoolSize(); //当前线程数

   // 当前线程数已达到最大线程数，放入队列
   if (poolSize == threadPoolExecutor.getMaximumPoolSize()) {
      return super.offer(o);
   }
   // 有空闲线程，放入队列
   if (threadPoolExecutor.getSubmittedTasksCount() <= poolSize) {
      return super.offer(o);
   }
   // 当前线程数小于最大线程数，返回false，即启动新的线程
   if (poolSize < threadPoolExecutor.getMaximumPoolSize()) {
      return false;
   }
   // 放入队列
   return super.offer(o);
}
```

#### Dubbo中的线程池

与Tomcat中线程池的实现类似，在执行拒绝策略时，异步打印堆栈信息

org.apache.dubbo.common.threadpool.support.eager.EagerThreadPool#getExecutor

```java
public Executor getExecutor(URL url) {
    String name = url.getParameter(THREAD_NAME_KEY, DEFAULT_THREAD_NAME);
    int cores = url.getParameter(CORE_THREADS_KEY, DEFAULT_CORE_THREADS);
    int threads = url.getParameter(THREADS_KEY, Integer.MAX_VALUE);
    int queues = url.getParameter(QUEUES_KEY, DEFAULT_QUEUES);
    int alive = url.getParameter(ALIVE_KEY, DEFAULT_ALIVE);

    // init queue and executor
    TaskQueue<Runnable> taskQueue = new TaskQueue<Runnable>(queues <= 0 ? 1 : queues);
    EagerThreadPoolExecutor executor = new EagerThreadPoolExecutor(cores,
            threads,
            alive,
            TimeUnit.MILLISECONDS,
            taskQueue,
            new NamedInternalThreadFactory(name, true),
            new AbortPolicyWithReport(name, url));
    taskQueue.setExecutor(executor);
    return executor;
}
```

org.apache.dubbo.common.threadpool.support.eager.EagerThreadPoolExecutor#execute

```java
public void execute(Runnable command) {
    if (command == null) {
        throw new NullPointerException();
    }
    // do not increment in method beforeExecute!
    submittedTaskCount.incrementAndGet(); //提交的任务数 +1
    try { 
        super.execute(command);
    } catch (RejectedExecutionException rx) {
        // retry to offer the task into queue.
        final TaskQueue queue = (TaskQueue) super.getQueue();
        try {
            //拒绝之后再次尝试放入队列
            if (!queue.retryOffer(command, 0, TimeUnit.MILLISECONDS)) {
                submittedTaskCount.decrementAndGet(); //任务数 -1
                throw new RejectedExecutionException("Queue capacity is full.", rx);
            }
        } catch (InterruptedException x) {
            submittedTaskCount.decrementAndGet();
            throw new RejectedExecutionException(x);
        }
    } catch (Throwable t) {
        // decrease any way
        submittedTaskCount.decrementAndGet();
        throw t;
    }
}
```

org.apache.dubbo.common.threadpool.support.eager.TaskQueue#offer

```java
public boolean offer(Runnable runnable) {
    if (executor == null) {
        throw new RejectedExecutionException("The task queue does not have executor!");
    }
		//当前启动的线程数
    int currentPoolThreadSize = executor.getPoolSize();
    // have free worker. put task into queue to let the worker deal with task.
    //有空闲的线程，放入队列，让worker线程处理
    if (executor.getSubmittedTaskCount() < currentPoolThreadSize) {
        return super.offer(runnable);
    }
	  //当启动的线程数小于最大线程数，放回false，jdk线程池中会将任务放入队列
    // return false to let executor create new worker.
    if (currentPoolThreadSize < executor.getMaximumPoolSize()) {
        return false;
    }

    // currentPoolThreadSize >= max 直接放入队列
    return super.offer(runnable);
}
```

org.apache.dubbo.common.threadpool.support.AbortPolicyWithReport#rejectedExecution

```java
public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
    String msg = String.format("Thread pool is EXHAUSTED!" +
            " Thread Name: %s, Pool Size: %d (active: %d, core: %d, max: %d, largest: %d), Task: %d (completed: "
            + "%d)," +
            " Executor status:(isShutdown:%s, isTerminated:%s, isTerminating:%s), in %s://%s:%d!",
        threadName, e.getPoolSize(), e.getActiveCount(), e.getCorePoolSize(), e.getMaximumPoolSize(),
        e.getLargestPoolSize(),
        e.getTaskCount(), e.getCompletedTaskCount(), e.isShutdown(), e.isTerminated(), e.isTerminating(),
        url.getProtocol(), url.getIp(), url.getPort());
    logger.warn(msg);
    dumpJStack(); //打印栈信息
    throw new RejectedExecutionException(msg);
}
```

```java
private void dumpJStack() {
    long now = System.currentTimeMillis();

    //dump every 10 minutes
    if (now - lastPrintTime < TEN_MINUTES_MILLS) {
        return;
    }

    if (!guard.tryAcquire()) {
        return;
    }

    ExecutorService pool = Executors.newSingleThreadExecutor();
    pool.execute(() -> {
        String dumpPath = url.getParameter(DUMP_DIRECTORY, System.getProperty("user.home"));

        SimpleDateFormat sdf;

        String os = System.getProperty(OS_NAME_KEY).toLowerCase();

        // window system don't support ":" in file name
        if (os.contains(OS_WIN_PREFIX)) {
            sdf = new SimpleDateFormat(WIN_DATETIME_FORMAT);
        } else {
            sdf = new SimpleDateFormat(DEFAULT_DATETIME_FORMAT);
        }

        String dateStr = sdf.format(new Date());
        //try-with-resources
        try (FileOutputStream jStackStream = new FileOutputStream(
            new File(dumpPath, "Dubbo_JStack.log" + "." + dateStr))) {
            JVMUtil.jstack(jStackStream);
        } catch (Throwable t) {
            logger.error("dump jStack error", t);
        } finally {
            guard.release();
        }
        lastPrintTime = System.currentTimeMillis();
    });
  	//shutDown只会立即关闭空闲的线程，还有任务执行的线程不会关闭
    //must shutdown thread pool ,if not will lead to OOM
    pool.shutdown();

}
```

org.apache.dubbo.common.utils.JVMUtil#jstack

```java
public static void jstack(OutputStream stream) throws Exception {
    ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
    for (ThreadInfo threadInfo : threadMxBean.dumpAllThreads(true, true)) {
        stream.write(getThreadDumpString(threadInfo).getBytes());
    }
}
```

```java
private static String getThreadDumpString(ThreadInfo threadInfo) {
    StringBuilder sb = new StringBuilder("\"" + threadInfo.getThreadName() + "\"" +
            " Id=" + threadInfo.getThreadId() + " " +
            threadInfo.getThreadState());
    if (threadInfo.getLockName() != null) {
        sb.append(" on " + threadInfo.getLockName());
    }
    if (threadInfo.getLockOwnerName() != null) {
        sb.append(" owned by \"" + threadInfo.getLockOwnerName() +
                "\" Id=" + threadInfo.getLockOwnerId());
    }
    if (threadInfo.isSuspended()) {
        sb.append(" (suspended)");
    }
    if (threadInfo.isInNative()) {
        sb.append(" (in native)");
    }
    sb.append('\n');
    int i = 0;

    StackTraceElement[] stackTrace = threadInfo.getStackTrace();
    MonitorInfo[] lockedMonitors = threadInfo.getLockedMonitors();
    for (; i < stackTrace.length && i < 32; i++) {
        StackTraceElement ste = stackTrace[i];
        sb.append("\tat " + ste.toString());
        sb.append('\n');
        if (i == 0 && threadInfo.getLockInfo() != null) {
            Thread.State ts = threadInfo.getThreadState();
            switch (ts) {
                case BLOCKED:
                    sb.append("\t-  blocked on " + threadInfo.getLockInfo());
                    sb.append('\n');
                    break;
                case WAITING:
                    sb.append("\t-  waiting on " + threadInfo.getLockInfo());
                    sb.append('\n');
                    break;
                case TIMED_WAITING:
                    sb.append("\t-  waiting on " + threadInfo.getLockInfo());
                    sb.append('\n');
                    break;
                default:
            }
        }

        for (MonitorInfo mi : lockedMonitors) {
            if (mi.getLockedStackDepth() == i) {
                sb.append("\t-  locked " + mi);
                sb.append('\n');
            }
        }
    }
    if (i < stackTrace.length) {
        sb.append("\t...");
        sb.append('\n');
    }

    LockInfo[] locks = threadInfo.getLockedSynchronizers();
    if (locks.length > 0) {
        sb.append("\n\tNumber of locked synchronizers = " + locks.length);
        sb.append('\n');
        for (LockInfo li : locks) {
            sb.append("\t- " + li);
            sb.append('\n');
        }
    }
    sb.append('\n');
    return sb.toString();
}
```

在spring中使用线程池时推荐使用ThreadPoolTaskExecutor,实现了InitializingBean、DisposableBean，生命周期由spring管理

初始化

org.springframework.scheduling.concurrent.ExecutorConfigurationSupport#afterPropertiesSet

```java
public void afterPropertiesSet() {
    this.initialize();
}

public void initialize() {
    if (this.logger.isInfoEnabled()) {
        this.logger.info("Initializing ExecutorService " + (this.beanName != null ? " '" + this.beanName + "'" : ""));
    }

    if (!this.threadNamePrefixSet && this.beanName != null) {
        this.setThreadNamePrefix(this.beanName + "-");
    }

    this.executor = this.initializeExecutor(this.threadFactory, this.rejectedExecutionHandler);
}
```

销毁

```java
public void destroy() {
    this.shutdown();
}

public void shutdown() {
    if (this.logger.isInfoEnabled()) {
        this.logger.info("Shutting down ExecutorService" + (this.beanName != null ? " '" + this.beanName + "'" : ""));
    }

    if (this.executor != null) {
        if (this.waitForTasksToCompleteOnShutdown) { //是否等待task都执行结束
            this.executor.shutdown();
        } else {
            this.executor.shutdownNow(); //立即关闭线程池
        }

        this.awaitTerminationIfNecessary(this.executor);
    }

}
```

动态

### 动态线程池

```java
@Target({ElementType.TYPE, ElementType.ANNOTATION_TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import(DtpConfigurationSelector.class)
public @interface EnableDynamicTp { //注解在启动类上
}
```

```java
@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface DynamicTp { //注解在需要动态修改配置的线程池上

    String value() default "";
}	
```

org.dromara.dynamictp.core.spring.DtpConfigurationSelector#selectImports

```java
public String[] selectImports(AnnotationMetadata metadata) {
    if (!BooleanUtils.toBoolean(environment.getProperty(DTP_ENABLED_PROP, BooleanUtils.TRUE))) { //是否启用动态修改
        return new String[]{};
    }
    return new String[] {
            DtpBaseBeanDefinitionRegistrar.class.getName(), //注册三个基础的BeanDefinition
            DtpBeanDefinitionRegistrar.class.getName(), //注册配置文件中的BeanDefinition
            DtpBaseBeanConfiguration.class.getName()//注册Bean
    };
}
```

org.dromara.dynamictp.core.spring.DtpBaseBeanDefinitionRegistrar#registerBeanDefinitions

```java
public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {
  //注册三个BeanDefinition
    registerHashedWheelTimer(registry);
    SpringBeanHelper.registerIfAbsent(registry, APPLICATION_CONTEXT_HOLDER, ApplicationContextHolder.class);

    SpringBeanHelper.registerIfAbsent(registry, DTP_POST_PROCESSOR, DtpPostProcessor.class,
            null, Lists.newArrayList(APPLICATION_CONTEXT_HOLDER, HASHED_WHEEL_TIMER));
}
```

org.dromara.dynamictp.core.spring.DtpBeanDefinitionRegistrar#registerBeanDefinitions

```java
public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {
    DtpProperties dtpProperties = DtpProperties.getInstance();
  	//根据配置信息填充DtpProperties
    BinderHelper.bindDtpProperties(environment, dtpProperties);
    val executors = dtpProperties.getExecutors();
    if (CollectionUtils.isEmpty(executors)) {
        log.warn("DynamicTp registrar, no executors are configured.");
        return;
    }
		//注册配置文件中配置的线程池
    executors.forEach(e -> {
        Class<?> executorTypeClass = ExecutorType.getClass(e.getExecutorType());
        Map<String, Object> propertyValues = buildPropertyValues(e);
        Object[] args = buildConstructorArgs(executorTypeClass, e);
        SpringBeanHelper.register(registry, e.getThreadPoolName(), executorTypeClass, propertyValues, args);
    });
}
```

org.dromara.dynamictp.core.spring.DtpPostProcessor#postProcessAfterInitialization

```java
public Object postProcessAfterInitialization(@NonNull Object bean, @NonNull String beanName) throws BeansException {
    if (!(bean instanceof ThreadPoolExecutor) && !(bean instanceof ThreadPoolTaskExecutor)) {
        return bean;
    }
  	//
    if (bean instanceof DtpExecutor) {
        return registerAndReturnDtp(bean);
    }
    //注册  jdk中ThreadPoolExecutor or spring中的ThreadPoolTaskExecutor
    return registerAndReturnCommon(bean, beanName);
}
```

```java
private Object registerAndReturnCommon(Object bean, String beanName) {
    String dtpAnnoValue;
    try {
        DynamicTp dynamicTp = beanFactory.findAnnotationOnBean(beanName, DynamicTp.class);
        if (Objects.nonNull(dynamicTp)) {
            dtpAnnoValue = dynamicTp.value();
        } else {
            BeanDefinition beanDefinition = beanFactory.getBeanDefinition(beanName);
            if (!(beanDefinition instanceof AnnotatedBeanDefinition)) {
                return bean;
            }
            AnnotatedBeanDefinition annotatedBeanDefinition = (AnnotatedBeanDefinition) beanDefinition;
            MethodMetadata methodMetadata = (MethodMetadata) annotatedBeanDefinition.getSource();
            if (Objects.isNull(methodMetadata) || !methodMetadata.isAnnotated(DynamicTp.class.getName())) {
                return bean;
            }
            dtpAnnoValue = Optional.ofNullable(methodMetadata.getAnnotationAttributes(DynamicTp.class.getName()))
                    .orElse(Collections.emptyMap())
                    .getOrDefault("value", "")
                    .toString();
        }
    } catch (NoSuchBeanDefinitionException e) {
        log.warn("There is no bean with the given name {}", beanName, e);
        return bean;
    }
    String poolName = StringUtils.isNotBlank(dtpAnnoValue) ? dtpAnnoValue : beanName;
    return doRegisterAndReturnCommon(bean, poolName);
}
```

```java
private Object doRegisterAndReturnCommon(Object bean, String poolName) {
    if (bean instanceof ThreadPoolTaskExecutor) {
        val proxy = newProxy(poolName, ((ThreadPoolTaskExecutor) bean).getThreadPoolExecutor());
        try {
            ReflectionUtil.setFieldValue("threadPoolExecutor", bean, proxy);
        } catch (IllegalAccessException ignored) { }
        DtpRegistry.registerExecutor(new ExecutorWrapper(poolName, proxy), REGISTER_SOURCE);
        return bean;
    }
    Executor proxy;
  	//封装原生的线程池
    if (bean instanceof ScheduledThreadPoolExecutor) {
        proxy = newScheduledTpProxy(poolName, (ScheduledThreadPoolExecutor) bean);
    } else {
        proxy = newProxy(poolName, (ThreadPoolExecutor) bean);
    }
  	//DtpRegistry根据线程池poolName管理所有线程池
    DtpRegistry.registerExecutor(new ExecutorWrapper(poolName, proxy), REGISTER_SOURCE);
    return proxy;
}
```

```java

@Configuration
@ConditionalOnClass(value = org.apache.curator.framework.CuratorFramework.class)
@ConditionalOnBean({DtpBaseBeanConfiguration.class})
@AutoConfigureAfter({DtpBaseBeanConfiguration.class})
public class DtpZkAutoConfiguration {

    @Bean
    public ZookeeperRefresher zookeeperRefresher() {
        return new ZookeeperRefresher();
    }
}
```

```java
public void afterPropertiesSet() {
    ConnectionStateListener connectionStateListener = (client, newState) -> {
        if (newState == ConnectionState.RECONNECTED) {
            this.loadAndRefresh();
        }

    };
    CuratorListener curatorListener = (client, curatorEvent) -> {
        WatchedEvent watchedEvent = curatorEvent.getWatchedEvent();
        if (null != watchedEvent) {
            switch(watchedEvent.getType()) {
            case NodeChildrenChanged:
            case NodeDataChanged:
                this.loadAndRefresh();
            }
        }

    };
 		//创建链接、监听配置变更
    CuratorFramework curatorFramework = CuratorUtil.getCuratorFramework(this.dtpProperties);
    String nodePath = CuratorUtil.nodePath(this.dtpProperties);
    curatorFramework.getConnectionStateListenable().addListener(connectionStateListener);
    curatorFramework.getCuratorListenable().addListener(curatorListener);
    this.cleanZkPropertySource(this.environment);
    log.info("DynamicTp refresher, add listener success, nodePath: {}", nodePath);
}
```

```java
private void loadAndRefresh() {
    this.refresh(CuratorUtil.genPropertiesMap(this.dtpProperties));
}
```

```java
protected void refresh(Map<Object, Object> properties) {
    if (MapUtils.isEmpty(properties)) {
        log.warn("DynamicTp refresh, empty properties.");
        return;
    }
    BinderHelper.bindDtpProperties(properties, dtpProperties);
    doRefresh(dtpProperties);
}
```

```java
protected void doRefresh(DtpProperties properties) {
    DtpRegistry.refresh(properties); // 修改线程池
    publishEvent(properties);//发布事件
}
```

```java
public static void refresh(DtpProperties dtpProperties) {
    if (Objects.isNull(dtpProperties) || CollectionUtils.isEmpty(dtpProperties.getExecutors())) {
        log.debug("DynamicTp refresh, empty thread pool properties.");
        return;
    }
  	//修改线程池信息
    dtpProperties.getExecutors().forEach(p -> {
        if (StringUtils.isBlank(p.getThreadPoolName())) {
            log.warn("DynamicTp refresh, thread pool name must not be blank, executorProps: {}", p);
            return;
        }
        ExecutorWrapper executorWrapper = EXECUTOR_REGISTRY.get(p.getThreadPoolName());
        if (Objects.nonNull(executorWrapper)) {
            refresh(executorWrapper, p);
            return;
        }
        log.warn("DynamicTp refresh, cannot find specified executor, name: {}.", p.getThreadPoolName());
    });
}
```

```java
private static void refresh(ExecutorWrapper executorWrapper, DtpExecutorProps props) {
    if (props.coreParamIsInValid()) {
        log.error("DynamicTp refresh, invalid parameters exist, properties: {}", props);
        throw new IllegalArgumentException("DynamicTp refresh, invalid parameters exist, properties: " + props);
    }
    TpMainFields oldFields = ExecutorConverter.toMainFields(executorWrapper);
    doRefresh(executorWrapper, props);
    TpMainFields newFields = ExecutorConverter.toMainFields(executorWrapper);
    if (oldFields.equals(newFields)) {
        log.debug("DynamicTp refresh, main properties of [{}] have not changed.",
                executorWrapper.getThreadPoolName());
        return;
    }
    // Get the changed keys
    List<FieldInfo> diffFields = EQUATOR.getDiffFields(oldFields, newFields);
    List<String> diffKeys = StreamUtil.fetchProperty(diffFields, FieldInfo::getFieldName);
    NoticeManager.doNoticeAsync(executorWrapper, oldFields, diffKeys);
    log.info("DynamicTp refresh, tpName: [{}], changed keys: {}, corePoolSize: [{}], maxPoolSize: [{}]," +
                    " queueType: [{}], queueCapacity: [{}], keepAliveTime: [{}], rejectedType: [{}]," +
                    " allowsCoreThreadTimeOut: [{}]", executorWrapper.getThreadPoolName(), diffKeys,
            String.format(PROPERTIES_CHANGE_SHOW_STYLE, oldFields.getCorePoolSize(), newFields.getCorePoolSize()),
            String.format(PROPERTIES_CHANGE_SHOW_STYLE, oldFields.getMaxPoolSize(), newFields.getMaxPoolSize()),
            String.format(PROPERTIES_CHANGE_SHOW_STYLE, oldFields.getQueueType(), newFields.getQueueType()),
            String.format(PROPERTIES_CHANGE_SHOW_STYLE, oldFields.getQueueCapacity(), newFields.getQueueCapacity()),
            String.format("%ss => %ss", oldFields.getKeepAliveTime(), newFields.getKeepAliveTime()),
            String.format(PROPERTIES_CHANGE_SHOW_STYLE, oldFields.getRejectType(), newFields.getRejectType()),
            String.format(PROPERTIES_CHANGE_SHOW_STYLE, oldFields.isAllowCoreThreadTimeOut(),
                    newFields.isAllowCoreThreadTimeOut()));
}
```

### 内存池

Netty，预先申请内存，提升申请内存速度，减少内存碎片

### 对象池

Netty，循环使用对象，减少资源在初始化和释放时的昂贵损耗

## 网络

### UDP 协议

一种没有状态的协议，发送一个个数据包时，协议本身不会关心这些包之间的关系，所以在复杂的网络下，包的顺序和可达性都是没有保证的，应用层需要自己处理这些包的丢失和乱序问题

在网络传输本来就比较稳定的内网环境下，或者对丢包可以容忍但对时延要求较高的场景下，UDP 协议有许多应用

### TCP协议

是面向连接的，传输基于字节流，也引入了状态，明确记录着每个包是否发送、是否被接受到、包本身的序列号等状态，提供了可靠有序的传输能力

#### 如何保证数据包发送到对端？

引入ack机制，每个包发送的时候会有一个序列号，接收端回 ACK 包的时候会把序列号 +1 发送回来

发送端如果没有收到某个包的 ACK 包，会在一段时间之后尝试重新发送（超时重传、快速重传），直到收到 ACK 为止

#### 如何提高发送数据包的效率？

正常情况下，每次发送一个新的包，都等待上一个包的ACK 回来之后才能发送，也就是每经过一个 RTT 的时间，我们只能发送一个包，这样一来一回的效率显然是很低的

在等待 ACK 的时候没有必要停止后续包的发送，因为网络传输虽然不稳定，但大部分包往往还是可达的，这样我们就可以获得数倍的传输效率提升

如果不幸遇到了丢包，接收端发来的ACK ，也就告诉了我们某个序列号之前的所有包全部收到，我们再根
据一定的策略，尝试重新发送对应丢失的包就可以了

发送方需要缓存已发出但尚未收到 ACK 的包，接收方收到包但没有被用户进程消费之前也得把收到的包留着



#### 什么是滑动窗口？

滑动窗口机制，可以用来控制发送端的流量，防止接收方处理不过来消息

接收端会建立一个滑动窗口，由接收方向发送方通告，TCP 首部里的 window 字段就是用来表示窗口大小的，窗口表示的就是接收方目前能接收的缓冲区的剩余大小

随着数据不断被发送，很快可用窗口就会被耗尽。在这种情况下，发送方也就不会继续发送数据了，这种发送端可用窗口为零的情况我们也称为“零窗口”。

当窗口为0时，会启动一个零窗口定时器，去定时地询问接收端窗口是否可用了

在接收端，如果进程读取缓冲区速度有所变化，会改变接收窗口的大小，每次通告给发送端，就可以控制发送端的发送速度了

#### 什么拥塞？

网络中每个节点不会有全局的网络通信情况，唯一能发现的就是自己的部分包丢了，这种时候它就有理由怀疑网络环境劣化，可能产生了拥塞

拥塞控制算法，则用来处理网络上数据包太多的情况，以避免网络中出现拥塞

#### 如何避免拥塞？

在出现拥塞的情况下，大部分节点都会不约而同地减少自己传输的包，这样网络拥塞情况就会得到极大的缓解，一直处于比较好的网络状态

在发送端定义一个窗口 CWND（congestion window），也就是拥塞窗口；发送端能发送的最多没有收到 ACK 的包，也不会超过拥塞窗口的范围

引入拥塞控制机制的 TCP 协议，发送端最大的发送范围是拥塞窗口和滑动窗口中小的一个

#### 如何实现拥塞控制算法

拥塞窗口会动态地随着网络情况的变化而进行调整，如果没有出现拥塞即没有出现丢包，我们扩大窗口大小，否则就减少窗口大小。

1、慢启动

​	在不确定拥塞是否会发生的时候，我们不会一上来就发送大量的包，而是会采用倍增的方式缓慢增加窗口的大小，窗口大小从 1 开始尝试，然后尝试 2、4、8、16 等越来越大的窗口

2、拥塞避免

​	倍增的方式窗口就会很快扩大；我们会在窗口大到一定程度时，减慢增加的速度，转成线性扩大窗口的方式，也就是每次收到新的 ACK 没有丢包的话只比上次窗口增大 1。慢启动阶段和拥塞避免阶段的分界点，叫“慢启动门限（ssthresh）”

随着窗口进一步缓慢增加，网络遇到了丢包的情况，就会假定这是拥塞造成的

3、超时重传

​	往往意味着拥塞情况更严重，会直接将 ssthresh设置为重传发生时窗口大小的一半，而窗口大小直接重置为 1，再进入慢启动阶段

4、快速重传

​	如果我们连续 3 次收到同样序号的 ACK，包还能回传，说明这个时候可能只是碰到了部分丢包，网络阻塞还没有很严重，我们就会采用柔和一点的策略，也就是快速恢复策略

​	会先把拥塞窗口变成原来的一半，ssthresh 也就设置成当前的窗口大小，然后开始执行拥塞避免算法

#### 区分MSS与MTU

最大报文段长度（MSS）是TCP协议的一个选项，用于在TCP连接建立时，收发双方协商通信时每一个报文段所能承载的最大数据长度（不包括文段头）

最大传输单元（MTU）用来限制上层（网络层）的数据包大小（IP Header + TCP Header + TCP Payload）

**TIME_WAIT一般出现在主动关闭的一端**。TIME_WAIT持续的时间是2MSL，保证旧的数据可以丢弃，成功发送ACK给对方。可以通过socket的选项SO_REUSEADDR来强制进程立即使用处于TIME_WAIT状态的连接占用的端口

HTTP

HTTP2.0

Netty



### 多路复用

#### 	select

​		每次调用select，需要将文件描述符拷贝进内核，最后再拷贝出来
​		采用遍历的方式来检查文件描述符是否处于就绪状态
​		支持的文件描述符数量太小，默认是1024

#### 	poll

​		与select模型类似，但是没有了文件描述符数量的限制

#### 	epoll

​		epoll_create()建立一个epoll对象，创建了红黑树和就绪链表，在内核中维护
​		红黑树负责维护socket的添加和删除，就绪链表负责存放准备就绪的事件
​		调用epoll_ctl，除了把socket放到红黑树上之外，还会注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪链表里
​		调用epoll_wait返回就绪列表

#### 	 水平触发LT和边缘触发ET

​			只要句柄上的事件一次没有处理完，下次调用epoll_wait时还会返回这个句柄，而ET模式仅在第一次返回。
​		高效的原因
​			红黑树维护socket文件描述符
​			通过注册回调获取准备就绪的socket，无需遍历所有的socket
​			避免了socket文件描述符的用户态和内核态的拷贝
​			ET模式避免了socket的重放操作

#### 		io-uring功能

​			epoll只能通知网络是否可以读写，但还是需要程序员写代码来读写网络。由于每次读写网络都会调用内核的函数，这样会造成大量的用户态和内核态切换，浪费很多计算资源
​			io_uring把epoll和之后的读写操作在内核态批量处理，同时用户进程和内核共享数据页表，这样既节省了状态切换开销，也节省了数据拷贝开销。

### BIO模型和NIO模型


BIO，同步阻塞 IO 模型

NIO ，一种新型 IO 编程模式，它的特点是同步、非阻塞

如果发起网络调用后，在服务端数据没有准备好的情况下客户端会阻塞，我们称为阻塞 IO；如果数据没有准备好，但网络调用会立即返回，我们称之为非阻塞 IO

如果发起网络调用的线程还可以做其他事情，我们称之为异步，否则称之为同步

NIO 与 BIO 的不同点在于，在调用 read 方法时，如果服务端没有返回数据，该方法不会阻塞当前调用线程

NIO引入了“事件就绪选择机制”，避免频繁调用read方法，查看是否有数据可读。事件就绪选择机制指的是，应用程序只需要在通道（网络连接）上注册感兴趣的事件（如网络读事件），操作系统会通知应用“数据已到达”，此时再调用读取 API

NIO 模型更适合需要大量在线活跃连接的场景，常见于服务端。如果每个连接发送的请求体过大，容易影响后续请求的处理。而Netty的read方法中会限制读取的次数和字节数，避免影响后面排队连接的读写操作

BIO 模型则适合只需要支持少量连接的场景，常常用于客户端

比如MySQL 客户端主要使用的是 BIO 模型，搭配连接池使用，为了保证数据库服务端的可用性，通常需要强制限制客户端能使用的连接数量。从连接池获取连接后，向mysql发送sql语句，连接会一直阻塞，直到mysql返回响应。如果是NIO模型，会无节制的使用一条连接发送sql，同时如果sql语句执行的特别慢时，导致服务端出现不可用的情况

而RPC框架中的消费端，与每个服务提供者只会建立一条长连接，使用NIO模型。因为一般消费者远大于服务提供者的数量，如果使用多连接，服务提供者很容易就被压垮。通过单一连接，保证单一消费者不会压死提供者

### 主从多 Reactor 模型

主 Reactor 模型，主要负责处理 OP_ACCEPT 事件（创建连接），通常一个监听端口使用一个线程

从 Reactor，本质是一个IO线程组，主要负责网络的读与写。当主Reactor收到一个新的客户端连接时，它会使用负载均衡算法从 线程组中选择一个线程，将 OP_READ、OP_WRITE 事件注册在线程的事件选择器中。接下来这个连接所有的网络读与写都会在被选择的这条线程中执行

业务线程池：通常 IO 线程解码出的请求将转发到业务线程池中运行，业务线程计算出对应结果后，再通过 IO 线程发送到客户端

### C10K 

C10K 问题本质上是一个操作系统问题，要在一台主机上同时支持 1 万个连接

哪些约束?

文件句柄

每个客户连接都代表一个文件描述符，一旦文件描述符不够用了，新的连接就会被放弃

在 Linux 下，单个进程打开的文件句柄数是有限制的，修改 /etc/sysctl.conf 文件，使得系统可用支持 10000 个描述符上限

系统内存

每个 TCP 连接都需要占用一定的发送缓冲区和接收缓冲区

cat 	/proc/sys/net/ipv4/tcp_wmem

网络带宽

假设 1 万个连接，每个连接每秒传输大约 1KB 的数据，那么带宽需要 10000 x 1KB/s x8= 80Mbps

如何解决?

非阻塞 I/O 加上 epoll 机制,单个线程感知IO事件，并调度IO操作

多线程处理IO，每个套接字的读写只在一个线程上执行

### C1000K

从物理资源使用上来说，100 万个请求需要大量的系统资源

​	从内存上说，每个请求需要 16KB 内存的话，那么总共就需要大约 15 GB 内存。
   从带宽上来说，假设只有 20% 活跃连接，即使每个连接只需要 1KB/s 的吞吐量，总共也需要 1.6 Gb/s 的吞吐量。千兆网卡显然满足不了这么大的吞吐量，所以还需要配置万兆网卡，或者基于多网卡 Bonding 承载更大的吞吐量

大量的连接也会占用大量的软件资源，比如文件描述符的数量、连接状态的跟踪（CONNTRACK）、网络协议栈的缓存大小（比如套接字读写缓存、TCP读写缓存）等等

大量请求带来的中断处理，也会带来非常高的处理成本。这样，就需要多队列网卡、中断负载均衡、CPU 绑定、RPS/RFS（软中断负载均衡到多个 CPU 核上），以及将网络包的处理卸载（Offload）到网络设备（如 TSO/GSO、LRO/GRO、VXLAN OFFLOAD）等各种硬件和软件的优化

C1000K 的解决方法，本质上还是构建在 epoll 的非阻塞 I/O 模型上。只不过，除了 I/O模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能

### C10M

有没有可能在单机中，同时处理 1000万的请求呢？这也就是 C10M 问题

Linux 内核协议栈做了太多太繁重的工作。从网卡中断带来的硬中断处理程序开始，到软中断中的各层网络协议处理，最后再到应用程序，这个路径实在是太长了

要解决这个问题，最重要就是跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序那里去。常见的机制比如DPDK 

DPDK，是用户态网络的标准。它跳过内核协议栈，直接由用户态进程通过轮询的方式，来处理网络接收。DPDK 还通过大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。

在大多数场景中，我们并不需要单机并发 1000 万的请求。通过调整系统架构，把这些请求分发到多台服务器中来处理，通常是更简单和更容易扩展的方案。

### 通信框架[sofa-bolt](https://github.com/sofastack/sofa-bolt)

一套基于 Netty 实现的网络通信框架

### dubbo中的请求分发模型

Netty中，全局并行执行，局部串行，一个连接只会由同一个线程执行，如果一个连接的处理变慢会影响后面哦爱对连接的处理

请求分发，就是根据请求的类型进行分发，有的请求处理比较快，直接在IO线程中处理即可，有的处理比较慢，可以交给业务线程处理

像kafka中就把请求分成了控制类请求和数据类请求，分开处理

org.apache.dubbo.remoting.transport.netty.NettyServer#NettyServer

```java
public NettyServer(URL url, ChannelHandler handler) throws RemotingException {
    super(ExecutorUtil.setThreadName(url, SERVER_THREAD_POOL_NAME), ChannelHandlers.wrap(handler, url));
}
```

org.apache.dubbo.remoting.transport.dispatcher.ChannelHandlers#wrap

```java
public static ChannelHandler wrap(ChannelHandler handler, URL url) {
    return ChannelHandlers.getInstance().wrapInternal(handler, url);
}
```

```java
protected ChannelHandler wrapInternal(ChannelHandler handler, URL url) {
    return new MultiMessageHandler(new HeartbeatHandler(ExtensionLoader.getExtensionLoader(Dispatcher.class)
            .getAdaptiveExtension().dispatch(handler, url)));
}
```

org.apache.dubbo.remoting.transport.netty.NettyServer#doOpen

```java
//继承自Netty中的SimpleChannelHandler处理连接、读、写、异常
final NettyHandler nettyHandler = new NettyHandler(getUrl(), this);
```

#### AllDispatcher

默认实现，所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。

```java
public class AllDispatcher implements Dispatcher {

    public static final String NAME = "all";

    @Override
    public ChannelHandler dispatch(ChannelHandler handler, URL url) {
        return new AllChannelHandler(handler, url); //继承WrappedChannelHandler，重写所有方法，先从WrappedChannelHandler获取线程池对象executor，再将执行的任务封装，交由线程池执行
    }

}
```

#### DirectDispatcher

所有消息都不派发到线程池，全部在 IO 线程上直接执行。

```java
public class DirectDispatcher implements Dispatcher {

    public static final String NAME = "direct";

    @Override
    public ChannelHandler dispatch(ChannelHandler handler, URL url) {
        return new DirectChannelHandler(handler, url);
    }

}
```

#### MessageOnlyDispatcher

只有**请求、响应消**息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。

```java
public class MessageOnlyDispatcher implements Dispatcher {

    public static final String NAME = "message";

    @Override
    public ChannelHandler dispatch(ChannelHandler handler, URL url) {
        return new MessageOnlyChannelHandler(handler, url); //与AllDispatcher类似，只重写received方法
    }

}
```

#### ExecutionDispatcher

只有**请求消息**派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行

```java
public class ExecutionDispatcher implements Dispatcher {

    public static final String NAME = "execution";

    @Override
    public ChannelHandler dispatch(ChannelHandler handler, URL url) {
        return new ExecutionChannelHandler(handler, url);//与MessageOnlyDispatcher类似，只不过在receive方法中区分了请求和响应
    }

}
```

#### ConnectionOrderedDispatcher

将连接断开事件由单独的线程池有序逐个执行，其它消息派发到线程池。

```java
public class ConnectionOrderedDispatcher implements Dispatcher {

    public static final String NAME = "connection";

    @Override
    public ChannelHandler dispatch(ChannelHandler handler, URL url) {
        return new ConnectionOrderedChannelHandler(handler, url);
    }

}
```

### Protobuf

怎样用最少的空间编码字段名

Protobuf会将字段名预分配数字，定义在 proto 文件中，比如
	message Person {
		string name = 1;
		uint32 id = 2;
		enum SexType {
			MALE = 0;
			FEMALE = 1;
		}
		SexType sex = 3;
}

Protobuf 是按照字段名、值类型、字段值的顺序来编码的

怎样高效地编码字段值？

字符串长度的编码逻辑与字段名相同，当长度小于 128（2 ）时，1 个字节就可以表示长度。若长度从 128 到 16384（2 ），则需要 2 个字节

如果你的消息中含有大量字符串，那么使用 Huffman 等算法压缩后再编码效果更好。

当使用 repeated 语法将多个数字组成列表时，还可以通过打包功能提升编码效率。在 Protobuf2 版本中，需要显式设置 [packed=True] 才能使用打包功能，而在Protobuf3 版本中这是默认功能

1个字节（前5个bit表示字段名 + 后3个bit表示类型） + 1个字节（长度） + 值

## 存储

LSM

### 写流程

首先将数据顺序写入 WAL log文件

再写到内存中mutable memtable（有序的内存结构，跳表），一般会有两个mutable memtable

如果mutable memtable到达一定的大小，转换成immutable memtable，写入另一个mutable memtable

执行Minor Compaction（Flushing）将immutable memtable转换成 level 0 sstable

后台根据一定的策略对不同level的sstable进行compaction，level0会限制文件的个数，其他层会限制文件的大小

### 读流程

先从mutable memtable读取，再从immutable memtable中读取

如果读不到，从文件中读取，每个文件都会有元数据，记录存放的key的范围

level0层的文件之间是无序的，每个文件的元数据都要查看一遍

level1以上，文件是按顺序存放的，很容易超找到所属的文件

### 合并的作用

丢弃不再被使用的旧版本数据；提升读性能，避免读取跨level读取多个文件

### write长尾延迟

#### 产生原因

Level0文件过多，Flushing就会停止，造成client卡顿

底层的合并过多，占用大量的磁盘资源，就造成写WAL、flushing延迟

#### 解决方案

优先flushing和低级别的合并，低level的合并优先于高level的合并，

根据系统的负载，给不同的操作设置不同的磁盘带宽

### 存在问题

### 读放大

读取时，首先读取memoryTable，如果读取不到，需要查找所在的sstable，由于sstable分层，因此该过程要由低向高level逐渐查找，在极端情况下，需要遍历所有sstable文件

### 写放大

写入时，除了将数据顺序写入日志外，还考虑将immemtable写入sstable文件，放到level 0以及sstable合并问题

### 代表产品

[SILK](https://www.usenix.org/system/files/atc19-balmau.pdf)

Preventing Latency Spikes in Log-Structured Merge Key-Value Stores

在低负载时期分配更多的IO带宽给内部操作
在client的流量洪峰时会减少Higher level Compaction的IO带宽占用，在client 流量低峰时增加Higher Level的Compactions的带宽

对LSM tree的internal ops进行优先级调度
Flush的优先级>L0 -> L1 Compaction优先级>Higher Level Compaction的优先级

#### LevelDB

http://bean-li.github.io/tags/#leveldb

https://kernelmaker.github.io/

https://youjiali1995.github.io/categories/

#### [PebblesDB](https://www.cs.utexas.edu/~vijay/papers/sosp17-pebblesdb.pdf)

Building Key-Value Stores using Fragmented Log-Structured Merge Trees

Compaction 造成了一些 key-value 反复的 rewrite，从而造成了 LSM-tree 的 write amplification，针对write amplification 做优化

从skip list借鉴了思想，引入guard概念
其将每一层进行分段，每个段称为一个guard，guard之间没有重叠的key，且每层的guard之间要求保序，但是guard内部可以无序

LSM造成写放大的原因是Level n和Level n+1合并时所有重叠的文件都需要重写一次

引入guard后，会以guard为单位进行合并，如果文件跨多个guard会进行重写，重写后的文件移到对应的guard，否则将其直接移到所属的guard，guard内部的文件是不需要排序的

#### RocksDB

https://zhuanlan.zhihu.com/p/77543818

https://blog.csdn.net/Z_Stand/article/details/109683804

https://kernelmaker.github.io/

https://youjiali1995.github.io/categories/

#### [Wisckey](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf)

Key、Value分开存储

Key的元数据信息（所属的文件以及文件中的位置）用LSM存储
Value按照顺序追加的方式存储在ValueLog文件中，但是带来了随机IO，可以搭配SSD
如果Value的占用空间小于一定阈值，可以将其和key同时存储在LSM中，避免了随机IO

只有元数据信息存储在LSM中，虽然也存在IO放大，但是因为数据量小，所以放大的效果明显降低
适用于Value比较大的场景

[TerarkDB](https://github.com/krareT/trkdb)

[goleveldb](https://github.com/syndtr/goleveldb)

[HashKV](https://www.usenix.org/system/files/conference/atc18/atc18-chan.pdf)

### 特性

随机写变顺序写

牺牲读性能

增加后台合并开销

### 读优化

布隆过滤器

​	通过多个 Hash 函数将一个数值映射到某几个字节上。这样用少量的字节就可以存储大量的数值，同时能快速地判断某个数值是否存在
​	虽然没有做映射的数值会有一定概率的误报，但可以保证“数值不存在”是绝对准确的

文件内部有序， 建立稀疏索引，方便key的查找

监控、业务、个性化设计

使用sendfile系统调用在大多数情况下只涉及一次上下文切换。在sendfile调用中，数据从文件系统缓冲区直接传输到套接字缓冲区，无需在用户空间和内核空间之间进行数据拷贝，因此减少了上下文切换的次数



## 算法

前缀树

限流算法

时间轮

跳表

布隆过滤器

位图

MVCC

LSM Tree

一致性哈希

Raft

全局唯一ID

分而治之

滑动窗口

LRU

调度算法

哈夫曼树

二分查找



# 实现

## 消息中间件

kafka/rocketmq/DDMQ/Qmq

## 一致性协议

Zookeeper

Raft

可以看源码分析/一致性协议 文件夹下的源码分析

## 注册中心

**注册中心是微服务体系的大脑，一旦出现问题会带来不可估量的损失，其可用性尤为重要。**

**综合来看，服务注册中心这种场景，AP 模式显然比 CP 模式更佳**

主要功能

服务注册

​	ip地址、端口号、序列化协议、路由规则、节点权重

​	注册粒度，接口粒度（RPC）、服务粒度（SpringCloud）

服务发现

​	启动拉取、通知回调、轮询拉取

健康检查

​	上报心跳：服务端上报心跳的线程与处理任务的线程不是同一个，心跳正常，不能说明服务正常

​	服务探测：模拟消费端发送请求，看看是否能够正常处理

变更通知

​	当服务提供方节点发生变更时，注册中心应该能够第一时间把变更事件或变更后的数据推送到服务订阅方

### 开源实现

#### Eureka（AP)

##### EurekeServer

AP模型，实现最终一致性

高可用通过部署多个服务节点实现

每个节点存储全量的服务信息，各个节点都是平等的，无主从之分，每个节点会同步本节点的数据到其他节点

每台Server都需要存储全量的服务数据，Server的内存明显会成为瓶颈，扩容的效果不明显。可根据业务划分注册中心集群

如果在15分钟内80%的客户端节点都没有正常的心跳，那么Eureka就认为客户端与注册中心发生了网络故障，注册中心进入自我保护机制

在一定时间内没有接收到某个微服务节点的心跳，将会注销该微服务节点(默认90s)

##### 服务消费方

周期性向服务端主动pull服务数据（默认30s）

存在实时性不足以及无谓的拉取性能消耗的问题

##### 服务提供方

启动时，向EurekeServer注册自己的信息

周期性向EurekeServer发送心（默认30s）跳以续约自己的信息

#### Zookeeper（CP）

ZooKeeper 是保住了一致性和分区容错性，选择性地牺牲了可用性

所有数据在内存中，当服务注册数、订阅数超过一定数量，不堪重负

写单点，不能水平扩展

按业务垂直拆分ZK集群

依赖ZK的Session活性心跳机制以及结合临时节点机制

#### Nacos（AP/CP）

注册中心分别实现了AP和CP模型，根据namespaceId和serviceName判断选择CP还是AP模型（DelegateConsistencyServiceImpl）

AP模型实现，最终一致性

1、本地存储

2、触发数据变更事件，通过UDP协议向消费方推送变更的服务

3、定时向其他的注册中心同步变更的数据，最终每个Nacos server的数据都保持一致性

CP模型实现	RAFT协议

#### Sofa-registry（AP）

支持海量的客户端，引入了中间层Session-Server，它是无状态的，很容易扩容

SessionServer承载客户端的连接，负责服务的发布和订阅
SessionServer也会对发布的服务进行保存，避免每次都请求DataServer
为了保持一致性，当DataServer上的服务信息发生了变更，及时推送变更的服务SessionServer

DataServer负责数据存储；数据按照dataInfoId一致性哈希算法存储，支持多副本备份。当DataServer扩容时，对数据进行迁入迁出

### 技术要点

#### 如何选择存储模型？

​		AP可以保证可用性，但不保证消费方能拉取到最新的数据

​		CP可以保证一致性，但当集群中的多数节点宕机了不能保证可用性

#### 如何解决海量注册请求？

​		将注册的时间随机化，避免同一时间注册

​		注册中心分片，将注册请求分散到其他节点

​		添加中间层，也可以缓存服务，合并重复的注册请求，再将合并后的请求发送给注册中心

​		服务提供方将注册中心的集群地址打散，避免注册请求都发送到同一台注册中心服务器

#### 如何处理连接超时

​	遍历扫描

​	动态分组（时间轮、桶）

#### 注册中心容灾考虑

​		服务调用链路弱依赖注册中心，只有服务发布、下线、扩缩容等必要时依赖

​		客户端设计考虑容灾，缓存拉取的服务信息

​		把所有服务清单列表同步一份到配置中心，注册中心发生故障后，可以从配置中心拉取数据

#### 如何解决服务状态检测？

​		探活，不仅是探测服务提供方是否宕机，还要探测是否可以对外提供正常的服务

​		服务提供方主动上报心跳，注册中心周期性检测服务提供方是否在一段时间内发送心跳

​		注册中心模拟消费方，发送请求，看是否能正常处理

​		服务消费方主动向服务提供方发起健康检查，根据一段时间内的成功响应的比率去判断下次是否继续向其发送请求

#### 如何优雅下线服务？

​		1、通知注册中心，此服务已下线，从服务列表中删除

​		2、注册中心通知消费方此服务已经下线，从服务列表中剔除，不再给服务提供者发送请求

​		3、通知服务提供者被迫下线。服务提供者接受到下线请求后，不再接受后续的请求，并且等待一定的时间将已经接受的请求处理完毕，如果在规定的时间未处理完成，则强制关闭

#### 如何解决通知风暴？

​		如果多个服务集群同时上线或者发生波动时，注册中心推送的消息就会更多，会占用过多的带宽资源

​		对变更的信息合并压缩

​		分批间隔发送

#### 保护策略

注册中心的自我保护

​		避免发生网络抖动，导致大量的心跳超时，进而剔除大量的服务

客户端自我保护

​		注册中心因为高负载，推送了异常数据。

​		客户端监测服务的可用节点数量下降超过一定阈值，进入自我保护，放弃使用新推送的服务注册信息

#### 如何解决海量存储？

1、可以将注册中心集群进行分组，服务提供方注册时随机选择某一个分组进行注册，每一组的集群负责存储一部分的服务注册请求

2、基于一致性 Hash 做了数据分片，每台注册中心节点只存储一部分的数据，随着数据规模的增长，只要添加注册中心节点即可

#### 如何解决海量客户端连接？

​	可以通过添加中间层解决，中间层无状态可以灵活扩容

## RPC设计

### 关键特性

#### 服务注册/发现

​			选择CP还是AP。在设计超大规模集群服务发现系统的时候，舍弃强一致性，更多地考虑系统的健壮性.
最终一致性才是分布式系统设计中更为常用的策略

#### 健康检测

​			判断服务的可用性
​			“连续”心跳失败次数必须到达某一个阈值。如果节点的心跳日志只是间歇性失败，也就是时好时坏，这样，失败次数根本没到阈值，调用方会觉得它只是“生病”了，并且很快就好了。那怎么解决呢？
​			基于滑动窗口，统计成功次数的百分比，可用率低于某个阈值时，进行降权处理

#### 路由策略

​			在负载均衡之前，选择符合发送规则的节点
​			IP路由、参数路由

#### 集群策略

​			快速失败
​			故障转移（重试）
​			失败自动恢复，后台记录失败请求重选节点定时重发，通常用于消息通知操作

#### 负载均衡策略

​			随机
​			轮询
​			本地服务优先
​			一致性哈希
​			最少连接
​			最小耗时
​			自适应（根据服务的处理能力，智能地控制发送给每个服务节点的请求流量）
​				收集运行信息，包括服务器的CPU核数、负载、内存等指标
​				收集耗时数据，如平均耗时、TP99、TP9999

#### 异常超时重试

​			业务逻辑必须是幂等的
​			重试时，去掉之前有问题的节点
​			在每次触发重试之前，我们需要先判定下这个请求是否已经超时，如果超时了会直接返回超时异常，否则我们需要重置下这个请求的超时时间
​			加个重试异常的白名单，用户可以将允许重试的异常加入到这个白名单中
​			时间轮实现超时

#### 优雅启动

​			启用预热
​				让刚启动的服务提供方应用不承担全部的流量，而是让它被调用的次数随着时间的移动慢慢增加，增加其权重
​			延迟暴露
​				应用启动完成后再将服务注册
​				在服务提供方启动后，接口注册到注册中心前，模拟调用逻辑，从而使 JVM 指令能够预热起来，并且也可以事先预加载一些资源，只有等所有的资源都加载完成后，最后才把接口注册到注册中心

#### 优雅关闭，服务下线

​			服务提供方通知注册中心，注册中心通知订阅该服务的消费方；涉及两次网络通信，不能保证实时性
​			服务提供方主动通知消费方
​			实现
​				服务方注册关闭的钩子（Runtime.addShutdownHook ），捕获关闭事件
​				开启关闭标志，通知消费方此服务已下线，并且拒绝接受新的请求
​				等待一定的时间，保证已经接受的请求处理完成
​				对请求进行计数，接入请求计数加1，处理完计数减1，计数为0，唤醒阻塞中的线程，进行资源的释放

#### 熔断限流

​			限流（服务方的自我保护）
​				单机限流
​					每个服务节点独立去执行限流逻辑
​				集群限流
​					依赖一个限流服务。调用端在发出请求时，先调用限流服务，如果达了限流阈值，返回限流异常
​			熔断（调用方的自我保护）
​				避免调用链中的异常请求，拖垮调用方甚至宕机
​				熔断器的工作机制主要是关闭、打开和半打开这三个状态之间的切换
​				在正常情况下，熔断器是关闭的
​				基于滑动窗口统计异常/超时的百分比，超过配置的阈值，熔断器打开；此时再发起请求，直接被熔断拦截
​				当熔断器打开一段时间后，会转为半打开状态。这时发送一个请求给服务端，如果能够正常地得到服务端的响应，则将状态置为关闭状态，否则设置为打开。

#### 		业务分组（流量隔离）

​			不同的调用方，调用流量不同。当某个调用方的流量突增时，可能影响到其他调用方，导致整体的可用性降低
​			为业务方划分不同的分组，每个业务方只能调用指定分组内的服务
​			根据业务的重要程度的不同划分核心服务和非核心服务，当核心服务的流量突增时，为保证核心服务的可用性，可以将其他业务分组下的服务移到核心服务分组下
​			由于分组之后，业务调用方所能调用的服务减少，出错的概率就升高了，保证调用方的高可用，在调用方配置多个分组，并将配置的分组区分下主次分组，只有在主分组上的节点都不可用的情况下才去选择次分组节点，只要主分组里面的节点恢复正常，就把流量都切换到主分组

#### 动态权重

动态权重标识service的处理能力，一个service能不能处理得过来，能不能响应得过来，应该由调用方说了算。

调用服务，快速处理了，处理能力跟得上；调用服务，处理超时了，处理能力很有可能跟不上了

可以把权重的范围限定为[0, 100]，把权重的初始值设为60分

每当service成功处理一个请求，认为service处理能力足够，权重动态+1

每当service超时处理一个请求，认为service处理能力可能要跟不上了，权重动态-10

如果某一个service的连接上，连续3个请求都超时，接下来的若干时间内，例如1秒，请求不再分配给这个service

如果某一个service的动态权重，降为了0，客户端就可以认为，服务器完全处理不过来了。接下来的若干时间内，例如1分钟，请求不再分配给这个service

### 基础

#### RPC通信流程

​			发布 RPC 服务，并将服务注册到服务注册中心上
​			当引用这个服务的应用启动时，会从服务注册中心订阅到相应服务的元数据信息。
​			当服务引用方拿到地址以后，就可以从中选取地址发起调用了

#### 可扩展且向后兼容的协议

#### 序列化

​			实现
​				JSON序列化
​				JDK 原生序列化
​				Protobuf
​				Hessian
​			优化点
​				对象尽量简单，没有太多的依赖关系，属性不要太多
​				入参对象与返回值对象体积不要太大，更不要传太大的集合
​				对象不要有复杂的继承关系

#### 通信模型

​			Reactor模型
​			Proactor模型

#### 动态代理

​			JDK动态代理
​			CGLib动态代理
​		功能点扩展
​			SPI机制

#### 线程模型

​			IO线程池
​			业务线程池
​				接口级别的线程池
​				方法级别的线程池

#### 调用方式

​			oneway
​			同步
​			异步
​				Future
​				Callback

#### 其他		

​		方法级别的缓存，减少请求次数
​		请求合并，提高了吞吐量，但是增加了延迟

1、尽可能减少io线程的占用时间；

​	使用非阻塞IO：非阻塞IO可以在IO操作未完成时立即返回，这样线程可以继续执行其他任务，而不是等待

​	使用IO多路复用技术：如`select`、`poll`、`epoll`等，可以同时监控多个IO通道的状态，从而在一个线程中处理多个IO操作

​	使用线程池：线程池可以预先创建并维护一组线程，当需要执行IO操作时，从线程池中获取一个线程，而不是每次都创建新线程

 2、尽可能减少线程上下文切换； 

​	使用合适的线程数量：过多的线程会导致频繁的上下文切换

​	使用无锁数据结构或减少锁的使用

​	批处理：将多个小任务组合成一个大任务，可以减少线程切换的次数

3、尽可能使用高效的序列化/反序列化

​	选择合适的序列化框架：不同的序列化框架有不同的性能和特点，如`Protobuf`、`JSON`、`XML`等。应根据实际需求选择合适的框架

​	使用二进制格式：相对于文本格式，二进制格式通常更紧凑，解析速度更快

​	避免不必要的序列化/反序列化：如果可能，尽量减少序列化/反序列化的次数，例如在服务器和客户端之间使用共享内存等方式

### 开源项目

​	octo-rpc（美团）
​	motan（微博）
​	dubbo（阿里）
​	sofa-rpc(蚂蚁)

## 分布式事务

### 解决方案

​	seata

​		AT

​		TCC

​		SAGA

​	ServiceComb Pack

​		TCC

​		SAGA

​	RocketMq半消息

​			1、先发送半prepare请求，Broker写消息到HalfTopic

​			2、执行事务操作，事务提交成功，发送commmit请求，将half消息保存到真实队列中，表示消费者可以消费

​		3、Broker定时扫描半消息，然后回调客户端提供的事务回查方法，判断对应的事务是否已经提交成功

​		涉及两次远程访问rocketmq、一次访问数据库

​	本地事务表+消息中间件

​		    1、业务操作数据和事务信息存储到同一数据库

​			2、事务提交成功时，将消息发送到消息中间件。若发送成功，将本地事务表中的状态修改为已发送；若发送失败，则重试；后端开启定时任务，扫描本地事务表，将未发送的消息发送到消息中间件。事务提交失败，则将事务回滚		

​	单机事务处理

​			避免分布式事务，所有的事务操作在单台机器通过本地事务上执行，然后定时将事务操作复制到对应的真实的数据库中

### 分布式事务理论

TCC（Try-Confirm-Cancel））

概念

​	一阶段：Try（检测预留资源）

​	二阶段：Confirm（真正的业务操作提交）/Cancel（预留资源释放）

注意事项

​	空回滚：Try未执行，Cancel执行了

​		Try 超时（丢包），分布式事务回滚，触发 Cancel

​		允许空回滚

​	悬挂：Cancel 比 Try 先执行

​		Try 超时（拥堵），分布式事务回滚，触发 Cance，拥堵的 Try 到达

​		拒绝空回滚后的Try 操作

幂等控制

FMT 模式（（Framework-managed transaction）

框架管理事务，托管事务的所有操作，一阶段和二阶段操作均由框架自动生成

一阶段：执行用户SQL，并且保存一条事务日志，包含修改前的数据快照和修改后的数据快照

二阶段：框架自动生成“提交/回滚”操作。提交:删除事务日志；回滚：如果当前的数据是和事务日志中记录的修改后的快照相同，则根据修改前的数据快照进行还原

### 优化点

二阶段异步执行

## 配置中心

### 开源实现

#### Apollo

1、用户在Portal操作配置发布，
2、Portal调用Admin Service的接口操作发布
3、Admin Service发布配置后，发送ReleaseMessage给各个Config Service
4、客户端从Config Service拉取配置信息

客户端实现

本地模式

从本地文件读取文件配置信息；注册监听器ConfigChangeListener，监听配置的变更

远端模式

先从Config Service拉取配置信息，然后启动定时器，周期性的从远端拉取配置信息。
启动长轮询，获取发生变更的namespace，客户端获取到namespace后，会立即请求Config Service获取该namespace的最新配置。获取到配置信息后会保存到本地文件

#### Nacos

客户端获取配置时优先从本地文件获取，如果获取不到，发送Http请求从远端获取，获取之后调用ConfigFilterChainManager的doFilter方法

注册监听器，监听配置的变化；后台通过长轮询获取变更的配置信息，并调用注册的监听器

客户端实现

拉取模式

Client主动拉取配置信息，服务端不需要对Client进行管理，但是Client不能及时获取更新的数据

推送模式

服务端主动推送最新的配置信息给Client，服务端需要对Client进行管理，服务端的复杂度上升

但是可以保证Client及时获取到最新的配置信息

Client连接比较多时，服务端可能出现瓶颈

对于配置下发敏感且Client数不是太大推荐使用推送模式

当Client数在万级以上推荐使用拉取模式，或者加入中间层，分担服务端的压力

## 链路追踪

最小的业务代码入侵系统

日志的记录收集对服务器的性能不会产生影响

丰富的查询统计功能

SkyWalking

zipkin

## Service Mesher

servicecomb-mesher（华为）

Mosn（蚂蚁）

Istio

Linkerd

## 分布式ID

特性

​	全局唯一

​	有序

​	具有业务意义

​	保证高可用

​	对数据库友好，占用空间不要太长

实现

​	UUID

​	Redis

​	Zookeeper顺序节点

​	Mongodb

​	Mysql自增ID

​	Snowflake（twitter）

​			整体长度通常是 64 （1 + 41 + 10+ 12 = 64）位，适合使用 Java 语言中的 long 类型来存储；头部是 1 位的正负标识位。紧跟着的高位部分包含 41 位时间戳，通常使用 System.currentTimeMillis()。后面是 10 位的 WorkerID，标准定义是 5 位数据中心 + 5 位机器 ID，最后的 12 位就是单位毫秒内可生成的序列号数目

​	Leaf-segment（美团）

​	tinyid（滴滴）

​	[seqsvr（微信）](https://www.infoq.cn/article/wechat-serial-number-generator-architecture/)

疑问：真的需要集中式的全局唯一Id吗？

## 分布式任务调度

### ElasticJob

使用 jar 的形式提供分布式任务的协调服务

引入分片的概念，将一个任务分解成多个独立的子任务

借助quartz框架，实现Job接口，注入自定义的ElasticJob，实现分布式任务调度

每个任务都有自己的Leader节点，创建Leader节点的服务器为主服务器，在任务开始调度的时候，需要为每个任务节点分配执行的分片项。分片仅可能发生在下次任务触发前

提供了失效转移和错过任务重新执行的功能，但是只适用于运行耗时较长且间隔较长的场景

运维平台

​	 读取作业注册中心展示作业配置和状态，展现作业运行轨迹及执行状态，或更新作业注册中心数据修改作业配置

[Saturn](https://github.com/vipshop/Saturn)

## 限流、熔断、降级 

### 开源实现

#### SDS(Service Downgrade System)

https://github.com/didi/sds

轻量级、简单、易用的限流、熔断、降级系统
sds-client做为客户端，提供了限流、熔断和数据统计等功能
sds-admin作为Server端主要是为了配置降级策略、提供丰富的仪表盘并且保存客户端上传的统计数据

#### Sentinel

https://github.com/alibaba/Sentinel

以方法为单位，基于滑动窗口，统计过去一段时间内的调用信息，比如调用次数、异常次数、超时时间等等，执行降级策略



阈值类型: 分为QPS和线程数 
QPS类型:每秒钟访问接口的次数
线程数:为接受请求该资源分配的线程数

流控模式
直接:就是达到设置的阈值后直接被流控抛出异常
关联:当关联的资源被流控时，当前的资源也会被流控，两个资源没有调用关系

链路：对于某个资源，有多个调用方，可以限制某个调用方的调用次数。有调用关系

流控效果

1、快速失败，直接跑出异常

2、预热

3、排队

中心思想：以固定的间隔时间让请求通过。当请求到来的时候，如果当前请求距离上个通过的请求通过的时间间隔不小于预设值，则让当前请求通过；否则，计算当前请求的预期通过时间，如果该请求的预期通过时间小于规则预设的 timeout 时间，则该请求会等待直到预设时间到来通过（排队等待处理）；若预期的通过时间超出最大排队时长，则直接拒接这个请求

适合场景：用于请求以突刺状来到，这个时候我们不希望一下子把所有的请求都通过，这样可能会
把系统压垮；同时我们也期待系统以稳定的速度，逐步处理这些请求，以起到“削峰填谷”的效果，而不是拒绝所有请求。	

选择排队等待的阈值类型必须是QPS

单机阈值是10，表示每秒通过的请求个数是10,那么每隔100ms通过一次请求.每次请求的最大等待时间为20000=20s，超过20S就丢弃请求

降级规则：响应时间、异常比例、异常数

热点参数

比如秒杀活动中，促销的商品请求流量十分大，我们就可以通过热点参数规则来控制促销商品的请求的并发量。而其他正常商品的请求不会收到限制

### 资源隔离

1、服务分组

2、线程资源隔离，服务端按请求分队列隔离；调用方工作线程隔离，不同的服务调用使用不同的线程

3、信号量限制调用的并发数量，但是无法控制超时时间

### 熔断

当对下游服务的调用异常量达到设定阈值后，打开断路器，触发熔断，在熔断期内，不会调用调用下游服务，直接调用降级方法；超过了熔断器，尝试调用下游服务，如果服务响应正常，则关闭断路器，否则断路器继续打开状态

状态流转

CLOSE关闭状态

异常量达到阈值，OPEN打开状态

OPEN状态超过一定时间后转为HALF状态

尝试调用下游服务，调用成功，则改为CLIOSE状态，否则处于OPEN状态

### 降级

所谓“降级”，就是当系统的容量达到一定程度时，限制或者关闭系统的某些非核心功能，从而把有限的资源保留给更核心的业务

执行降级无疑是在系统性能和用户体验之间选择了前者

### 限流

当系统容量达到瓶颈时，通过限制一部分流量来保护系统，并做到既可以人工执行开关，也支持自动化保护的措施

限流既可以是在客户端限流，也可以是在服务端限流

限流的实现方式既要支持 URL 以及方法级别的限流，也要支持基于QPS 和线程的限流

### 拒绝服务

当系统负载达到一定阈值时，例如 CPU 使用率达到 90% 或者系统 load 值达到 2*CPU 核数时，系统直接拒绝所有请求

在最前端的 Nginx 上设置过载保护，当机器负载达到某个值时直接拒绝HTTP 请求并返回 503 错误码，在 Java 层同样也可以设计过载保护

### 注意事项

注意数据一致性的问题和用户体验的问题