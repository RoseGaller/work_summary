# **性能问题归根结底是某个资源不够**

提升产品体验、降低运维成本

性能问题

​	单机容量低

​	响应慢

​		cpu、内存、网络

方法论

​	数据驱动

​	系统诊断

综合性诊断工具

​	JProfiler、JMC、FlameGraph

在线诊断工具

​	Greys、HouseMD、Btrace

业务层、组件层、系统层

# 如何获取性能数据

## 获取系统性能数据

nmon：按 C 键可加入 CPU 面板；按 M 键可加入内存面板；按 N 键可加入网络；按 D 键可加入磁盘

数据采集：nmon -f -s 5 -c 12 -m /home/qgc/Desktop/
-f 参数:生成文件
-T 参数:显示资源占有率较高的进程
-s 参数:-s 10表示每隔10秒采集一次数据
-c 参数:-c 10表示总共采集十次数据
 -m 参数:指定文件保存目录

## 获取JVM性能数据

jvisualvm

-Dcom.sun.management.jmxremote.port=9005
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false

CPU分析

统计方法的执行次数和执行耗时，用于分析哪个方法执行时间过长

内存分析

以通过内存监视和内存快照等方式进行分析，进而检测内存泄漏问题，优化内存使用情况

线程分析

查看线程的状态变化，以及一些死锁情况

JMC

jcmd PID JFR.start
jcmd  PID JFR.dump  filename=recording.jfr
jcmd PID JFR.stop

https://www.jb51.net/article/186337.htm

## 获取单个请求的调用链耗时

arthas中的trace命令

## 获取web接口的性能数据

ab、wrk

wrk 是一款针对 Http 协议的测试工具

wrk --latency -c 100 -t 2 --timeout 2 http://xxxx.com

-c, --connections <N>  跟服务器建立并保持的TCP连接数量 
-d, --duration    <T>  压测时间          
-t, --threads     <N>  使用多少个线程进行压测，压测时，是有一个主线程来控制我们设置的n个子线程间调度  
-s, --script      <S>  指定Lua脚本路径      
-H, --header      <H>  为每一个HTTP请求添加HTTP头     
--latency          在压测结束后，打印延迟统计信息  
--timeout     <T>  超时时间    
-v, --version          打印正在使用的wrk的详细版本信                                                     

# 怎样写代码能够让CPU执行得更快？

CPU 缓存分为数据缓存与指令缓存，对于数据缓存，我们应在循环体中尽量操作同一块内存上的数据，由于缓存是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时也有性能提升

对于指令缓存，有规律的条件分支能够让 CPU 的分支预测发挥作用，进一步提升执行效率

对于多核系统，如果进程的缓存命中率非常高，则可以考虑绑定 CPU 来提升缓存命中率

多线程并行访问不同的变量，这些变量在内存布局是相邻的（比如类中的多个变量），此时 CPU 缓存就会失效，通过填充缓存行来解决

# 如何提升内存分配的效率？

不同的 C 库内存池，都有它们最适合的应用场景，例如 TCMalloc 对多线程下的小内存分配特别友好，而 Ptmalloc2 则对各类尺寸的内存申请都有稳定的表现，更加通用

分配内存时，如果在满足功能的情况下，可以在栈中分配的话，就选择栈

Ptmalloc2 则对各类尺寸的内存申请都有稳定的表现，更加通用

TCMalloc适用于分配 256KB 以下的内存，特别是在多线程环境下

内存池管理着堆内存，它的分配速度比不上在栈中分配内存。只是栈中分配的内存受到生命周期和容量大小的限制，应用场景更为有限。然而它比内存池中的堆内存分配速度快很多！如果有可能的话，尽量在栈中分配分配

# 如何根据业务场景选择合适的锁？

互斥锁能够满足各类功能性要求，特别是被锁住的代码执行时间不可控时，它通过内核执行线程切换及时释放了资源，但它的性能消耗最大

如果能够确定被锁住的代码取到锁后很快就能释放，应该使用更高效的自旋锁，它特别适合基于异步编程实现的高并发服务

如果能区分出读写操作，读写锁就是第一选择，它允许多个读线程同时持有读锁，提高了并发性

当并发访问共享资源，冲突概率非常低的时候，可以选择无锁编程

不管使用哪种锁，锁范围内的代码都应尽量的少，执行速度要快。在此之上，选择更合适的锁能够大幅提升高并发服务的性能！

# 导致性能慢的原因

## 重试

​	控制重试的次数

​	控制重试的间隔

​	控制超时时间

​	识别重试的成功率，减少不必要的重试，通过熔断、快速失败等实现

## 公共资源争抢

​	定时任务、监控软件征用资源

​	降低资源争用

​	错峰，错开定时任务运行的时间

​	避免资源的共享

## 网络抖动

​	网络延时的最大值与最小值的差值

工具

traceroute -n -T www.xxx.com

ping www.xxx.com

MTR

尽量减少或避免太远的访问，多机房部署时，尽量同机房调用

## 缓存失效

进程内或进程外缓存失效，穿透到数据库，增加网络延迟

## 网络的延迟

传输延时

处理延时

网络传输慢

网络抖动

数据大

应用程序处理慢

# 常用命令

周期性的对I/O信息、内存信息、cpu使用情况采样
sar -u 1 3 cpu
sar -r 1 3 内存
sar -b 1 3 IO
sar -n 1 3 网络
sar -d 1 3 磁盘

ll /proc/${PID}/fd | wc -l

ll /proc/${PID}/task | wc -l （效果等同pstree -p | wc -l）

就能知道进程打开的句柄数和线程数。

sysctl配置的内核参数

dmesg系统级别异常

lsof查看打开文件

# 性能优化切入点

## 复用优化

重复的代码可以提取成公用的方法

缓冲

常见于对数据的暂存，然后批量传输或者写入，用来缓解不同设备之间频繁地、缓慢地随机写，主要针对的是写操作

缓存

进程内缓存（堆内缓存）

Ehcache

Caffeine

Guava Cache

JCache

进程外缓存

主要针对的是读操作

多级缓存

计算结果缓存

数据缓存

池化

对对象进行保存，用时从池中获取，用完再放回池子。降低对象的创建、销毁的开销

线程池

数据库连接池

对象池

浏览器缓存

CDN 缓存

nginx缓存

应用本地缓存:一般缓存一些数据量不大、更新频率较低的数据，分布式环境下很难实现各个服务器的同步更新

分布式缓存

## 结果集优化

Protobuf

可读性低，体积小，共并发场景可提高效率

json

可读性高，体积大

开启压缩，降低传输内容的大小

批量处理

## 高效的实现

高性能的网络通信框架Netty

## 算法优化

高效的算法实现、空间换时间，加快处理速度

## 计算优化

并行执行

多机执行

多进程执行

多线程

多协程执行

变同步为异步

多步骤执行彼此无依赖，多线程执行

非核心步骤稍后执行，消息队列异步执行

懒性加载

分屏加载

延迟加载

## 资源冲突优化

锁的优化

锁消除

JIT编译器，可以消除某些对象的加锁操作

无锁

减小锁的粒度

减少锁的持有时间

锁分级

synchronize的锁升级，偏向锁、轻量级锁、重量级锁

锁分离

读写锁

乐观锁

悲观锁

自旋锁

在用户态代码中完成加锁与解锁操作。被锁住的代码执行时间很短，可以用自旋锁取代互斥锁。

互斥锁

由操作系统内核实现，增加上下文的切换开销

java加锁的方式

​	synchronized

​	普通方法加锁时锁的对象是this

​	静态方法加锁时锁的对象是Class对象

## JVM优化

G1垃圾收集器

**确认是不是内存本身就分配过小** jmap -heap PID

**查看存活对象的情况**jmap -histo:live PID | more

## 数据库优化

## 操作系统优化

## 架构优化

调用方式

同异步调用

RPC框架选型

MQ选型

序列化方式

多路复用器

架构优化

动静分离

集群优化

数据隔离

服务拆分

异步驱动

负载均衡

程序优化

配置优化

并行优化

缓存优化

锁优化

## 网络优化

提升吞吐量

提升连接并发数

事件驱动

多路复用

降低连接成本

降低时延

TCP握手优化

重试次数

超时时间

复用策略

缓冲区队列

套接字缓冲区优化

滑动窗口

读写缓冲区

系统调整策略

网络专线

CDN优化

## 硬件优化

CPU

数据缓存优化

数据指令优化

内存

应用内存池

磁盘

磁盘缓存

零拷贝

IO优化

SSD固态硬盘

## Java代码优化法则

使用局部变量代替堆上分配

减少变量的作用范围

访问静态变量直接使用类名

字符串拼接使用StringBuilder

集合初始化时指定初始大小

遍历Map使用EntrySet，使用KeySet时获取的Key的集合，需要执行一次get操作

自增推荐使用LongAddr

不用异常控制流程

不在循环中使用try catch

合理使用PreparedStatement，使用预编译队对SQL的执行会提速，防止SQL注入

日志打印的注意事项

减少事务作用范围

使用位运算代替乘除运算

# 性能定律

## 帕累托法则

​	也被称为 80/20 法则、关键少数法则，或者八二法则

​	大约 80% 的用户使用集中在 20% 的程序功能

​	80% 的开发时间往往用在最精髓的 20% 代码上

​	80% 的代码演进和改动发生在大约 20% 的代码上

​	差不多有 80% 的错误发生在大约 20% 的代码上

​	 80％的流量将在总时间段的特定 20％内发生

​	程序的 80% 的时间是在运行大约 20% 的代码。

## 阿姆达尔定律

​	用于衡量处理器进行并行处理时总体性能的提升度

​	优先加速占用时间最多的模块，因为这样可以最大限度地提升加速比

# 操作系统性能瓶颈

## CPU

### 多处理器和 NUMA

现在的 CPU 普遍采用多处理器（Socket）来提高 CPU 性能，每个处理器都有自己可以直接访问的本地内存（Local Memory）

当 CPU 处理器访问本地内存时，会有较短的响应时间（称为本地访问 Local Access）。而如果需要访问外地 / 远程内存时候，就需要通过互联通道访问，响应时间就相比本地内存变慢了（称为远端访问 Remote Access）。所以 NUMA（Non-Uniform MemoryAccess）就此得名

对每一个进程和线程而言，当它运行在某一个处理器上时，它所对应的内存使用默认的分配方案是——优先尝试在请求线程当前所处的处理器的本地内存上分配。如果本地内存不足，才会分配到外地 / 远程内存上去

### 多核结构和多级缓存

处理器内部一般都是多核（Core）架构。CPU 的缓存通常分成了三个级别：L1、L2 和 L3

级别越小就越接近 CPU，速度更快，同时容量也越小。L1 和 L2 一般在核的内部

L3 缓存是三级缓存中最大的一级，同时也是最慢的一级,在同一个处理器内部的核会共享同一个 L3 缓存

### 超线程（Hyperthreading，HT）

一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程

当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器

具有 HT 超线程技术的处理器进行线程切换的成本很低

### CPU 的性能监测工具

top

uptime

查看过去的 CPU 负载情况

 mpstat 和 pidstat

查看每个核还有每个进程的情况

vmstat 

显示虚拟内存、内核线程、磁盘、系统进程、I/O 模块、中断等信息。

perf

性能剖析工具

### 常见的性能问题的表现

中断

各个核上的中断不平衡，导致有的核会超载而导致系统响应缓慢，但是其他的核反而空闲

超载

多核之间的平衡没做好

CPU做了无用功

应用程序本身的设计需要优化

空闲

太多的分支预测错误

太多内存数据操作

### CPU性能指标

CPU负载情况

CPU使用率

​	系统CPU

​	用户CPU

​	IO等待CPU

​	注意：使用率和负载的关系往往不是线性的

中断

​	软中断	

​	硬中断

上下文切换

平均负载

​	理想情况下，平均负载等于逻辑 CPU 个数，这表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了

CPU 缓存的命中率

### CPU超标量

超标量（superscalar）CPU架构是能够在相同的CPU主频下实现更高的CPU吞吐率（throughput）

超标量处理器运用了指令级并行运算，将一条指令分解为多个步骤（周期），并且每一个周期时间相同，允许多条指令同时存在，提升了指令并行性，进而提升性能

### 工具集

系统平均负载

​	uptime，运行状态（等待、正在运行）进程数和不可中断进程数（磁盘IO）

​	不可中断进程数增多了，那么就需要做 I/O 的分析，也就是用 dstat 或 sar  -d等工具，进一步分析 I/O 的情况	

​	如果是运行状态进程数增多了，那就需要回到 top 和 pidstat，找出这些处于运行状态的到底是什么进程，然后再用进程分析工具，做进一步分析

CPU 上下文切换

​	vmstat来查询系统总体的上下文切换情况

​	pidstat 给它加上 -w 选项，可以查看每个进程上下文切换的情况

性能调优利器--火焰图

mpstat

​	用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标

 pidstat -u 5 1

​	用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。

查看哪个函数导致了CPU使用率升高:perf top -g -p PID(-g 开启调用关系分析)

pidstat -p PID显示了指定进程的cpu使用率

execsnoop：监控短时进程；包括进程 PID、父进程 PID、命令行参数

perf

​	以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题

​	perf record(提供了保存数据的功能) 和 perf report(对保存后的数据解析展示)

​	perf top，实时显示占用 CPU 时钟最多的函数或者指令，可以用来查找热点函数，

 	perf top -g -p PID，查看进程调用关系
 	
 	ps aux | grep PID 根据PID从所有进程中查找

pstree：以树形结构显示程序和进程之间的关系

短时进程导致的CPU过高

​	execsnoop：查看进程的父进程 PID 以及它的命令行参数

软中断导致CPU使用率升高

​	查看 /proc/softirqs文件中各种类型软中断的变化情况，确定到底是哪种软中断出的问题（watch -d cat /proc/softirqs）（NET_RX（网络接收））

​	发现是网络接收中断导致的问题，那就可以继续用网络分析工具 sar 和 tcpdump 来分析（sar -n DEV 1：查看系统的网络收发情况；tcpdump -i eth0 -n tcp port 80：抓取eth0上的包， -i eth0 选项指定网卡eth0，并通过 tcp port 80 选项指定TCP协议的 80 端口）

THP(透明大页)

​	一个2M的连续物理内存，用一个页表项来映射更大的内存，这样可以减少Page Fault，因为需要的页数少了，这也会提升 TLB 命中率

​	grep -i HugePages /proc/meminfo(查看系统分配的大页)

​	echo never > /sys/kernel/mm/transparent_hugepage/enabled（禁用大页）

​	是否启用大页需要考虑业务数据的局部性

中断

软中断是用来处理硬中断在短时间内无法完成的任务的

硬中断执行时间短，如果发生频率不高，一般不会给业务带来明显影响

追踪瞬时系统快照

sysctl -w kernel.sysrq = 1（启用sysrq所有功能）

echo t > /proc/sysrq-trigger（当前的任务快照保存下来）

dmesg（查看快照信息）

### 优化

应用程序优化

算法优化：使用复杂度更低的算法，可以显著加快处理速度

异步处理：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的并发处理能力

多线程代替多进程：前面讲过，相对于进程的上下文切换，线程的上下文切换并不切换进程地址空间，因此可以降低上下文切换的成本

善用缓存：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来

系统优化

将进程或者线程和CPU进行绑定，提升多核CPU下的缓存命中率

提升指令缓存的命中率

CPU含有分支预测器，如果分支预测器可以预测接下来要在哪段代码执行（比如 if 还是 else 中的指令），就可以提前把这些指令放在缓存中，CPU 执行时就会很快

提升数据缓存的命中率

遍历访问数组时，按照内存布局顺序访问将会带来很大的性能提升

填充缓存行，默认64字节，消除伪共享

NUMA（Non-Uniform Memory Access）优化：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存

中断负载均衡：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上

优先级调整：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理

## 磁盘IO

### 文件系统

Linux 文件系统为每个文件都分配两个数据结构，索引节点（indexnode）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。

索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等，索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。

目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存

磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成

磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。超级块，存储整个文件系统的状态；索引节点区，用来存储索引节点；数据块区，则用来存储文件数据

### 虚拟文件系统

为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS

VFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节

文件系统三种类型

第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS

第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统

第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。

### 文件系统 I/O

缓冲 I/O 与非缓冲 I/O

缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件

非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存

直接 I/O 与非直接 I/O

直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件

非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘

阻塞 I/O 和非阻塞 I/O

阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务

非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果

同步和异步 I/O

同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得I/O 响应

异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序

### Linux 磁盘 I/O 的工作原理

#### 磁盘类型

按存储介质分

1、机械磁盘

机械磁盘主要由盘片和读写磁头组成，数据就存储在盘片的环状磁道中。在读写数据前，需要移动读写磁头，定位到数据所在的磁道，然后才能访问数据

如果 I/O 请求刚好连续，那就不需要磁道寻址，自然可以获得最佳性能

机械磁盘的最小读写单位是扇区，一般大小为 512 字节

2、固态磁盘

通常缩写为 SSD，由固态电子元器件组成。固态磁盘不需要磁道寻址，所以，不管是连续 I/O，还是随机 I/O 的性能，都比机械磁盘要好得多

固态磁盘的最小读写单位是页，通常大小是 4KB、8KB 等

存在“先擦除再写入”、写放大的限制

按接口分

比如可以把硬盘分为 IDE（Integrated Drive Electronics）、SCSI（Small Computer SystemInterface） 、SAS（Serial Attached SCSI） 、SATA（Serial ATA） 、FC（FibreChannel） 等

不同的接口，往往分配不同的设备名称。如果是多块同类型的磁盘，就会按照a、b、c 等的字母顺序来编号

#### 块设备

在 Linux 中，磁盘实际上是作为一个块设备来管理的，也就是以块为单位读写数据，并且支持随机读写

每个块设备都会被赋予两个设备号，分别是主、次设备号

主设备号用在驱动程序中，用来区分设备类型

次设备号则是用来给多个同类设备编号

#### 通用块层

与虚拟文件系统 VFS 类似，为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备

通用块层除了为文件系统和应用程序，提供访问块设备的标准接口外，还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率

四种调度算法

第一种 NONE ，更确切来说，并不能算 I/O 调度算法。因为它完全不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理

第二种 NOOP ，是最简单的一种 I/O 调度算法。它实际上是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘

第三种 CFQ（Completely Fair Scheduler），也被称为完全公平调度器，是现在很多发行版的默认 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求

最后一种 DeadLine 调度算法，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理，DeadLine 调度算法，多用在 I/O 压力比较重的场景，比如数据库等

#### IO栈

##### 1、文件系统层

包括虚拟文件系统和其他各种文件系统的具体实现

为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据

##### 2、通用块层

包括块设备 I/O 队列和 I/O 调度器

对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层

##### 3、设备层

包括存储设备和相应的驱动程序

负责最终物理设备的 I/O 操作

Linux优化IO的方式

为了优化文件访问的性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，以减少对下层块设备的直接调用

为了优化块设备的访问效率，会使用缓冲区，来缓存块设备的数据



### 如何对IO进行优化？

#### 如何寻找瓶颈？

可以先用 top ，来观察 CPU 和内存的使用情况，主要是sys、 iowait、Buffer/Cache  ；查看buffer/cache占用，建议使用pcstat或者hcache

然后再用 iostat ，来观察磁盘的 I/O 情况

使用 pidstat 加上 -d 参数，查看每个进程的 I/O 情况

通过strace -p pid查看进程的系统调用write情况

怎样知道哪里在写文件

​	借助bcc 软件包的下filetop工具

​	主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称

​	ps -efT | grep 线程Id	：根据线程Id找到对应的进程

opensnoop工具，可以动态跟踪内核中的open 系统调用，可以得到写入文件的具体路径

strace默认是不开启线程追踪的，而写文件是由子线程执行的，所以看不到write调用

可以通过pstree查看进程的线程信息，再用strace跟踪。或者，通过strace -fp pid 跟踪所有线程

通过lsof -p  pid查看进程打开了哪些文件

#### 如何确定IO性能指标

##### 磁盘性能指标

使用率：指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈

饱和度：指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求

IOPS（Input/Output Per Second）：指每秒的 I/O 请求数。在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能

吞吐量：指每秒的 I/O 请求大小。在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能

响应时间：指 I/O 请求从发出到收到响应的间隔时间

##### 文件系统

存储空间的使用情况，包括容量、使用量以及剩余空间等

索引节点的使用情况，它也包括容量、使用量以及剩余量等

缓存使用情况,页缓存、目录项缓存、索引节点缓存、各个具体文件系统（如 ext4、XFS 等）的缓存

##### 文件 I/O

包括 IOPS（包括 r/s 和 w/s）、响应时间（延迟）以及吞吐量（B/s）等

通常还要考虑实际文件的读写情况，比如，结合文件大小、文件数量、I/O 类型等，综合分析文件 I/O 的性能

#### 如何进行IO基准测试

推荐用性能测试工具 fio ，来测试磁盘的 IOPS、吞吐量以及响应时间等核心指标

在基准测试时，一定要注意根据应用程序 I/O的特点，来具体评估指标

安装 Ubuntu：apt-get install -y fio ; CentOS :yum install -y fio 

man fio 查询它的使用方法

测试出，不同 I/O 大小（一般是 512B 至 1MB 中间的若干值）分别在随机读、顺序读、随机写、顺序写等各种场景下的性能情况

执行参数：

​	nam,表示io类型，read、write、randread、randwrite，比如-name=randread 

​	direct，表示是否跳过系统缓存， 1 ，就表示跳过系统缓存， -direct=1

​	iodepth，表示使用异步 I/O，同时发出的 I/O 请求上限 ， 比如-iodepth=64 

​	rw，表示 I/O 模式。 read/write 分别表示顺序读 / 写，而randread/randwrite 则分别表示随机读 / 写

​	ioengine，表示 I/O 引擎，它支持同步（sync）、异步（libaio）、内存映射（mmap）、网络（net）等各种 I/O 引擎

​	bs，表示 I/O 的大小

​	filename，表示文件路径，当然，它可以是磁盘路径（测试磁盘性能），也可以是文件路径（测试文件系统性能）

测试报告的指标：

​	slat ，是指从 I/O 提交到实际执行 I/O 的时长

​	clat ，是指从 I/O 提交到 I/O 完成的时长

​	lat ，指的是从 fio 创建 I/O 到 I/O 完成的总时长

对同步 I/O 来说，由于 I/O 提交和 I/O 完成是一个动作，所以 slat 实际上就是 I/O 完成的时间，而 clat 是 0

使用异步 I/O（libaio）时，lat 近似等于 slat + clat 之和

fio 支持 I/O 的重放

​	blktrace /dev/sdb 使用 blktrace 跟踪磁盘 I/O，注意指定应用程序正在操作的磁盘

​	 blkparse sdb -d sdb.bin	将结果转化为二进制文件

​	fio --name=replay --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin  使用 fio 重放日志

### 如何对I/O 观测？

磁盘IO观测

iostat 是最常用的磁盘 I/O 性能观测工具

提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats

进程 I/O 观测

pidstat，加上 -d 参数，就可以看到进程的 I/O 情况

iotop，是类似于 top 的工具，可以按照 I/O大小对进程排序，然后找到 I/O 较大的那些进程



### 应用程序优化

追加写代替随机写，减少寻址开销

充分利用系统缓存，降低IO次数

可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统

频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数

在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，减少系统调用次数

cgroups来限制进程 / 进程组的 IOPS 以及吞吐量，避免I/O被某个应用完全占用

零拷贝

​	降低上下文切换开销和内存的拷贝次数

​	磁盘文件拷贝到pagecache，pagecache拷贝到socket缓冲区，socket缓冲区拷贝到网卡。如果网卡支持SG-DMA技术，可以省略socket缓冲区的拷贝，直接从cache拷贝到网卡

在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。ionice 支持三个优先级类：Idle、Best-effort 和 Realtime。其中， Best-effort 和 Realtime 还分别支持 0-7 的级别，数值越小，则表示优先级别越高

### 文件系统优化

第一，你可以根据实际负载场景的不同，选择最适合的文件系统

比如 Ubuntu 默认使用ext4 文件系统，而 CentOS 7 默认使用 xfs 文件系统

相比于 ext4 ，xfs 支持更大的磁盘分区和更大的文件数量

但是 xfs 文件系统的缺点在于无法收缩，而 ext4 则可以

第二，在选好文件系统后，还可以进一步优化文件系统的配置选项

包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等

第三，可以优化文件系统的缓存

比如，你可以优化 pdflush 脏页的刷新频率（比如设置 dirty_expire_centisecs 和dirty_writeback_centisecs）以及脏页的限额（比如调整 dirty_background_ratio 和dirty_ratio 等）

最后，在不需要持久化时，你还可以用内存文件系统 tmpfs，以获得更好的 I/O 性能

### 磁盘优化

最简单有效的优化方法，就是换用性能更好的磁盘，比如用 SSD 替代 HDD

根据磁盘和应用程序的模式选择合适的IO调度算法，SSD通常选用NOOP算法，数据库选用deadline

为应用程序的数据，设置单独的磁盘

在顺序读比较多的场景下，增大磁盘预读的数据

可以调整磁盘队列的长度/sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）

最后，要注意，磁盘本身出现硬件错误，也会导致 I/O 性能急剧下降

可以查看 dmesg 中是否有硬件 I/O 故障的日志

 还可以使用 badblocks、smartctl 等工具，检测磁盘的硬件问题

用 e2fsck 等来检测文件系统的错误

如果发现问题，你可以使用 fsck 等工具来修复

### 命令工具集合

df：文件系统的磁盘空间使用情况。 -h 选项，可以获得更好的可读性； -i 参数，查看索引节点的使用情况。除了文件数据，索引节点也占用磁盘空间。如果小文件过多，会出现索引节点空间不足的情况

vmstat观察缓存和 I/O 的变化趋势

观察页缓存的大小

free 输出的 Cache，是页缓存和可回收 Slab 缓存的和，可以从/proc/meminfo ，直接得到它们的大小

内核使用 Slab 机制，管理目录项和索引节点的缓存，/proc/meminfo 只给出了Slab 的整体大小，具体到每一种 Slab 缓存，还要查看 /proc/slabinfo 这个文件。

iostat：查看每个磁盘的IO使用率、队列长度、队列等待时间、处理时间。-d 选项是指显示出 I/O 的性能指标；-x 选项是指显示出扩展统计信息

pidstat：-d查看各进程的IO情况

filetop：主要跟踪内核中文件的读写情况，并输出线程ID、读写大小、读写类型以及文件名称

ps -efT | grep tid：根据线程id查找进程

opensnoop：动态跟踪内核中的open系统调用，找出文件的路径

iotop：按照 I/O大小对进程排序

strace：分析系统调用,-p指定进程号，查看进程正在读写的文件；-f 跟踪所有线程读写；-tt 在每行输出的前面，显示毫秒级别的时间；-T 显示每次系统调用所花费的时间；-e 控制要跟踪的事件和跟踪行为,比如指定要跟踪的系统调用名称

lsof：查看进程打开文件列表,-p指定进程号

echo 1 > /proc/sys/vm/drop_caches 释放pageCache

如何判断磁盘是在 顺序读 还是 随机读？

​	可以使用strace把系统调用都找出来，write是否连续就可以看到了

​	或者使用 blktrace 观察

## 网络

### 修改TCP缓冲区

发送缓冲区

缓冲区动态调节功能默认是打开的

net.ipv4.tcp_wmem = 4096        16384   4194304

接收缓冲区

net.ipv4.tcp_moderate_rcvbuf = 1；打开自动调节功能

接收缓冲区的调节是通过 tcp_mem 配置完成的net.ipv4.tcp_mem = 88560     118080  177120(当 TCP 内存小于第 1 个值时，不需要进行自动调节；在第 1 和第 2 个值之间时，内核开始调节接收缓冲区的大小；大于第 3 个值时，内核不再为 TCP 分配新内存，此时新连接是无法建立的)

关键点

保证缓冲区的动态调整上限达到带宽时延积（带宽与时延的乘积），而下限保持默认的 4K 不变即可；而对于内存紧张的服务而言，调低默认值是提高并发的有效手段

调大 tcp_mem 的上限可以让 TCP 连接使用更多的系统内存，这有利于提升并发能力（tcp_wmem 和 tcp_rmem 的单位是字节，而 tcp_mem 的单位是页面大小。而且，千万不要在 socket 上直接设置SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能）

### 三次握手

client发送syn（syn_send）

client收不到server的ack，会重发syn

net.ipv4.tcp_syn_retries = 2

server响应ack/syn（syn_recv）

收到syn，将其加入到半连接队列
net.ipv4.tcp_max_syn_backlog = 16384

查看SYN_RECV连接数
netstat -n -p | grep SYN_RECV | wc -l

积压的半连接个数超过阈值，新的 SYN 包就会被丢弃。防止SYN Flood 攻击
net.ipv4.tcp_syncookies = 1

serve收不到client返回ack，进行重试发送ack/syn
net.ipv4.tcp_synack_retries = 2

查看队列溢出导致 SYN 被丢弃的个数
netstat -s | grep "SYNs to LISTEN"

client响应ack

server收到ack，产生全连接，放入全连接队列，长度是由 listen(sockfd, backlog) 这个函数里的 backlog 控制的
net.core.somaxconn = 16384

当服务器中积压的全连接个数超过该值后，新的全连接就会被丢弃掉，发送reset来通知 Client不要再重试
设置net.ipv4.tcp_abort_on_overflow = 0，表明不再发送 reset ，给client重试的机会

查看全连接队列溢出的连接个数：netstat-s|grep"listenqueue"

### TFO（TCP fast open）

绕过三次握手，直接发送数据

启用fastopen：net.ipv4.tcp_fastopen=3

### 四次挥手

主动关闭方发送FIN，进入FIN_WAIT_1状态

对端收到FIN，响应ACK，进入CLOSE_WAIT状态

主动关闭方收到ACK，进入FIN_WAIT_2状态

迟迟收不到对端的 FIN 包，那就会一直处于这个状态，于是就会一直消耗系统资源

tcp_fin_timeout，默认为 60s，超过这个时间后就会自动销毁该连接

对端调用close方法，发送FIN，进入LAST_ACK状态

如果迟迟收不到ack，会重试发送FIN

FIN 报文的重发次数仍由 tcp_orphan_retries 参数控制。

主动方收到FIN，响应ACK，进入TIME_WAIT状态

TIME_WAIT 状态会持续一段时间，默认存活60秒

成功发送ACK给被动方

让之前的连接发送的报文在网络中失效

net.ipv4.tcp_max_tw_buckets = 10000，限制该状态的最大个数

net.ipv4.tcp_tw_reuse = 1，net.ipv4.tcp_timestamps=1重用占用的端口号

### tcp重传

tcpdump -s 0 -i eth0 -w tcpdumpfile（保存进出某个网卡的数据包）
 tshark -r tcpdumpfile -R tcp.analysis.retransmission（tshark工具过滤TCP重传包）

TCP tracepoint

cd /sys/kernel/debug/tracing/events/
echo 1 > tcp/tcp_retransmit_skb/enable (开启Tracepoint 来追踪 TCP重传)

 cat trace_pipe（查看TCP重传事件）

echo 0 > tcp/tcp_retransmit_skb/enable(关闭此功能)

### 慢启动优化

增大 init_cwnd （初始拥塞窗口）可以显著地提升网络性能

### 启用BBR拥塞算法

net.ipv4.tcp_congestion_control=bbr

### 超时重传优化

改进RTO 的初始值，这可以显著节省业务的阻塞时间

### 延迟确认

针对 TCP ACK 的一种优化机制，也就是说，不用每次请求都发送一个 ACK，而是先等一会儿（比如 40ms），如果这段时间内，正好有其他包需要发送，那就捎带着ACK一起发送过去。当然，如果一直等不到其他包，那就超时后单独发送 ACK

延迟发送数据（Nagle算法）

TCP 协议中用于减少小包发送数量、合并TCP小包的一种优化算法，目的是为了提高实际带宽的利用率

### DNS优化

对DNS解析的结果进行缓存（dnsmasq）

对DNS解析的结果进行预取

使用HTTPDNS代替常规的DNS解析

基于DNS的全局负载均衡

### 常用工具

dstat

CPU使用率、磁盘 I/O吞吐（读写）、 网络吞吐（收发包）、内存分页统计、系统中断、上下文切换

dstat -tcp

lis

处于监听状态的连接数

act

处于连接状态的连接数

syn

处于三次握手阶段的连接数

tim

time_wait状态的连接数

clo:close-wait状态的连接数

ss  -natp（看 TCP 连接的详细信息）

nstat -z | grep -i  drop(查看系统的网络状态)

tcpdump

TCP Tracepoints

ss -nipt

iperf、netperf、pktgen

sar检查网络状况，观察PPS每秒收发的报文数，还可以观察BPS每秒收发的字节数

Netperf

网络性能的测量工具，主要针对基于 TCP 或 UDP 的传输

Netstat

可以显示与 IP、TCP、UDP 和 ICMP 协议相关的统计数据，提供 TCP连接列表，TCP 和 UDP 监听，进程内存管理的相关报告，一般用于检验本机各端口的网络连接情况

Traceroute

以帮我们知道，数据包从我们的计算机到互联网远端的主机，是走的什么网络路径

### 网络的性能指标

第一个指标是可用性

ping 命令其实就是向远端的机器发送 ICMP 的请求数据包，并等待接收对方的回复。通过请求和应答返回的对比，来判断远端的机器是否连通

第二个指标是响应时间

第三个指标是网络带宽容量

第四个指标网络吞吐量

指在某个时刻，在网络中的两个节点之间，端到端的实际传输速度

第五个指标网络利用率

指网络被使用的时间占总时间的比例

## 内存

### PageCache

#### 时间局部性原理

刚被访问的数据在短时间内再次被访问的概率很高；

#### 空间局部性原理

刚被访问的数据的附近的数据将来也会被访问

#### 预读

磁盘读取不是按需读取，而是按页读取，

 read 方法只读取了 0-32KB 的字节，但内核会把其后的 32-64KB 也读取到 PageCache

#### 不适合的场景

大文件传输

大件中某一部分内容被再次访问到的概率其实非常低，大文件长期长期占用pagecache，热点小文件无法充分使用pagecache，甚至挤出pagecache

#### LRU

避免大量只读一次的文件涌入Active链表，将Active链表上的热数据冲刷出去

第一次读取文件后，文件内容都是inactive的，放到inactive链表。

第二次读取这些内容后，会把它放在active链表上

回收时优先把inactive list的部分page给回收掉，为了维持inactive/active的平衡，就需要把active 链表的部分page给demote到inactive 链表上

#### 查看脏页的积压、回写情况

cat /proc/vmstat | egrep "dirty|writeback"

#### 回收方式

直接回收

后台回收

#### 查看回收行为

sar -B 1

pgscank/s : kswapd(后台回收线程) 每秒扫描的 page 个数

pgscand/s: Application 在内存申请过程中每秒直接扫描的 page 个数

pgsteal/s: 扫描的 page 中每秒被回收的个数

%vmeff: pgsteal/(pgscank+pgscand), 回收率，越接近100说明系统越安全，越接近0说明系统内存压力越大

#### 观察应用的page cache

lsof可以查看某个应用打开的文件

fincore可以看这些文件有多少内容在pagecache。

#### 优化

避免直接内存回收，及早地触发后台回收来

​	vm.min_free_kbytes = 4194304，可以增大这个配置选项来及早地触发后台回

避免脏页积压过多

​	sar -r 来观察系统中的脏页个数

​	vm.dirty_background_bytes = 0
​	vm.dirty_background_ratio = 10
​	vm.dirty_bytes = 0
​	vm.dirty_expire_centisecs = 3000
​	vm.dirty_ratio = 20

避免热点PageCache被回收

​	mlock(2) 防止被回收以及被 drop

​	madvise(2) 告诉内核来立即释放这些 Page Cache

### 缓存和缓存命中率

缓存（也就是cache）是 CPU 与内存之间的临时数据交换器，是为了解决两种速度不匹配的矛盾而设计的

随着多核 CPU 的发展，CPU 缓存通常分成了三个级别：L1、L2、L3。一般而言，每个核上都有 L1 和 L2 缓存。L1 缓存其实分成两部分：一个用于存数据，也就是 L1d Cache，另外一个用于存指令，L1i Cache

要采用多级缓存，并逐级增加缓存大小，就是为了提高各级缓存的命中率，从而最大限度地降低直接访问内存的概率

### 缓存一致性

缓存一致性协议解决了缓存内容不一致的问题，但同时也造成了缓存性能的下降

为了达到数据访问的一致，就需要各个处理器和内核，在访问缓存和写回内存时遵循一些协议，这样的协议就叫缓存一致性协议。常见的缓存一致性协议有 MSI、MESI 等

### 内存带宽和延迟

内存对性能的制约包括三个方面

第一个方面就是内存的使用大小

采用高效的，使用内存少的算法和数据结构

第二个方面是内存访问延迟

各级缓存，都是为了降低内存的直接访问，从而间接地降低内存访问延迟的

尽量降低数据和程序的大小，那么各级缓存的命中率也会相应地提高

第三个方面就是内存带宽

单位时间内，可以并行读取或写入内存的数据量，通常以字节 / 秒为单位表示

### 内存的分配

应用程序向操作系统申请内存时，系统会分配内存，这中间总要花些时间，因为操作系统需要查看可用内存并分配

如果空闲内存不够，系统需要采取措施回收内存，这个过程可能会阻塞

应用申请内存块的大小不定且申请频繁操作时，会造成大量的内存碎片，这些内存碎片会导致系统性能下降

内存池，就是提前申请分配一定数量的、大小仔细考虑的内存块留作备用

### NUMA 的影响

NUMA 包含多个处理器（或者节点），它们之间通过高速互连网络连接而成。每个处理器都有自己的本地内存，但所有处理器可以访问全部内存，访问远端内存的延迟远远大于本地内存访问

部署应用程序时，最好将访问相同数据的多个线程放在相同的处理器上。根据情况，有时候也需要强制去绑定线程到某个节点或者 CPU 核上

### 工具

free

显示总的内存、使用的内存、空闲内存等

Linux下的 /proc 文件系统

根据进程的 ID 来查看每个进程的详细信息，包括分配到进程的内存使用

vmstat、sar查看趋势

系统剩余内存、缓冲区、缓存、swap换入、swap换出、缺页异常

cachestat 

查看整个系统缓存的读写命中情况

cachetop

查看每个进程的缓存命中情况

pcstat 

查看文件在内存中的缓存大小以及缓存比例

如果 RES 太高而 SHR 不高，那可能是堆内存泄漏；如果 SHR 很高，那可能是 tmpfs/shm 之类的数据在持续增长，如果 VIRT 很高而 RES 很小，那可能是进程不停地在申请内存，但是却没有对这些内存进行任何的读写操作，即虚拟地址空间存在内存泄漏。	
通过 pmap 我们能够清楚地观察一个进程的整个的地址空间，包括它们分配的物理内存大小，这非常有助于我们对进程的内存使用概况做一个大致的判断。比如说，如果地址空间中[heap]太大，那有可能是堆内存产生了泄漏；再比如说，如果进程地址空间包含太多的 vma（可以把 maps 中的每一行理解为一个 vma），那很可能是应用程序调用了很多 mmap 而没有 munmap；	

pmap 同样也是解析的 /proc 里的文件，具体文件是 /proc/[pid]/maps 和 /proc/[pid]/smaps，其中 smaps 文件相比 maps 的内容更详细，可以理解为是对 maps 的一个扩展

安装sar :yum install sysstat 配置sudo vi /etc/sysconfig/sysstat，ENABLED=”true”；sudo systemctl start sysstat

​	-u：CPU使用情况;-r：内存使用情况;-b：磁盘使用情况;-n：网络使用情况;-o ：将SAR监控结果保存到文件中；

内存分配分析工具 memleak 

​	yum install bcc-tools

​	默认安装路径/usr/share/bcc/tools/memleak

### 伪共享

以缓存行为单位进行存储，哪怕你修改了缓存行中一个很小很小的数据，它都会将整个缓存行刷新

在 JDK8 以上的版本，通过开启参数 -XX:-RestrictContended，就可以使用注解 @sun.misc.Contended 进行补齐，来避免伪共享的问题

### HugePage

使用较少的映射表来管理大内存，而将页增大的技术

HugePage 也伴随着一些副作用，比如竞争加剧，在一些大内存的机器上，开启后在一定程度上会增加性能

### TLB

### 预先加载

 JVM 的 -XX:+AlwaysPreTouch 参数，JVM 会在启动的时候，就把所有的内存预先分配

最好禁止 Swap。如果必须开启 Swap，降低 swappiness 的值，减少内存回收时 Swap的使用倾向

减少内存的动态分配。比如，可以使用内存池、大页（HugePage）等

尽量使用缓存和缓冲区来访问数据

使用 cgroups 等方式限制进程的内存使用情况。这样，可以确保系统内存不会被异常进程耗尽

通过 /proc/pid/oom_adj ，调整核心应用的 oom_score。这样，可以保证即使内存紧张，核心应用也不会被 OOM 杀死.oom_adj 的范围是 [-17, 15]，数值越大，表示进程越容易被 OOM 杀死；数值越小，表示进程越不容易被 OOM 杀死，其中 -17 表示禁止 OOM;

echo -16 > /proc/$PID/oom_adj

查看因为OOM杀死的进程：dmesg |grep -E 'kill|oom|out of memory'

# 如何高效传输文件

基于用户缓冲区传输文件时，过多的内存拷贝与上下文切换次数会降低性能

零拷贝技术在内核中完成内存拷贝，天然降低了内存拷贝次数。它通过一次系统调用合并了磁盘读取与网络发送两个操作，降低了上下文切换次数

零拷贝技术基于 PageCache，而 PageCache 缓存了最近访问过的数据，提升了访问缓存数据的性能，同时，为了解决机械磁盘寻址慢的问题，它还协助 IO 调度算法实现了 IO 合并与预读

零拷贝有一个缺点，就是不允许进程对文件内容作一些加工再发送，比如数据压缩后再发送

高并发场景下，为了防止 PageCache 被大文件占满后不再对小文件产生作用，大文件不应使用 PageCache，进而也不应使用零拷贝技术处理

绕过 PageCache 的 IO 称为直接 IO。对于磁盘，异步 IO 只支持直接 IO

直接 IO 的应用场景并不多，主要有两种：第一，应用程序已经实现了磁盘文件的缓存，不需要 PageCache 再次缓存，引发额外的性能消耗。比如 MySQL 等数据库就使用直接IO；二，高并发下传输大文件

如果网卡支持SG-DMA技术，可以省去socket缓冲区的拷贝

# nginx优化

## 协议

改为HTTP/2协议

## 压缩

对于文本文件使用 Brotli 压缩算法

提前在磁盘中压缩好，然后通过 add_header 等指令在响应头部中告诉客户端该如何解压

## 提高内存使用率

Nginx 上多使用小于 256KB 的小内存，TCMalloc 的性能要远高于 Linux 默认的 PTMalloc2 内存池

## 限速

limit_conn 可以限制并发连接

limit_req 可以基于 leacky bucket 漏斗原理限速

## Nginx如何防止流量打穿缓存

开启Nginx 的合并回源功能：proxy_cache_lock on;

开启过期缓存功能：proxy_cache_use_staleupdating;

开启Nginx 的合并回源功能。Nginx 会将多个并发请求合并为1条回源请求，并锁住所有的客户端请求，直到回源请求返回后，才会更新缓存，同时向所有客户端返回响应

## 长、短连接

与客户端使用短连接

http模块：keepalive_timeout 0;

与下游 Web 服务使用长连接

proxy_http_version 1.1;

proxy_set_headerConnection ""; 清除header中的Connection，默认是close

## 反向代理的意义

承载海量的连接

保证内网的安全

负载均衡

缓存加速Web请求

隐藏目标服务器

## 减少磁盘IO

Sendfile零拷贝

SSD固态硬盘

AIO异步读写

增大error_log级别

关闭access_log

压缩access_log

批量、压缩写入

syslog代替本地IO

​	在另一个服务器上搭建 rsyslog 服务，然后配置 Nginx 通过 UDP 协议，将 access.log 日志文件从网络写入到 rsyslog 中，这完全移除了日志磁盘 IO

## TCP协议优化

### 减少报文的往返次数

​	启用fast_open

​	增大初始网络拥塞窗口

### 提高硬件资源的利用效率

​	启用TCP_DEFER_ACCEPT 功能，epoll_wait 并不会返回仅完成三次握手的连接，只有连
接上接收到的 TCP 数据报文后，它才会返回 socket，这样 Worker 进程就将原本 2 次切
换就降为 1 次了，虽然会牺牲一些即时性，但提高了 CPU 的效率。

### 高网络效率

打开 tcp_nopush、tcp_nodelay 功能，将小报文合并后批量发送，减少 IP 与 TCP 头部的占比

### 网络快速容错

建立连接

tcp_syn_retries

tcp_synack_retries

关闭连接

fin_timeout

tcp_retries1

tcp_retries2

lingering

端口复用

tcp_tw_reuse

tcp_tw_recycle

## CPU优化

优化cpu缓存的亲和性，将进程和cpu绑定

使用优秀的正则表达式库，可以提供更好的执行性能，比如PCRE

升级为万兆网卡

启用reuseport多个进程监听相同的端口，内核确保只有一个进程被唤醒，不会有惊群问题*

# 应用层协议优化

## 启用缓存避免发送请求

​	将文件大小和修改时间拼接为一个字符串作为文件摘要，将其与服务端的进行比较，相同则返回304，不同则返回最新的资源

## 降低请求次数

​	减少重定向的次数

​	合并请求

​	客户端限制请求的次数

## 降低数据的大小

​	无损压缩:指压缩后不会损失任何信息，可以完全恢复到压缩前的原样文本文件、二进制可执行文件都会使用这类压缩方法

​	有损压缩:通过牺牲质量来提高压缩比，主要针对的是图片和音视频; Brotli 无损压缩算法替换 gzip

# Tomcat调优

## 调整线程池参数

maxThreads

最大线程数

minSpareThreads

最小活 跃线程数

maxIdleTime

空闲线程等待时间

 maxQueueSize

最大的等待请求数，超过则请求拒绝

prestartminSpareThreads

是否在启动时就生成 minSpareThreads 个线程

## 调整连接器参数

禁用AJP连接器

acceptCount

acceptorThreadCount

maxConnections

compression

## 调整IO模式

Tomcat8以后的版本默认使用NIO模式

当Tomcat并发性能有较高要求或者出现瓶颈时，可以尝试使用APR模式，从操作系统级别解决异步IO问题

## 动静分离

可以使用Nginx+Tomcat相结合的部署方案，Nginx负责静态资源访问
Tomcat负责Jsp等动态资源访问处理(因为Tomcat不擅⻓处理静态资源)

# 网络数据传输慢，问题出在哪

netstat/ss命令查看网络收发包，ss -lnt  列出当前所有正在监听tcp端口的连接

1、如果客户端上的接收队列 RecvQ 不为零，则客户端应用程序是性能瓶颈

2、如果服务器上的发送队列 SendQ 为零，则服务器应用程序是性能瓶颈

3、如果客户端的接收队列 RecvQ 为零，而服务器的发送队列 SendQ 为非零，则网络本身是性能瓶颈

java性能调优

哪些计算机资源会成为系统的性能瓶颈

CPU、内存、磁盘IO、网络、异常、数据库、锁竞争

性能调优的标准有哪些？

响应时间

数据库响应时间：数据库操作所消耗的时间，往往是整个请求链中最耗时的

服务端响应时间：服务端包括 Nginx 分发的请求所消耗的时间以及服务端程序执行所消耗的时间

网络响应时间：网络传输时，网络硬件需要对传输的请求进行解析等操作所消耗的时间

客户端响应时间：对于普通的 Web、App 客户端来说，消耗时间是可以忽略不计的，但如果客户端嵌入了大量的逻辑处理，消耗的时间就有可能变长，从而成为系统的瓶颈

吞吐量

在测试中，往往会比较注重系统接口的 TPS（每秒事务处理量），TPS 体现了接口的性能，TPS 越大，性能越好

把吞吐量自下而上地分为两种：磁盘吞吐量和网络吞吐量

磁盘吞吐量，磁盘性能有两个关键衡量指标

一种是 IOPS（Input/Output Per Second），即每秒的输入输出量（或读写次数），这种是指单位时间内系统能处理的 I/O 请求数量，I/O 请求通常为读或写数据操作请求

另一种是数据吞吐量，这种是指单位时间内可以成功传输的数据量

网络吞吐量，这个是指网络传输时没有帧丢失的情况下，设备能够接受的最大数据速率

计算机资源分配使用率

通常由 CPU 占用率、内存使用率、磁盘 I/O、网络 I/O 来表示资源使用率。这几个参数好比一个木桶，如果其中任何一块木板出现短板，任何一项分配不合理，对整个系统性能的影响都是毁灭性的

负载承受能力

当系统压力上升时，你可以观察，系统响应时间的上升曲线是否平缓。这项指标能直观地反馈给你，系统所能承受的负载压力极限。例如，当你对系统进行压测时，系统的响应时间会随着系统并发数的增加而延长，直到系统无法处理这么多请求，抛出大量错误时，就到了极限

扩展

发布新版本时，将迭代之前版本的系统性能指标作为参考标准，通过自动化性能测试，校验迭代发版之后的系统性能是否出现异常，这里就不仅仅是比较吞吐量、响应时间、负载能力等直接指标了，还需要比较系统资源的 CPU 占用率、内存使用率、磁盘I/O、网络 I/O 等几项间接指标的变化

如何制定系统的性能调优策略

性能测试攻略

微基准性能测试

微基准性能测试可以精准定位到某个模块或者某个方法的性能问题，特别适合做一个功能模块或者一个方法在不同实现方式下的性能对比。例如，对比一个方法使用同步实现和非同步实现的性能

宏基准性能测试

综合测试，需要考虑到测试环境、测试场景和测试目标

首先看测试环境，我们需要模拟线上的真实环境

然后看测试场景。我们需要确定在测试某个接口时，是否有其他业务接口同时也在平行运行，造成干扰。如果有，请重视，因为你一旦忽视了这种干扰，测试结果就会出现偏差

最后看测试目标。我们的性能测试是要有目标的，这里可以通过吞吐量以及响应时间来衡量系统是否达标

在做性能测试时，还要注意一些问题

1、热身问题

在 Java 编程语言和环境中，.java 文件编译成为 .class 文件后，机器还是无法直接运行.class 文件中的字节码，需要通过解释器将字节码转换成本地机器码才能运行。为了节约内存和执行效率，代码最初被执行时，解释器会率先解释执行这段代码

随着代码被执行的次数增多，当虚拟机发现某个方法或代码块运行得特别频繁时，就会把这些代码认定为热点代码（Hot Spot Code）。为了提高热点代码的执行效率，在运行时，虚拟机将会通过即时编译器（JIT compiler，just-in-time compiler）把这些代码编译成与本地平台相关的机器码，并进行各层次的优化，然后存储在内存中，之后每次运行代码时，直接从内存中获取即可

2、性能测试结果不稳定

 在做性能测试时发现，每次测试处理的数据集都是一样的，但测试结果却有差异。这是因为测试时，伴随着很多不稳定因素，比如机器其他进程的影响、网络波动以及每个阶段JVM 垃圾回收的不同等等

可以通过多次测试，将测试结果求平均，或者统计一个曲线图，只要保证我们的平均值是在合理范围之内，而且波动不是很大，这种情况下，性能测试就是通过的

3、多 JVM 情况下的影响

应该尽量避免线上环境中一台机器部署多个JVM 的情况

合理分析结果，制定调优策略

在完成性能测试之后，需要输出一份性能测试报告，帮我们分析系统性能测试的情况。其中测试结果需要包含测试接口的平均、最大和最小吞吐量，响应时间，服务器的 CPU、内存、I/O、网络 IO 使用率，JVM 的 GC 频率等，通过观察这些调优标准，可以发现性能瓶颈

通过自下而上的方式分析查找问题

首先从操作系统层面，查看系统的 CPU、内存、I/O、网络的使用率是否存在异常，再通过命令查找异常日志，最后通过分析日志，找到导致瓶颈的原因

还可以从 Java 应用的 JVM层面，查看 JVM 的垃圾回收频率以及内存分配情况是否存在异常，分析日志，找到导致瓶颈的原因

如果系统和 JVM 层面都没有出现异常情况，我们可以查看应用服务业务层是否存在性能瓶颈，例如 Java 编程的问题、读写数据瓶颈等等

从应用层到操作系统层的几种调优策略

1、优化代码

​	应用层的问题代码往往会因为耗尽系统资源而暴露出来，降低内存消耗

​	还有一些是非问题代码导致的性能问题，这种往往是比较难发现的，需要依靠我们的经验来优化

2、优化设计

​	面向对象有很多设计模式，可以帮助我们优化业务层以及中间件层的代码设计

​	比如共享一个创建对象，这样可以减少频繁地创建和销毁对象所带来的性能消耗

3、优化算法

​	好的算法可以帮助我们大大地提升系统性能。例如，在不同的场景中，使用合适的查找算法可以降低时间复杂度

4、时间换空间

​	有时候系统对查询时的速度并没有很高的要求，反而对存储空间要求苛刻，这个时候我们可以考虑用时间来换取空间

5、空间换时间

​	使用存储空间来提升访问速度。现在很多系统都是使用的 MySQL 数据库，较为常见的分表分库是典型的使用空间换时间的案例

6、参数调优

​	除了业务层代码的优化，JVM、Web 容器以及操作系统的优化也是非常关键的

​	根据自己的业务场景，合理地设置 JVM 的内存空间以及垃圾回收算法可以提升系统性能

兜底策略，确保系统稳定性

第一，限流，对系统的入口设置最大访问限制。这里可以参考性能测试中探底接口的 TPS。同时采取熔断措施，友好地返回没有成功的请求

第二，实现智能化横向扩容。智能化横向扩容可以保证当访问量超过某一个阈值时，系统可以根据需求自动横向新增服务

第三，提前扩容。这种方法通常应用于高并发系统，例如，瞬时抢购业务系统。这是因为横向扩容无法满足大量发生在瞬间的请求，即使成功了，抢购也结束了

java编程性能调优

网络 I/O 模型优化

 阻塞式 I/O

在整个 socket 通信工作流程中，socket 的默认状态是阻塞的。也就是说，当发出一个不能立即完成的套接字调用时，其进程将被阻塞，被系统挂起，进入睡眠状态，一直等待相应的操作响应

可能存在的阻塞主要包括以下三种

connect 阻塞：当客户端发起 TCP 连接请求，通过系统调用 connect 函数，TCP 连接的建立需要完成三次握手过程，客户端需要等待服务端发送回来的 ACK 以及 SYN 信号，同样服务端也需要阻塞等待客户端确认连接的 ACK 信号，这就意味着 TCP 的每个 connect都会阻塞等待，直到确认连接

accept 阻塞：一个阻塞的 socket 通信的服务端接收外来连接，会调用 accept 函数，如果没有新的连接到达，调用进程将被挂起，进入阻塞状态

read、write 阻塞：当一个 socket 连接创建成功之后，服务端用 fork 函数创建一个子进程， 调用 read 函数等待客户端的数据写入，如果没有数据写入，调用子进程将被挂起，进入阻塞状态

非阻塞式 I/O

如果没有数据返回，就会直接返回一个 EWOULDBLOCK 或 EAGAIN 错误，此时进程就不会一直被阻塞

当我们把以上操作设置为了非阻塞状态，我们需要设置一个线程对该操作进行轮询检查，这也是最传统的非阻塞 I/O 模型

I/O 复用

如果使用用户线程轮询查看一个 I/O 操作的状态，在大量请求的情况下，这对于 CPU 的使用率无疑是种灾难

Linux 提供了 I/O 复用函数 select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上。这样，系统内核就可以帮我们侦测多个读操作是否处于就绪状态

零拷贝

零拷贝是一种避免多次内存复制的技术，用来优化读写 I/O 操作。

在网络编程中，通常由 read、write 来完成一次 I/O 读写操作。每一次 I/O 读写操作都需要完成四次内存拷贝，路径是 I/O 设备 -> 内核空间 -> 用户空间 -> 内核空间 -> 其它I/O 设备

Linux 内核中的 mmap 函数可以代替 read、write 的 I/O 读写操作，实现用户空间和内核空间共享一个缓存数据

线程模型优化

Reactor 模型

事件接收器 Acceptor：主要负责接收请求连接
事件分离器 Reactor：接收请求后，会将建立的连接注册到分离器中，依赖于循环监听多路复用器 Selector，一旦监听到事件，就会将事件 dispatch 到事件处理器
事件处理器 Handlers：事件处理器主要是完成相关的事件处理，比如读写 I/O 操作

单线程 Reactor 线程模型

所有的 I/O 操作都是在一个 NIO 线程上完成

多线程 Reactor 线程模型

为了解决这种单线程的 NIO 在高负载、高并发场景下的性能瓶颈，后来使用了线程池

在 Tomcat 和 Netty 中都使用了一个 Acceptor 线程来监听连接请求事件，当连接成功之后，会将建立的连接注册到多路复用器中，一旦监听到事件，将交给 Worker 线程池来负责处理。

主从 Reactor 线程模型

在这个模型中，Acceptor 不再是一个单独的 NIO 线程，而是一个线程池。Acceptor 接收到客户端的 TCP 连接请求，建立连接之后，后续的 I/O 操作将交给 Worker I/O 线程。

基于线程模型的 Tomcat 参数调优

在 BIO 中，Tomcat 中的 Acceptor 只负责监听新的连接，一旦连接建立监听到 I/O 操作，将会交给 Worker 线程中，Worker 线程专门负责 I/O 读写操作

在 NIO 中，Tomcat 新增了一个 Poller 线程池，Acceptor 监听到连接后，不是直接使用Worker 中的线程处理请求，而是先将请求发送给了 Poller 缓冲队列。在 Poller 中，维护了一个 Selector 对象，当 Poller 从队列中取出连接后，注册到该 Selector 中；然后通过遍历 Selector，找出其中就绪的 I/O 操作，并使用 Worker 中的线程处理相应的请求

acceptorThreadCount：该参数代表 Acceptor 的线程数量，在请求客户端的数据量非常巨大的情况下，可以适当地调大该线程数量来提高处理请求连接的能力，默认值为 1 ******

maxThreads：专门处理 I/O 操作的 Worker 线程数量，默认是 200，可以根据实际的环境来调整该参数，但不一定越大越好

acceptCount：Tomcat 的 Acceptor 线程是负责从 accept 队列中取出该 connection，然后交给工作线程去执行相关操作，这里的 acceptCount 指的是 accept 队列的大小

maxConnections：表示有多少个 socket 连接到 Tomcat 上。在 BIO 模式中，一个线程只能处理一个连接，一般 maxConnections 与 maxThreads 的值大小相同；在 NIO 模式中，一个线程同时处理多个连接，maxConnections 应该设置得比 maxThreads 要大的多，默认是 10000

# 常见性能优化策略的总结

## **代码**

​	分析相关的代码，找出相应的瓶颈，再来考虑具体的优化策略。有一些性能问题，完全是由于代码写的不合理，通过直接修改一下代码就能解决问题的，比如for循环次数过多、作了很多无谓的条件判断、相同逻辑重复多次等

## **数据库**

​	**SQL调优**

​		由自带的慢查询日志或者开源的慢查询系统定位到具体的出问题的SQL，然后使用explain、profile等工具来逐步调优

​	**架构层面的调优**

​		读写分离、多从库负载均衡、水平和垂直分库分表等方面

​	**连接池调优**

## **缓存**

## **异步**

## **NoSQL**

​	如果业务数据不需要和其他数据作关联，不需要事务或者外键之类的支持，而且有可能写入会异常频繁，这个时候就比较适合用NoSQL（比如HBase）

## **JVM调优**

## **度量系统（监控、报警、服务依赖管理）**

确定指标，按照需求出发，主要需要二方面的指标：

1. 接口性能相关，包括单个接口和全部的QPS、响应时间、调用量（统计时间维度越细越好；最好是，既能以节点为维度，也可以以服务集群为维度，来查看相关数据）。其中还涉及到服务依赖关系的管理，这个时候需要用到服务依赖管理系统
2. 单个机器节点相关，包括CPU使用率、Load值、内存占用率、网卡流量等。如果节点是一些特殊类型的服务（比如MySQL、Redis、Tair），还可以监控这些服务特有的一些关键指标。

采集数据 

​	通常采用异步上报的方式，具体做法有两种：第一种，发到本地的Flume端口，由Flume进程收集到远程的Hadoop集群或者Storm集群来进行运算；第二种，直接在本地运算好以后，使用异步和本地队列的方式，发送到监控服务器

计算数据

可以采用离线运算（MapReduce/Hive）或者实时/准实时运算（Storm/Spark）的方式，运算后的结果存入MySQL或者HBase；某些情况，也可以不计算，直接采集发往监控服务器

存储结果、

展现和分析

​	提供统一的展现分析平台，需要带报表（列表/图表）监控和报警的功能。

#  常见性能优化方式

## 提升单个请求处理效率

​	1、提升调用链上各节点的处理速度

​		在数据库层面，可以考虑加索引、读写分离、分库分表等

​		在应用层层面，可以考虑加缓存（本地缓存，分布式缓存，或两者叠加）、复杂查询走ES索引

​		在代码编写时，可以考虑更高效的算法和数据结构，比如：读多写少用数组、写多读少用链表、取余采用位运算等

​	2、请求内部做并行化处理

​		将单个请求拆分为多个子请求，各子请求并行处理，最后对子请求结果合并后返回

​	3、请求处理异步化

​		最典型的方法是采用消息队列

## 并行处理多个请求

​	当有多个外部请求进来时，可以让系统内部多个节点分别处理这些请求，或者节点内部做并行处理

​	节点采用集群部署，并通过负载均衡策略，将用户请求分摊到不同的节点进行处理；节点内部采用线程池，通过另开线程来实现

重新梳理了整个调用链上，接口的强弱依赖关系，以及每个接口的RT情况

针对弱依赖接口，从超时时间、缓存策略、降级策略三个层面进行了优化

​	RPC调用超时时间设置策略;统计出弱依赖接口 TP99（RT较稳定的接口）/ TP95 （RT波动较大接口）的RT，设置它们的超时时间为 (1 + 50%) (TP99 或 TP95)

​	缓存策略

​		给接口添加前置缓存。我们采用了公司自研的分布式缓存zanKV，缓存的更新策略是：采用了两个缓存，缓存A和缓存B（缓存A的失效时间为m分钟，缓存B为n分钟，且n>2m），首先从缓存A读数据，有则直接返回，没有则从B读数据，并在返回之前，异步启动一个更新线程，同时更新缓存A和缓存B

​	降级策略

​		接口接入熔断降级机制，并对异常做捕获，返回默认值

针对强依赖接口，从超时时间、重试策略、缓存策略三个层面做了优化

​	RPC调用超时时间设置策略

​		统计出强依赖接口 TP99 的RT，设置它们的超时时间为 (1 + 50%) (TP99)

​    重试策略

​		根据接口RT波动性，基于dubbo的重试机制，设置重试次数为2或3次

​	缓存策略

​		缓存预热、热点访问、多级缓存

没有任何前后依赖关系的任务做了并行化处理





传输层

路径可达性测试

测试TCP握手

​	telnet www.baidu.com 443

​	nc -w 2 -zv www.baidu.com 443

查看当前连接情况

​	netstat -ant

查看当前连接的传输速率

​	iftop sudo iftop

查看丢包和乱序等的统计	

​	netstat  -s 统计信息

​	watch --diff netstat -s	 把发生变化的数值进行高亮

ss 命令是 Iproute2 包里的命令，也是 netstat 的“取代者”。它提供了对 socket 的丰富的统计信息，可以查看到当前连接的统计信息：ss -s

网络层

​	ping

​	traceroute www.baidu.com  -I(ICMP)查看网络路径状况; UDP 作为探测协议的，但是很多网络设备并不会对UDP 作出回应。所以我们改成 ICMP 协议做探测后，网络设备就有回应了.它不能对这个路径做连续多次的探测

mtr实现丰富的探测报告。尤其是它对每一跳的丢包率的百分比，是用来定位路径中节点问题的重要指标;当你在遇到“连接状况时好时坏的问题”的时候,用 mtr，可以获取更加全面和动态的链路状态信息

mtr www.baidu.com -r -c 10

查看路由

​	route -n 或者netstat -r 或者 ip  route

数据链路层和物理层

ethtool -S enp0s3

如何抓取报文？

tcpdump host 10.10.10.10

tcpdump port 22

-w 文件名

-c 数量

-s 延长抓包时间

-n 不做地址转换

-X 扩展性数据

ip.addr eq my_ip：过滤出源IP或者目的IP为my_ip的报文
ip.src eq my_ip：过滤出源IP为my_ip的报文
ip.dst eq my_ip：过滤出目的IP为my_ip的报文

ip.addr eq 10.255.252.31 and tcp.flags.reset eq 1

如何判断是否丢包

hping3 命令可以验证服务是否可以正常访问

hping3 -c 10 -S -p {port} {host} -c 表示发送 10 个请求，-S 表示使用 TCP SYN，-p 指定端口为 80

从协议栈中，逐层排查丢包问题

链路层

当缓冲区溢出等原因导致网卡丢包时，Linux 会在网卡收发数据的统计信息中，记录下收发错误的次数

ethtool 或者 netstat ，来查看网卡的丢包记录 netstat -i

输出中的 RX-OK、RX-ERR、RX-DRP、RX-OVR ，分别表示接收时的总包数、总错误数、进入 Ring Buffer 后因其他原因（如内存不足）导致的丢包数以及 Ring Buffer 溢出导致的丢包数

网络层和传输层

netstat -s 命令，就可以看到协议的收发汇总，以及错误信息





按照时间排序的前10条数据

mysqldumpslow -s t -t 10 慢查询日志.log

-s : 排序方式；-c：访问次数；-t：返回多少条数据

pt-query-digest XXXX-slow.log.last







从系统资源瓶颈的角度来说，USE 法是最为有效的方法，即从使用率、饱和度以及错误数这三个方面，来分析 CPU、内存、磁盘和文件系统 I/O、网络以及内核资源限制等各类软硬件资源。

从应用程序瓶颈的角度来说，我们可以把性能问题的来源，分为资源瓶颈、依赖服务瓶颈以及应用自身瓶颈这三类。资源瓶颈跟系统资源瓶颈，本质是一样的；依赖服务瓶颈，你可以使用全链路跟踪系统进行定位；而应用自身的问题，可以通过系统调用、热点函数，或者应用自身的指标监控以及日志监控等，进行分析定位



ab -c 100 -n 10000 目标地址

-c:100个并发请求，-n:10000总共发送的请求量,-k:启用长连接

如果目标 URL 只是站点名本身，还是需要在结尾处加上“>/”，要不然 ab 会报这个错误：ab: invalid URL

sar -n DEV 1 10 #查看网卡性能

iostat 1 10 #查看IO性能

mpstat 2 5 #查看CPU性能

netstat -ant | grep LISTEN | wc -l

#查看 TIME_WAIT 状态的连接数量

netstat -ant |awk '{++a[$6]} END{for (i in a) print i, a[i]}'

ss -ant | awk '{++s[$1]}END{for(k in s) print k,s[k]}'

#检查 Linux 机器的本地源端口范围

cat /proc/sys/net/ipv4/ip_local_port_range 

sysctl net.ipv4.ip_local_port_range

源端口耗尽的问题，本质是 TIME_WAIT 需要停留 2MSL 的时长。要解决 TIME_WAIT 连接过多的问题，可以修改net.ipv4.tcp_max_tw_buckets 这个内核参数。它的默认值为 16384，改为更小的值后，超过这个数值的 TIME_WAIT 连接会被清除，这样，TIME_WAIT 连接数就被控制住了。



在网络性能领域，包量是对 PPS 的简称，一般用来衡量一台主机的网络处理能力。而评估包量的工具是 sar，我们可以运行下面的命令获取到实时的包量值：sar -n DEV 1 10





1、判断一个报文是否是之前报文的重传，可以根据两个关键信息：序列号、载荷长度。这两个值分别相同的多个报文，互相就是重传关系了

2、压测数据起不来的一个常见原因是源端口耗尽，要验证这一点，可以执行以下命令，来查看TIME_WAIT 或者 ESTABLISHED 状态的连接数是否到达了上限

netstat -ant |awk '{++a[$6]} END{for (i in a) print i, a[i]}'

ss -ant | awk '{++s[$1]}END{for(k in s) print k,s[k]}'

net.ipv4.tcp_tw_reuse = 1 \#开启重用

3、连接数到达上限值的问题，往往跟可用的本地源端口范围有关。要查看端口范围，我们可
以执行这条命令

sysctl net.ipv4.ip_local_port_range

4、压测中遇到 RST 的原因，可能跟两端的 idle timeout 不合理有关，建议把客户端的 idle
timeout 设置为比服务端 idle timeout 更低的值





nc测试tcp连接的工具

while true;do nc -zv 10.111.1.111 3306;sleep 1;done



[S.]就是 SYN+ACK，点号[.]就是 ACK。显然，在 LB 这里既收到了客户端发来的全部
4 个 SYN，自己也发出了 4 个 SYN+ACK，可是并没有收到任何一个[.]，也就是握手的第三
个包。顺便提一下，[F.]是指 FIN+ACK，[P.]是指 PUSH+ACK，[R.]就是 RST+ACK 



strace -p $(pidof nginx | awk '{print $1}')

strace -cp {pid} -c:对进程发起的各种系统调用进行了执行次数和执行时间的统计

502：LB收到了后端的无效回复

503：LB明确的知道服务不可用，LB不会转发请求给后端，而是直接向前端回复503

504：LB转发了请求给后端，但后端没有在时限内返回，到了时间点LB就向前端回复504

Perf 业务调用函数的cpu分布 

Systemtab 动态追踪

uptime 或者 top 命令输出里的 CPU load 值，表示的是待运行的任务队列的长度;一般建议 load 值不超过核数
*0.7。也就是 8 核的机器，建议在 load 达到 5.6 的时候，就需要重点关注了

**什么是长 ping 测试?**







 tcpdump 的小知识点。在 tcpdump 的命令行输出中：当路径中某个节点的问题比较严重时，用 nc 工具相对容
易复现出问题。当问题比较隐蔽，比如出错率比较低的时候，可能单纯用 nc 就不够方便了，我们需要另外一个工具 mtr。它是加强版的 traceroute，可以做持续的三层可达性的探测。
[S]代表 SYN；
[.]代表 ACK；
[F.]代表 FIN+ACK；
[P.]代表 PUSH+ACK；
[R.]代表 RST+ACK。



# 常见的cpu性能问题

## 如何处理cpu软中断过多

top命令可以查看si的cpu使用率，也可以看到cpu使用率过高的进程

watch 命令，可以定期运行一个命令来查看输出， -d 参数，还可以高亮出变化的部分

 watch -d cat /proc/softirqs	其中，NET_RX，也就是网络数据包接收软中断的变化速率。

sar 可以用来查看系统的网络收发情况，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的 PPS，即每秒收发的网络帧数

sar -n DEV 1 # -n DEV 表示显示网络收发的报告，间隔 1 秒输出一组数据

如果接收的 PPS 比较大，而接收的 BPS 却很小，说明平均每个网络帧都很小也就是我们通常所说的小包问题

通过使用 tcpdump 抓取 eth0 上的包可以知道这是一个什么样的网络帧，以及从哪里发过来的

tcpdump -i eth0 -n tcp port 80 #-i eth0 只抓取 eth0 网卡；-n 不解析协议名和主机名；cp port 80 表示只抓取 tcp 协议并且端口号为 80 的网络帧

## 如何处理cpu上下文切换过多

vmstat 5 #每隔 5 秒输出 1 组系统总体的上下文切换情况；cs（context switch）是每秒上下文切换的次数；in（interrupt）则是每秒中断的次数；r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数；b（Blocked）则是处于不可中断睡眠状态的进程数。

pidstat -w 5 #查看每个进程上下文切换的情况；-u:	输出 CPU 使用指标;-t:输出线程的上下文切换指标； cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数； nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数；

自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换

非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换

中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。

## 如何处理CPU 使用率过高

 perf top，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数。perf top 虽然实时展示了系统的性能信息，但它的缺点是并不保存数据，也就无法用于离线或者后续的分析。

perf record 则提供了保存数据的功能，保存后的数据，需要你用 perf report 解析展示； -g 参数，开启调用关系的采样，方便我们根据调用链来分析性能问题

