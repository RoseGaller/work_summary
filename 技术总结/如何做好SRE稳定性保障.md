# SRE 的目的，本质上是减少故障次数（即提升 MTBF（Mean Time Between Failure，平均故障时间间隔）），降低故障修复时间（MTTR（Mean Time To Repair， 故障平均修复时间）），增加系统正常运行时间）

# 稳定性保障规划路线

## 故障预防

​	计算服务无状态化，可以弹性扩缩容，冗余，避免单节点

​	对于存储节点，多副本复制、主备机制，可以自动切换

​	要做好架构设计，提供限流、降级、熔断这些 Design-for-Failure 的服务治理手段，以具备故障快速隔离的条件，同时注意数据一致性问题；

​	设置好超时时间、重试次数

​	引入混沌工程这样的故障模拟机制，在线上模拟故障，提前发现问题

## 故障发现

​	舆情感知、监控告警

## 故障定位

​	日志分析、链路追踪、指标观测

## 故障恢复

​	容灾切换、服务降级、服务限流、异常熔断、系统自适应保护

## 故障改进

​	故障复盘，总结经验，找到不足，落地改进措施，改进验收，故障模拟，混沌工程，容量压测

# MTTR 可以细分为 4 个指标

MTTI

​	平均故障发现时间，从故障实际发生，到我们真正开始响应的时间；这个过程可能是用户或者客户反馈舆情监控或者监控告警等渠道触发的

MTTK

​	平均故障定位时间

MTTF

​	平均故障解决时间，也就是从知道了根因在哪里，到我们采取措施恢复业务为之。采取的手段比如限流、降级、熔断甚至重启

MTTV

​	平均故障修复验证时间，就是故障解决后，我们通过用户反馈，监控指标观察等手段，来确认业务是否真正恢复所用的时间

# 如何衡量系统可用性

## 1、时间维度

Availability = Uptime / (Uptime + Downtime)

时长维度，是从故障角度出发对系统稳定性进行评估

时长维度的系统可用性包括三个关键要素：衡量指标、衡量目标、影响时长，只有当问题达到一定影响程度，这时才会计算不可用时长

比如，系统请求状态码为非 5xx （衡量指标）的比例，也就是请求成功率低于 95%（衡量目标），已经连续超过 10 分钟（影响时长），这时就要算作故障

用时长维度来衡量系统稳定性的方式，其主要缺点就是粒度不够精细

## 2、请求维度

Availability = Successful request / Total request

请求维度，是从成功请求占比的角度出发，对系统的稳定性进行评估

请求维度的系统可用性同样包含三个关键要素，第一个衡量指标，请求成功率；第二个衡量目标，成功率达到 95% 才算系统运行正常；第三个是统计周期，比如一天、一周、一个月等等，我们是在一个统计周期内计算整体状况，而不是看单次的

故障一定意味着不稳定，但是不稳定，并不意味着一定有故障发生

# 设定稳定性目标考虑的因素

## 1、成本因素

从理论上来说，肯定是 9 越多稳定性越好，但是相应付出的成本和代价也会更高

## 2、业务容忍度

稳定性怎么设定，很大程度上还要取决于业务上的容忍度

对于核心业务或核心应用，一般对系统稳定性要求是3个“9”或“4 个 9”

对于非核心业务或应用，比如商品评论，商品评分等，或许“2 个 9”也能容忍

## 3、系统当前的稳定性状况

结合系统的实际情况，定一个合理的标准比定一个更高的标准会更重要

# SLI和SLO

SLI，Service Level Indicator，服务等级指标，其实就是我们选择哪些指标来衡量我们的稳定性

SLO，Service Level Objective，服务等级目标，指的就是我们设定的稳定性目标，比如“几个 9”这样的目标

“状态码为非 5xx 的比例”就是 SLI，“大于等于 99.95%”就是 SLO

SLI 就是我们要监控的指标，SLO就是这个指标对应的目标

## 选择 SLI的两大原则

原则一：选择能够标识一个主体是否稳定的指标，如果不是这个主体本身的指标，或者不能标识主体稳定性的，就要排除在外。
原则二：针对有用户界面的业务系统，优先选择与用户体验强相关或用户可以明显感知的指标

## 快速识别 SLI的方法：VALET

Volume- 容量	

容量是指服务承诺的最大容量是多少。比如，一个应用集群的 QPS、TPS、会话数以及连接数等等

Availablity- 可用性	

可用性代表服务是否正常。比如，请求调用的非 5xx 状态码成功率，就可以归于可用性
Latency- 时延	

时延是说响应是否足够快。这是一个会直接影响用户访问体验的指标。，通常对于时延这个指标，我们不会直接做所有请求时延的平均，通常会以类似“90% 请求的时延 <=80ms，或者 95% 请求的时延 <=120ms ”这样的方式来设定时延 SLO

Errors- 错误率	

错误率有多少？这里除了 5xx 之外，我们还可以把 4xx 列进来，因为前面我们的服务可用性不错，但是从业务和体验角度，4xx 太多，用户也是不能接受的。

或者可以增加一些自定义的状态码，看哪些状态是对业务有损的，或者是对用户的体验影响比较大的

Tickets- 人工介入	

是否需要人工介入？如果一项工作或任务需要人工介入，那说明一定是低效或有问题的

## 如何通过 SLO 计算可用性？

Availability = Successful request / Total request

第一种，直接根据成功的定义来计算。

Successful = （状态码非 5xx） & （时延 <= 80ms）

对单次请求的成功与否的判定太过死板，容易错杀误判

第二种方式，SLO 方式计算

SLO1：99.95% 状态码成功率
SLO2：90% Latency <= 80ms
SLO3：99% Latency <= 200ms

Availability = SLO1 & SLO2 & SLO3
只有当这个三个 SLO 同时达标时，整个系统的稳定性才算达标，有一个不达标就不算达标

# 错误预算

Error Budget，就是错误预算。最大的作用就是提示你还有多少次犯错的机会

错误预算是通过SLO 反向推导出来的

如何应用Error Budget？

第一种是稳定性燃尽图

第二种是把错误预算应用在故障定级中

按照该问题消耗的错误预算比例，将故障等级设置为 P0~P4 这么 5 个级别，P0 为最高，P4 为最低。

第三种是用错误预算来确定稳定性共识机制

​	第一，剩余预算充足或未消耗完之前，对问题的发生要有容忍度

​	第二，剩余预算消耗过快或即将消耗完之前，SRE 有权中止和拒绝任何线上变更

第四种是把错误预算应用在告警中

​	相同相似告警，合并后发送

​	基于错误预算来做告警，也就是说我们只关注对稳定性造成影响的告警

从 SLO 达成情况、“人肉”投入情况以及用户实际满意度三个维度来衡量自己业务和系统的 SLO 有效性

该收紧 SLO 就要提高稳定性要求，但是也不能设定太过超出能力范围的目标，始终达不成，SLO 也就没有意义了。

当然，在 SLO 可以达成的情况下，我们还是希望提升我们的用户价值交付效率，围绕着这个终极目标，不断优化自己的 SLO 和错误预算策略

# 落地SLO时还需要考虑哪些因素

区分哪些是核心业务，哪些是非核心业务

核心应用之间的依赖关系，我们称之为强依赖，而其它应用之间的依赖关系，我们称之为弱依赖

## 设定 SLO 有哪些原则？

第一，核心应用的 SLO 要更严格，非核心应用可以放宽

第二，强依赖之间的核心应用，SLO 要一致

第三，弱依赖中，核心应用对非核心的依赖，要有降级、熔断和限流等服务治理手段

第四，Error Budget 策略，核心应用的错误预算要共享，就是如果某个核心应用错误预算消耗完，SLO 没有达成，那整条链路，原则上是要全部暂停操作的

## 如何验证核心链路的 SLO？

### 容量压测

容量压测的主要作用，就是看 SLO 中的 Volume，也就是容量目标是否可以达成

容量压测的另一个作用，就是看在极端的容量场景下，验证我们前面说到的限流降级策略是否可以生效

有些问题必须要在极端场景下模拟才能验证出问题，比如各种服务治理措施，只有在大流量高并发的压力测试下，才能被验证出是否有效。

### 混沌工程

可以帮助我们做到在线上模拟真实的故障，做线上应急演练，提前发现隐患

### 合适做系统验证

错误预算充足就可以尝试，尽量避开错误预算不足的时间段

选择凌晨，业务量相对较小的情况下做演练。这样即使出现问题，影响面也可控，而且会有比较充足的时间执行恢复操作

同时，我们还要评估故障模拟带来的影响

# 故障发现

## On-Call 机制

当一个分布式系统发生故障时，我们的策略一定不是找到根因，而是优先恢复业务。所以，我们往往是通过故障隔离的方式，先快速恢复业务，也就是我们经常听到的 Designfor Failure 这样的策略，再具体一点，就是服务限流、降级、熔断甚至是主备切换这样的手段

MTTI，就是故障的平均确认时间，也就是从故障实际发生时间点，到触发采取实际行动去
定位前的这段时间

第一件事，判断出现的问题是不是故障；

我们平时遇到的问题很多，但是这些问题的影响不见得都很严重，或者都需要我们立即做出响应去处理。即便是故障，我们也会分不同的响应级别去处理

根据错误预算制定故障等级的方式来判定响应方式

第二件事，确定由谁来响应和召集

这两件事，其实就是 SRE 里面提到的On-Call 机制

## 流程机制建设

1、确保关键角色在线

关键角色不是指一两个值班的运维或 SRE 人员，而是整个产品体系中的所有关键角色

不过，接收故障告警或第一时间响应故障的，一般是运维、PE 或 SRE 这样的角色，业务开发同事要确保及时响应 SRE 发起的故障应急

2、组织 War Room 应急组织

专门处理故障的“消防群”（暗含着灭火的意思），会将这些关键角色纳入其中，当有故障发生时会第一时间在消防群通报，这时对应的 On-Call 同事就要第一时间最高优先级响应呼叫（Page）

如果是工作日，对于严重故障，会立刻组织成立 War Room，责任人会马上聚到一起

如果是非工作时间，则会通过电话会议的方式拉起虚拟 War Room

3、建立合理的呼叫方式

建议使用固定的 On-Call 手机，手机在谁手中，谁就承担 On-Call 职责

除非问题迟迟得不到解决，或者遇到了疑难杂症，这种时候再呼叫其他同事参与进来

4、确保资源投入的升级机制

要给到运维和 SRE 授权，当他发现问题不是自己或现有 On-Call 人员能解决的时候，他有权调动其他必要资源投入

如果故障等级偏高，一下无法明确具体找哪些人员投入的时候，SRE 或运维可以直接上升到自己的主管或相关主管那里，要求上级主管协调资源投入

5、与云厂商联合的 On-Call

企业上云是大势所趋，绝大多数情况下，我们对问题和故障的处理，是离不开与云产品工程师一起高效联动的

提前做好与云厂商之间的协作磨合，联合故障演练，必要的授权等等

## 监控报警

​	操作系统监控
​		Zbbix
​		Open-Falcon
​		Promtheus
​	链路监控
​		Cat
​		SkyWalking
​		Zipkin
​		PinPoint
​	业务监控
​	中间件监控
​	报警策略
​		时间维度
​		报警级别
​		阈值设置

# 故障处理

在故障处理过程中采取的所有手段和行动，一切以恢复业务为最高优先级

## 关键角色分工

Incident Commander，故障指挥官，简称为 IC。这个角色是整个指挥体系的核心，他最重要的职责是组织和协调，而不是执行，下面所有的角色都要接受他的指令并严格执行

Communication Lead，沟通引导，简称 CL。负责对内和对外的信息收集及通报，这个角色一般相对固定，由技术支持、QA 或者是某个 SRE 来承担都可以，但是要求沟通表达能力要比较好

Operations Lead，运维指挥，简称 OL。负责指挥或指导各种故障预案的执行和业务恢复

Incident Responders，简称 IR。就是所有需要参与到故障处理中的各类人员，真正的故障定位和业务恢复都是他们来完成的，比如具体执行的 SRE、网络和系统运维、业务开发、平台开发、网络或系统运维、DBA，甚至是 QA 等

## 流程机制

1、故障发现后，On-Call 的 SRE 或运维，最一开始就是 IC，有权召集相应的业务开发或其它必要资源，快速组织 War Room。
2、如果问题和恢复过程非常明确，IC 仍然是 SRE，就不做转移，由他来指挥每个人要做的具体事情，以优先恢复业务优先。
3、如果问题疑难，影响范围很大，这时 SRE 可以要求更高级别的主管介入，比如 SRE 主管或总监等，一般的原则是谁的业务受影响最大，谁来牵头组织。这时 SRE 要将 IC 的职责转移给更高级别的主管，如果是全站范围的影响，必要时技术 VP 或 CTO 也可以承担IC 职责，或者授权给某位总监承担

## 反馈机制

一般要求以团队为单位，每隔 10～15 分钟做一次反馈，反馈当前处理进展以及下一步Action，如果中途有需要马上执行什么操作，也要事先通报，并且要求通报的内容包括对业务和系统的影响是什么，最后由 IC 决策后再执行，避免忙中出错

没有进展也是进展，也要及时反馈。IC 会跟 OL 以及团队主管决策是否要采取更有效果的措施，比如 10 分钟之内还没定位结果，可能我们就会选
择做有损的降级服务，不能让故障持续蔓延，这个时候，反馈就显得尤为重要

除了要做好怎么快速恢复业务，信息同步的及时和透明也非常关键，并且有些安抚措施必须要快速执行到位

## 决定故障处理效率的三因素

技术层面的故障隔离手段是否完备；

故障处理过程中的指挥体系是否完善，角色分工是否明确

故障处理机制保障是否经过足够的演练

## 有效的故障排查手段

通用的故障排查过程 + 发生故障的系统足够了解

排查过程反复采用“假设 - 排除”

收到报警时，先搞清问题的严重程度
	对于大型问题，立即声明一个全员参与的会议
	大不多数的人第一反应是立即开始故障排除过程，试图尽快找到问题根源，正确做法是：尽最大可能让系统恢复服务，止损
快速定位问题时：保存问题现场，比如日志、监控等

​	监控系统记录了整个系统的监控指标，良好的 Dashboard可以方便快速定位问题

​	日志是另外一个无价之宝，日志记录每个操作的信息和对应的系统状态可以让你了解在某一个时刻整个组件究竟在做什么

​	链路追踪工具

​	检查最近对系统的修改可能对查找问题根源很有帮助

应急响应的原则
​		首要原则，第一时间恢复服务
​		影响重大，应升级处理
​		如果短时间解决不了，及时升级处理并尽可能的止损

# 故障复盘

## 故障复盘的黄金三问

第一问：故障原因有哪些？
第二问：我们做什么，怎么做才能确保下次不会再出现类似故障？
第三问：当时如果我们做了什么，可以用更短的时间恢复业务？

## 故障判定的三原则

1、健壮性原则

这个原则是说每个部件自身要具备一定的自愈能力，比如主备、集群、限流、降级和重试等等

2、第三方默认无责

如果使用到了第三方的服务，如公有云的各类服务，包括 IaaS、PaaS、CDN 以及视频等等，我们的原则就是默认第三方无责

涉及第三方服务的情况，在判定改进责任时会分为两部分，对内谁的服务受影响谁改进，并对外推进第三方改进，但是一定要按照我们之前讲的，稳定性一定要做到相对自我可控，而不是完全依赖外部

3、分段判定原则

这个原则主要应用在情况比较复杂的场景。当发生衍生故障，或者故障蔓延的原因与触发原因不同时，我们会将一次故障分段判断

# 以赛带练组织协同

“以赛带练”的目的就是要检验我们的系统稳定性状况到底如何，我们的系统稳定性还有哪些薄弱点

第一步，大促项目开工会。

明确业务指标，指定大促项目的技术保障负责人

同时会明确技术团队的分工以及各个团队的接口人

然后根据大促日期，倒排全链路压测计划。

第二步，业务指标分解及用户模型分析评审会

业务指标分解和用户模型分析阶段，需要业务开发和 PE 团队共同配合，主要是共同分析出核心链路，同时 PE 要分析链路上的应用日常运行情况，特别是 QPS 水位

第三步，应急预案评审会

在预案准备阶段，仍然是 PE 与业务开发配合，针对核心链路和核心应用做有针对性的预案讨论。这时就要细化到接口和方法上，看是否准备好限流、降级和熔断策略，策略有了还要讨论具体的限流值是多少、降级和熔断的具体条件是怎样的，最后这些配置值和配置策略都要落到对应的稳定性配置中心管理起来

第四步，容量压测及复盘会

例行化

第一项，核心应用变更及新上线业务的稳定性评审工作

包括容量评估和压测、预案策略是否完备等工作

PE 会跟业务开发一起评估变动的影响，比如变动的业务逻辑会不会导致性能影响，进而影响容量

对于新增加的接口或逻辑，是否要做限流、降级和熔断等服务治理策略，如果评估出来是必需的，那上线前一定要把这些策略完善好；同时在测试环境上还要做验证，上线后要关注 SLO 是否发生变化等

第二项，周期性技术运营工作

要例行关注错误预算的消耗情况，每周或每月输出系统整体运行报表，并召集业务开发一起开评审会，对变化较大或有风险的 SLO重点评估，进而确认是否要采取改进措施或暂停变更，以此来驱动业务开发关注和提升稳定性。

# 应急预案

## 稳定性目标

### 降低发生故障概率

故障预防：

稳定性设计规范

非数据服务做到无状态、做到实例可以被随时停止、重启、增加

线上服务最小集群单元2个实例，避免单点服务

对外接口需要设置合理的限流，防止突发流量

调用外部服务或者存储层如果有重试的必要，需要有限的合理的重试

依赖数据库的服务需要合理设置数据库连接池

服务之间的调用，服务调用存储层需要设置合理的超时

服务需要具备故障隔离能力，依赖方异常需要有应对措施，降级、熔断

高可用架构

变更准入规范、容量规划、SLA管理规范

故障隔离：多活机房级别容灾、业务逻辑隔离、架构无单点、熔断降级限流

### 降低故障影响的范围

故障发现能力：监控告警异常通知、应急值班oncall、全链路压测、故障演练、风险巡检、舆情客诉感知

应急处理能力：根因定位与决策、应急预案体系、故障自愈能力、应急流量调度、专家知识经验库

## 整体解决思路

第一步，先对业务做分级，从核心业务场景围绕核心链路开始梳理

第二步，生产高频演练，保证预案有效性。

第三步，梳理常见的故障，抽象出常用的止损手段，比如切换开关、熔断降级、自定义操作等

## 应急预案的5个要素

触发条件：即在什么情况下需要执行这条预案，哪个资源出了什么问题，这个条件应是可评估或可量化的

执行动作：即预案具体要做什么事情，步骤要清晰，可观测和可回滚

影响范围：预案执行之后，预估对业务会造成什么样的影响，比如用户体验、数据一致性、资损等

操作人：预案的实际执行人，日常维护负责人，避免人员单点，要有Back up

同步机制：应急预案执行后，需要明确信息同步给哪些人，相应的沟通计划是什么样的，信息推送到哪些渠道等等，在应急指挥时避免信息不一致导致的时机延误

## 预案梳理的流程

### 分析业务场景

在预案梳理时不要贪多，把核心与非核心业务分开，先保障核心业务，并找到几条关键链路来梳理

### 识别关键资源

找到核心业务后，保障这个业务的连续性，需要识别它依赖哪些资源，网络入口、业务应用、中间件、存储、基础设施等等都要识别出来

依然是先看核心的强依赖，对关键资源做重保预案。

对于弱依赖（组件挂掉后不影响核心业务），我们可以通过熔断、降级等自动化的预案覆盖

### 列出故障模式

接着分析这些识别出来的核心资源可能发生哪些故障，比如负载变高影响内存CPU，使用率延迟变大RT变高，错误率增多系统异常，或者其他个性化的故障，都要分析出来做成故障列表

### 制定相应措施

一个故障可能会有多种预案，比如应用重启、限流、扩容等等，这些预案都要详细写下来

做数据的补偿和善后，因为业务恢复后，还要找到被故障影响的用户，通过数据分析后给用户做补偿和善后

## 预案梳理的4个要点

简单：执行动作要足够简单，写好步骤，最好无脑执行

可靠：影响范围足够明确，通过高、中、低风险来对预案进行等级分类，如果是高风险预案，要有审核机制

高效：优先考虑自动化，对手工预案，尽量通过工具脚本来封装

完善：对于业务有损的预案，提前准备好取数工具和修复工具，提升善后效率

##  应急预案的日常治理

首先是按照前面的方法产出预案清单；

然后根据预案制定针对性的实战演练计划，需要确保预案在故障发生时能生效；

接着开始预案执行，执行完后的系统表现也需要做好观测和记录，比如系统资源的变化、业务的恢复情况、资源水位、告警、日志……

最后是效果验证，观察预案是否真的生效，并逐步完善预案





在实践 SRE 的时候，提到的更多是面对某个场景，我们的容量评估应该怎么做？

细化到每个应用、每个接口上限流阈值是多少，

降级和熔断的具体判断策略是怎么样的？

发生故障时，我们 Step by Step 的响应过程应该是怎么样的？需要哪些人参与？大家应该怎么协作？

对于监控，怎么才能更准确？需要用到什么具体算法，参数应该怎么设



可观测数据：日志、统计指标、链路追踪

# 如何编写可观测的日志？

日志开发时（前 5 项）：怎么样写出更有效率的日志

1、日志编写位置

系统/应用启动和参数变更、关键操作节点、大型任务进度上报、异常

2、写入性能

不要在for循环中输出日志、不要输出大量的日志、建议开启info或者以上的日志输出等级、不要输出大字段无用的字段

3、占位符

4、可读性

会话标识：当前操作的用户和与当前请求相关的信息

请求标识：每个请求都拥有一个唯一的标识

参数信息：在日志中增加参数信息能帮你了解到，是什么情况下产生的问题

发生数据的结果：发生数据的结果可以帮你了解程序执行的结果

5、关键信息隐蔽

对于关键的信息不显示或者进行掩码显示，以免信息被盗取后出现数据内容泄漏

日志完成后（ 3 项）：上线前后有哪些需要注意的？

1、减少代码位置信息的输出

尽量不要在日志格式中输出当前日志所在的代码行和方法名称信息

2、文件分类

将不同的业务逻辑按照不同的日志文件来分类，可以帮助你提高检索日志信息时的效率

3、日志review

每一次功能上线后，要对日志进行观察，确认日志内容的输出情况，比如日志内容是否符合预期，会不会有不合适的地方？

# 黄金指标

黄金指标是 Google 针对大量分布式监控的经验总结，它可以在服务级别帮助衡量终端用户体验、服务中断、业务影响等层面的问题

错误
错误指的是当前系统所有发生过的错误请求

在基础层中，我们可以把系统宕机、进程或者端口挂掉、网络丢包这样的情况认定为是故障。

在业务层中，监控的内容也就相对比较多了，比如 Dubbo 调用出错、日志中出现的错误，都是可以被认定为“错误”的指标

延迟
延迟指的是服务在处理请求时花费的时间

在基础层中，我们可以监控 I/O 等待、网络延迟等信息。

在业务层中，则可以监控接口调用的耗时、MySQL 查询的耗时等信息

流量
流量是表现系统负载情况的数据，比如我们常见的 QPS、UV

在基础层中，常见的监控指标有磁盘的读写速度、网络 I/O 情况等。

在业务层中则有请求量、MySQL 查询次数等等

饱和度
饱和度通常是指某个资源的使用率

在基础层中，需要监控的指标有 CPU 使用率、I/O 使用率、句柄使用情况等。

在业务层中则会有线程池的线程使用数、JVM 中的堆内存使用率等信息

立体监控

多维度监控，机器、操作系统层面，进程、端口层面，日志层面，接口层面，用户层面

**如可快速实现机器、操作系统级别的监控？**

zabbix、openfalcon

CPU，LOAD，内存，网络，磁盘异常说明系统一定异常，但**这些参数正常并不能说明系统正常**

**如何快速实现进程、端口级别的监控？**

需要监控中心和监控agent，监控中心分发命令到各个被监控机器的agent上

监控agent和应用部署在一台机器上，执行监控收集信息，上报到监控中心

进程与端口监控的**不足**：进程与端口异常说明系统一定异常，但**它们正常并不能说明系统正常**，例如：进程和端口都在，但ERROR日志狂刷

**如何快速实现日志的监控？**

**ERROR日志的监控**

日志按照级别分离，ERROR日志单独拿出来一个文件是最好的

日志切分规范也很重要，建议按照小时切分

启动定时器，扫描error文件的数量(ls | wc -l)，超过阈值就报警

**日志关键字监控**

当日志中出现一些事先设定的关键字（或者出现频率超过一定阈值），例如exception、timeout就报警

ERROR日志超过阈值说明系统一定异常，**不超过阈值并不能说明系统正常**，例如：进程死锁，此时并不会刷ERROR日志

**如何快速实现接口的监控**

**统一keepalive接口**

应用统一实现一个keepalive接口

监控中心统一调用keepalive接口

**接口处理时间统一上报快速实施要点**

应用层实现处理时间的收集，由于并发量很大，需要在本地进行初步汇总，或者使用upd上报，时间上报需要异步，不要因为这个而增加业务处理时间

上报异常说明系统一定异常，**上报正常不能说明系统正常**

例如：某个服务后端的数据库挂了，此时这个服务的keepalive接口返回其实是正常的，接口的处理时间可能会比平常要快很多



通过模拟调用方调用服务的接口，来对服务进行监控

一个服务提供的接口很多，可以选取最核心的接口进行发包监控

如果一定要对写接口进行监控，务必插入操作和删除操作要是成对进行的

模拟调用的结果要进行业务校验，例如一个http请求仅仅检查返回码是200是不够的，还要检测返回的html或者json的内容是更准确的



极端业务场景下，我们应该如何做好稳定性保障

所面对的极端业务场景：可预测性场景、不可预测性场景

挑战：运维自动化、容量评估和压测、限流降级、开关预案（系统功能开关，比
如当缓存故障时，我们就需要将请求转发到数据库上）、故障模拟、监控体系

极端业务场景下的不确定因素：用户的业务访问模型



故障应急

第一原则：优先恢复业务，而不是定位问题

预案的执行不能仅仅在故障发生时才执行，而是应该把故障模拟和恢复演练放在平
时，凡是没有演练过的预案，都是耍流氓

故障模拟，可以分为不同层面来梳理

IDC 层面，如电力切换、UPS 切换、核心网络设备切换，单设备故障等

系统层面，如 CPU、磁盘 IO、网络 IO、网络时延、丢包等异常场景，这些都有开源或
Linux 系统自带的工具支持，比如 Stress 工具模拟 CPU 升高，dd 模拟磁盘 IO，tc 模拟
网络问题

应用层面，最典型的就是 RT 升高，抛出异常，返回错误码等等，这里还是会用 Spring
的注解功能，在运行时模拟异常状况，然后有针对性地看各种限流降级和开关预案策略能
否生效

第二方面，有效的组织协调

需要有一定的应急机制，来确保相关人员能够快速响应和高效协作。同时，因为对业务造成的影响导致业务团
队会承受很多外部压力，这时也需要有统一的口径对外反馈



全链路跟踪系统

第一个场景，问题定位和排查,瓶颈分析和异常错误定位

第二个场景，服务运行状态分析

单个服务运行质量: QPS、RT 和错误码，同时还可以跟之前的趋势进行对比

应用和服务依赖：根据调用链的分析，统计出应用与应用之间，服务与服务之间的依赖关系及依赖比例

这个依赖管理的作用，就是给我们前面介绍的容量压测和限流降级这两个工作做好准备。我们可以根据来源依赖和比例评估单链路的扩容准备；同时根据去向依赖进行流量拆分，为下游应用的扩容提供依据

# **提升稳定性的几种思路**

## **系统拆分**

​	拆分不是以减少不可用时间为目的，而是以减少故障影响面为目的。因为一个大的系统拆分成了几个小的独立模块，一个模块出了问题不会影响到其他的模块，从而降低故障的影响面

**接入层&服务层** 

​		一般是按照业务模块、重要程度、变更频次等维度拆分

**数据层** 

​		一般先按照业务拆分后，如果有需要还可以做水平垂直拆分、读写分离、数据冷热分离等

## **解耦**

​	系统进行拆分之后，会分成多个模块。模块之间的依赖有强弱之分。如果是强依赖的，那么如果依赖方出问题了，也会受到牵连出问题。这时可以梳理整个流程的调用关系，做成弱依赖调用。弱依赖调用可以用MQ的方式来实现解耦。即使下游出现问题，也不会影响当前模块

## **技术选型**

## **冗余部署&故障自动转移**

服务层的冗余部署很好理解，一个服务部署多个节点，出现了故障，通过“自动故障转移”来实现系统的高可用

数据层的冗余部署，一般分为一主一从、一主多从、多主多从。不过大致的原理都是**数据同步实现多从，数据分片实现多主**，故障转移时都是**通过选举算法选出新的主节点**后在对外提供服务（这里如果写入的时候不做强一致同步，故障转移时会丢失一部分数据

## **容量评估**

在系统上线前需要对整个服务用到的机器、DB、cache都要做容量评估，机器容量的容量可以采用以下方式评估：

- 明确预期流量指标-QPS；
- 明确可接受的时延和安全水位指标（比如CPU%≤40%，核心链路RT≤50ms）；
- 通过压测评估单机在安全水位以下能支持的最高QPS（建议通过混合场景来验证，比如按照预估流量配比同时压测多个核心接口）；
- 最后就可以估算出具体的机器数量了。

DB和cache评估除了QPS之外还需要评估数据量，方法大致相同，等到系统上线后就可以根据监控指标做扩缩容了

## **服务快速扩容能力&泄洪能力**

泄洪能力一般是指冗余部署的情况下，选择几个节点作为备用节点，平时承担很小一部分流量，当流量洪峰来临时，通过调整流量路由策略把热节点的一部分流量转移到备用节点上。

对比扩容方案这种成本相对较高，但是好处就是**响应快，风险小**

## **流量整形&熔断降级**

流量整形也就是常说的限流，主要是防止超过预期外的流量把服务打垮

常见的限流解决方案

1、客户端限流

2、接入层限流

Nginx 限流：设置 QPS、并发数以及 CPU 的 Idle 作为限流指标

API 路由网关模式：对于 QPS 和并发限流，直接在配置中心中进行配置即可

3、应用限流

基础服务限流

资源是我们要进行限流的对象，可能是一个应用，或者一个方法，也可能是一个接口或者 URL，体现了不同的限流粒度和类型。

策略就是限流的规则，比如 QPS 和并发数限流

时间精度。主要指对于 QPS、并发数或 CPU 的阈值判断。比如对于 QPS，我们就会设定一个 QPS 时间精度（假设 3s），如果低于阈值则不启用策略，如果超过阈值就启动限流策略

指标计数。对于并发限制请求，会统计当前的并发数，1 次请求进入到限流模块加 1，等请求结束退出时减 1，当前正在处理的请求数就是并发

Web 类型的限流。对于 Web 类型 URL 接口限流，我们就利用 Servlet 的 Filter 机制进行控制即可

熔断则是为了自身组件或者依赖下游故障时，可以快速失败防止长期阻塞导致雪崩

## **资源隔离**

资源隔离，可以限制单个下游接口可使用的最大线程资源，确保在未熔断前尽可能小的影响整个服务的吞吐量。

说到隔离机制，这里可以扩展说一下，由于每个接口的流量跟RT都不一样，很难去设置一个比较合理的可用最大线程数，并且随着业务迭代，这个阈值也难以维护。这里可以采用共享加独占来解决这个问题，每个接口有自己的独占线程资源，当独占资源占满后，使用共享资源，共享池在达到一定水位后，强制使用独占资源，排队等待。这种机制优点比较明显就是可以在资源利用最大化的同时保证隔离性。

**这里的线程数只是资源的一种，资源也可以是连接数、内存等等**

## **系统性保护**

​	在系统快要崩溃之前对所有流量入口进行无差别限流，当系统恢复到健康水位后停止限流

​	结合应用的 Load、总体平均 RT、入口 QPS 和线程数等几个维度的监控指标，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性

## **可观测性&告警**

基于Metrics（有没有故障）、Traces（哪里有故障）、Logs（故障的原因）三大支柱配置告警规则，可以提前发现系统可能存在的风险&问题，避免故障的发生

## **变更流程三板斧**

**可灰度**

​	 用小比例的一部分流量来验证变更后的内容，减小影响用户群

**可回滚** 

​	出现问题后，能有有效的回滚机制。涉及到数据修改的，发布后会引起脏数据的写入，需要有可靠的回滚流程，保证脏数据的清除

**可观测** 

​	通过观察变更前后的指标变化，很大程度上可以提前发现问题

在代码控制，集成编译、自动化测试、静态代码扫描做好规范
